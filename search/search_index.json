{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+","social":[{"link":"https://github.com/mindsdb/mindsdb","type":"github"},{"link":"https://twitter.com/mindsdb","type":"twitter"},{"link":"https://www.mindsdb.com","type":"link"}]},"docs":[{"location":"","text":"Quickstart \u00b6 Follow the steps below to start making data forecasts with MindsDB using standard SQL. Check out our Getting Started Guide to set up and work with MindsDB using your own data and models. 1. Create a MindsDB Cloud Account \u00b6 Create your free MindsDB Cloud account to start practicing right away using the MindsDB Cloud Editor. If you prefer a local MindsDB installation, follow the Deployment guides of MindsDB documentation. You can install MindsDB in Docker or follow the standard installation using pip . 2. Connect to MindsDB from a SQL Client \u00b6 You can use the MindsDB Cloud SQL Editor or open your preferred SQL client, such as DBeaver or MySQL CLI, and connect to MindsDB. Using the MindsDB Cloud SQL Editor Using a third-party SQL Client Log in to your MindsDB Cloud account. The Editor is the first thing you'll see! To connect to MindsDB from a third-party SQL client, use the connection details below. \"user\":[your_mindsdb_cloud_username], # your Mindsdb Cloud email address is your username \"password\":[your_mindsdb_cloud_password], \"host\":\"cloud.mindsdb.com\", \"port\":\"3306\" If you do not have a preferred SQL client yet, we recommend using the MindsDB SQL Editor or DBeaver Community Edition . Follow this guide to set up your MindsDB SQL Editor. And here , you'll find how to connect to MindsDB from DBeaver. 3. Connecting a Database Using CREATE DATABASE \u00b6 We have a sample database that you can use right away. To connect a database to your MindsDB Cloud account, use the CREATE DATABASE statement, as below. CREATE DATABASE example_data WITH ENGINE = \"postgres\" , PARAMETERS = { \"user\" : \"demo_user\" , \"password\" : \"demo_password\" , \"host\" : \"3.220.66.106\" , \"port\" : \"5432\" , \"database\" : \"demo\" } ; On execution, we get: Query OK , 0 rows affected ( 3 . 22 sec ) 4. Previewing Available Data \u00b6 You can now preview the available data with a standard SELECT statement. SELECT * FROM example_data . demo_data . home_rentals LIMIT 10 ; On execution, we get: + -----------------+---------------------+------+----------+----------------+---------------+--------------+--------------+ | number_of_rooms | number_of_bathrooms | sqft | location | days_on_market | initial_price | neighborhood | rental_price | + -----------------+---------------------+------+----------+----------------+---------------+--------------+--------------+ | 0 . 0 | 1 . 0 | 484 | great | 10 | 2271 | south_side | 2271 | | 1 . 0 | 1 . 0 | 674 | good | 1 | 2167 | downtown | 2167 | | 1 . 0 | 1 . 0 | 554 | poor | 19 | 1883 | westbrae | 1883 | | 0 . 0 | 1 . 0 | 529 | great | 3 | 2431 | south_side | 2431 | | 3 . 0 | 2 . 0 | 1219 | great | 3 | 5510 | south_side | 5510 | | 1 . 0 | 1 . 0 | 398 | great | 11 | 2272 | south_side | 2272 | | 3 . 0 | 2 . 0 | 1190 | poor | 58 | 4463 | westbrae | 4124 | | 1 . 0 | 1 . 0 | 730 | good | 0 | 2224 | downtown | 2224 | | 0 . 0 | 1 . 0 | 298 | great | 9 | 2104 | south_side | 2104 | | 2 . 0 | 1 . 0 | 878 | great | 8 | 3861 | south_side | 3861 | + -----------------+---------------------+------+----------+----------------+---------------+--------------+--------------+ You could also browse the databases of MindsDB using the command below. SHOW databases ; On execution, we get: + ---------------------+ | Database | + ---------------------+ | information_schema | | mindsdb | | files | | example_data | + ---------------------+ To learn more about MindsDB tables structure, check out this guide . 5. Creating a Predictor Using CREATE MODEL \u00b6 Now you are ready to create your first predictor. Use the CREATE MODEL statement, as below. CREATE MODEL mindsdb . home_rentals_predictor FROM example_data ( SELECT * FROM demo_data . home_rentals ) PREDICT rental_price ; On execution, we get: Query OK , 0 rows affected ( 9 . 79 sec ) 6. Checking the Status of a Predictor \u00b6 It may take a couple of minutes until the predictor is trained. You can monitor the status of your predictor by executing the following command: SELECT status FROM mindsdb . models WHERE name = 'home_rentals_predictor' ; On execution, we get: + ----------+ | status | + ----------+ | training | + ----------+ Or: + ----------+ | status | + ----------+ | complete | + ----------+ Alternatively, you can use the SHOW MODELS command as below. SHOW MODELS [ FROM * database_project_name * ] [ LIKE * model_name * ] [ WHERE * filter * ]; Here is an example: SHOW MODELS FROM mindsdb LIKE 'home_rentals_predictor' WHERE status = 'complete' ; The status of the predictor must be complete before you can start making predictions. 7. Making a Prediction Using SELECT \u00b6 The SELECT statement allows you to make predictions based on features, where features are the input variables, or input columns, that are used to make forecasts. Let's predict what would be the rental price of a 1000 square feet house with two bathrooms. SELECT rental_price FROM mindsdb . home_rentals_predictor WHERE number_of_bathrooms = 2 AND sqft = 1000 ; On execution, we get: + --------------+ | rental_price | + --------------+ | 1130 | + --------------+ Congratulations If you got this far, you have successfully trained a predictive model using SQL and got the future data!","title":"Quickstart"},{"location":"#quickstart","text":"Follow the steps below to start making data forecasts with MindsDB using standard SQL. Check out our Getting Started Guide to set up and work with MindsDB using your own data and models.","title":"Quickstart"},{"location":"#1-create-a-mindsdb-cloud-account","text":"Create your free MindsDB Cloud account to start practicing right away using the MindsDB Cloud Editor. If you prefer a local MindsDB installation, follow the Deployment guides of MindsDB documentation. You can install MindsDB in Docker or follow the standard installation using pip .","title":"1. Create a MindsDB Cloud Account"},{"location":"#2-connect-to-mindsdb-from-a-sql-client","text":"You can use the MindsDB Cloud SQL Editor or open your preferred SQL client, such as DBeaver or MySQL CLI, and connect to MindsDB. Using the MindsDB Cloud SQL Editor Using a third-party SQL Client Log in to your MindsDB Cloud account. The Editor is the first thing you'll see! To connect to MindsDB from a third-party SQL client, use the connection details below. \"user\":[your_mindsdb_cloud_username], # your Mindsdb Cloud email address is your username \"password\":[your_mindsdb_cloud_password], \"host\":\"cloud.mindsdb.com\", \"port\":\"3306\" If you do not have a preferred SQL client yet, we recommend using the MindsDB SQL Editor or DBeaver Community Edition . Follow this guide to set up your MindsDB SQL Editor. And here , you'll find how to connect to MindsDB from DBeaver.","title":"2. Connect to MindsDB from a SQL Client"},{"location":"#3-connecting-a-database-using-create-database","text":"We have a sample database that you can use right away. To connect a database to your MindsDB Cloud account, use the CREATE DATABASE statement, as below. CREATE DATABASE example_data WITH ENGINE = \"postgres\" , PARAMETERS = { \"user\" : \"demo_user\" , \"password\" : \"demo_password\" , \"host\" : \"3.220.66.106\" , \"port\" : \"5432\" , \"database\" : \"demo\" } ; On execution, we get: Query OK , 0 rows affected ( 3 . 22 sec )","title":"3. Connecting a Database Using CREATE DATABASE"},{"location":"#4-previewing-available-data","text":"You can now preview the available data with a standard SELECT statement. SELECT * FROM example_data . demo_data . home_rentals LIMIT 10 ; On execution, we get: + -----------------+---------------------+------+----------+----------------+---------------+--------------+--------------+ | number_of_rooms | number_of_bathrooms | sqft | location | days_on_market | initial_price | neighborhood | rental_price | + -----------------+---------------------+------+----------+----------------+---------------+--------------+--------------+ | 0 . 0 | 1 . 0 | 484 | great | 10 | 2271 | south_side | 2271 | | 1 . 0 | 1 . 0 | 674 | good | 1 | 2167 | downtown | 2167 | | 1 . 0 | 1 . 0 | 554 | poor | 19 | 1883 | westbrae | 1883 | | 0 . 0 | 1 . 0 | 529 | great | 3 | 2431 | south_side | 2431 | | 3 . 0 | 2 . 0 | 1219 | great | 3 | 5510 | south_side | 5510 | | 1 . 0 | 1 . 0 | 398 | great | 11 | 2272 | south_side | 2272 | | 3 . 0 | 2 . 0 | 1190 | poor | 58 | 4463 | westbrae | 4124 | | 1 . 0 | 1 . 0 | 730 | good | 0 | 2224 | downtown | 2224 | | 0 . 0 | 1 . 0 | 298 | great | 9 | 2104 | south_side | 2104 | | 2 . 0 | 1 . 0 | 878 | great | 8 | 3861 | south_side | 3861 | + -----------------+---------------------+------+----------+----------------+---------------+--------------+--------------+ You could also browse the databases of MindsDB using the command below. SHOW databases ; On execution, we get: + ---------------------+ | Database | + ---------------------+ | information_schema | | mindsdb | | files | | example_data | + ---------------------+ To learn more about MindsDB tables structure, check out this guide .","title":"4. Previewing Available Data"},{"location":"#5-creating-a-predictor-using-create-model","text":"Now you are ready to create your first predictor. Use the CREATE MODEL statement, as below. CREATE MODEL mindsdb . home_rentals_predictor FROM example_data ( SELECT * FROM demo_data . home_rentals ) PREDICT rental_price ; On execution, we get: Query OK , 0 rows affected ( 9 . 79 sec )","title":"5. Creating a Predictor Using CREATE MODEL"},{"location":"#6-checking-the-status-of-a-predictor","text":"It may take a couple of minutes until the predictor is trained. You can monitor the status of your predictor by executing the following command: SELECT status FROM mindsdb . models WHERE name = 'home_rentals_predictor' ; On execution, we get: + ----------+ | status | + ----------+ | training | + ----------+ Or: + ----------+ | status | + ----------+ | complete | + ----------+ Alternatively, you can use the SHOW MODELS command as below. SHOW MODELS [ FROM * database_project_name * ] [ LIKE * model_name * ] [ WHERE * filter * ]; Here is an example: SHOW MODELS FROM mindsdb LIKE 'home_rentals_predictor' WHERE status = 'complete' ; The status of the predictor must be complete before you can start making predictions.","title":"6. Checking the Status of a Predictor"},{"location":"#7-making-a-prediction-using-select","text":"The SELECT statement allows you to make predictions based on features, where features are the input variables, or input columns, that are used to make forecasts. Let's predict what would be the rental price of a 1000 square feet house with two bathrooms. SELECT rental_price FROM mindsdb . home_rentals_predictor WHERE number_of_bathrooms = 2 AND sqft = 1000 ; On execution, we get: + --------------+ | rental_price | + --------------+ | 1130 | + --------------+ Congratulations If you got this far, you have successfully trained a predictive model using SQL and got the future data!","title":"7. Making a Prediction Using SELECT"},{"location":"Getting%20Started%20with%20MindsDB%20on%20Digital%20Ocean/","text":"Getting Started with MindsDB on Digital Ocean \u00b6 Introduction \u00b6 MindsDB has the vision to enforce advanced predictive capabilities directly in our Database. With this, anyone who knows SQL can utilize the built-in machine learning techniques without needing any other outsourced tools or training. In short, MindsDB aims to provide intelligent databases to make better data-driven decisions in the existing database without the effort of creating a new one. Deployment on Digital Ocean \u00b6 Digital Ocean is a Cloud computing platform that provides cloud computing services with predictable pricing, developer-friendly features, and scalability as you need. Today, we will be deploying the MindsDB server on a Digital Ocean Droplet (a scalable virtual machine on DO Platform) and trying to connect to it using our local MySQL client or Dbeaver. Step 1: Sign up for a Digital Ocean account if you don't have one yet. They provide 100$ Credits when you sign up so that you can try it for free. Step 2: Once you've created an account, you should see a dashboard like this. If you look towards the left panel, you can find the Droplets option. Now click on Droplets. Step 3: Once you are in the Droplets Dashboard, hit Create Droplet . Step 4: Now the Create Droplet dashboard opens. Here you need to choose the configurations you want for your virtual machine and can also select any additional applications you want to install on your VM from the Marketplace. For now, we will move ahead with the following configurations that would boot up a standard VM for us with the required performance. Distributions : Ubuntu 20.04(LTS) x64 Choose a Plan : Shared CPU (Basic) CPU Options : $20/mo (4GB/2CPUs, 80GB SSD Disk, 4TB Transfer) Datacenter Region : Closest Region to you to reduce Latency (Bangalore for me) Authentication : SSH Key (If you have one) or Root Password (Set your Root Password below to login to the VM) Select Additional Option : Choose Monitoring and IPv6 which are free services Finalize and Create : Set the number of Droplets you want (1 for now) and the Name of Droplet Select a Project : Leave it at default for now Or you can simply go to the top and select Marketplace and search for Docker there and select Docker on Ubuntu directly, as we would be using Docker to install the MindsDB server on our Droplet, instead of choosing a Ubuntu LTS distribution and then installing docker on top of it manually. Hit Create Droplet and wait for a while till it spins one for us in the cloud. Step 5: Once your Droplet is created, you can always click on the Droplets in the left Navigation pane and Click on the Droplet name to go into the Droplet Dashboard details. Now click on the Console from the top right corner. This should open up a terminal for you where you can interact with your VM. Step 6: Now while you are inside the console, simply check whether Docker is installed or not using the following command. docker run hello-world If the command successfully returns Hello From Docker , then you're good to go. Now we start with the installation of MindsDB on our Droplet. Step 7: Now we will use the following commands in sequence to get done with the installation of MindDB Sever. docker pull mindsdb/mindsdb This should pull the latest Production image to our Droplet. You can also use the latest Beta version as well using the following command. docker pull mindsdb/mindsdb_beta Step 8: After that, we need to publish the ports so that we can hit the endpoints and communicate with the MindsDB Server in our Droplet. So, we will first expose the MindsDB GUI Port and then the MySQL Port that we will be using for now with the commands below. MindsDB GUI with MySQL API docker run -p 47334:47334 -p 47335:47335 mindsdb/mindsdb You can always follow the MindsDB Getting Started documentation if you need further information or want to install MindsDB by any other means other than Docker. Step 9: You might see a warning on the console after you run the above command that some data sources may not be available and you can always find details on how to add the specific data source on the MindDB Repo . For now, we will add the data source for MSSQL using the command below. Open another console for the droplet and run it if MindsDB is still running in the first console. pip install mindsdb-datasources[mssql] NOTE: You have to install python first to be able to use pip . Run the following to get it installed. apt install python3-pip You may encounter a few errors with regards to the versioning of pandas and scramp which you can ignore. Step 10: Now you can access the MindsDB GUI using the Droplet IPv4:47334 . Make sure you have turned off any VPN service you're using or else you may not be able to connect to the GUI using the URL. Step 11: This step enables us to connect a database client to our MindsDB server. For now, I would be using DBeaver to do the same. You can also connect your local MySQL server following the steps here . NOTE: You should follow the local deployment steps instead of cloud as this is a local instance hosted on the cloud and not the official MindsDB cloud edition. The default username is mindsdb and the default password is empty. We will first need to define the connection settings in Dbeaver before we can connect it. Always make sure you're using MySQL only or MySQL8 or higher from the available options. Click on Next and fill in the following details in the upcoming screen. Hostname : Your Droplet IPv4 Port : 47335 Username : mindsdb Password : Leave it empty. Now hit the Test Connection and once it returns success, click on Finish . NOTE: If you're running it for the first time, it may ask you to download some additional driver files. Check Force Download/Overwrite and hit `Download to proceed. Tables in MindsDB \u00b6 Once connected to the mindsdb database, it will contain 3 tables predictors , commands and datasources . You can simply run a query show tables; to see these as shown in the snippet above. The Predictors table contains all the newly trained ML models as a new record. Each column in the Predictors table contains information about each of these models. You can always find more information by visiting MindsDB Documentation page. This concludes the tutorial on how to deploy the MindsDB server on a Digital Ocean Droplet and connect to it using a local client. Please drop a like let me know in the comments if you found this useful or have any other suggestions. NOTE: The dashboard images belong to the official platforms of Digital Ocean and MindsDB respectively and are used here only for illustrative purposes. This tutorial has already been published on Dev.to and you find it here","title":"Getting Started with MindsDB on Digital Ocean"},{"location":"Getting%20Started%20with%20MindsDB%20on%20Digital%20Ocean/#getting-started-with-mindsdb-on-digital-ocean","text":"","title":"Getting Started with MindsDB on Digital Ocean"},{"location":"Getting%20Started%20with%20MindsDB%20on%20Digital%20Ocean/#introduction","text":"MindsDB has the vision to enforce advanced predictive capabilities directly in our Database. With this, anyone who knows SQL can utilize the built-in machine learning techniques without needing any other outsourced tools or training. In short, MindsDB aims to provide intelligent databases to make better data-driven decisions in the existing database without the effort of creating a new one.","title":"Introduction"},{"location":"Getting%20Started%20with%20MindsDB%20on%20Digital%20Ocean/#deployment-on-digital-ocean","text":"Digital Ocean is a Cloud computing platform that provides cloud computing services with predictable pricing, developer-friendly features, and scalability as you need. Today, we will be deploying the MindsDB server on a Digital Ocean Droplet (a scalable virtual machine on DO Platform) and trying to connect to it using our local MySQL client or Dbeaver. Step 1: Sign up for a Digital Ocean account if you don't have one yet. They provide 100$ Credits when you sign up so that you can try it for free. Step 2: Once you've created an account, you should see a dashboard like this. If you look towards the left panel, you can find the Droplets option. Now click on Droplets. Step 3: Once you are in the Droplets Dashboard, hit Create Droplet . Step 4: Now the Create Droplet dashboard opens. Here you need to choose the configurations you want for your virtual machine and can also select any additional applications you want to install on your VM from the Marketplace. For now, we will move ahead with the following configurations that would boot up a standard VM for us with the required performance. Distributions : Ubuntu 20.04(LTS) x64 Choose a Plan : Shared CPU (Basic) CPU Options : $20/mo (4GB/2CPUs, 80GB SSD Disk, 4TB Transfer) Datacenter Region : Closest Region to you to reduce Latency (Bangalore for me) Authentication : SSH Key (If you have one) or Root Password (Set your Root Password below to login to the VM) Select Additional Option : Choose Monitoring and IPv6 which are free services Finalize and Create : Set the number of Droplets you want (1 for now) and the Name of Droplet Select a Project : Leave it at default for now Or you can simply go to the top and select Marketplace and search for Docker there and select Docker on Ubuntu directly, as we would be using Docker to install the MindsDB server on our Droplet, instead of choosing a Ubuntu LTS distribution and then installing docker on top of it manually. Hit Create Droplet and wait for a while till it spins one for us in the cloud. Step 5: Once your Droplet is created, you can always click on the Droplets in the left Navigation pane and Click on the Droplet name to go into the Droplet Dashboard details. Now click on the Console from the top right corner. This should open up a terminal for you where you can interact with your VM. Step 6: Now while you are inside the console, simply check whether Docker is installed or not using the following command. docker run hello-world If the command successfully returns Hello From Docker , then you're good to go. Now we start with the installation of MindsDB on our Droplet. Step 7: Now we will use the following commands in sequence to get done with the installation of MindDB Sever. docker pull mindsdb/mindsdb This should pull the latest Production image to our Droplet. You can also use the latest Beta version as well using the following command. docker pull mindsdb/mindsdb_beta Step 8: After that, we need to publish the ports so that we can hit the endpoints and communicate with the MindsDB Server in our Droplet. So, we will first expose the MindsDB GUI Port and then the MySQL Port that we will be using for now with the commands below. MindsDB GUI with MySQL API docker run -p 47334:47334 -p 47335:47335 mindsdb/mindsdb You can always follow the MindsDB Getting Started documentation if you need further information or want to install MindsDB by any other means other than Docker. Step 9: You might see a warning on the console after you run the above command that some data sources may not be available and you can always find details on how to add the specific data source on the MindDB Repo . For now, we will add the data source for MSSQL using the command below. Open another console for the droplet and run it if MindsDB is still running in the first console. pip install mindsdb-datasources[mssql] NOTE: You have to install python first to be able to use pip . Run the following to get it installed. apt install python3-pip You may encounter a few errors with regards to the versioning of pandas and scramp which you can ignore. Step 10: Now you can access the MindsDB GUI using the Droplet IPv4:47334 . Make sure you have turned off any VPN service you're using or else you may not be able to connect to the GUI using the URL. Step 11: This step enables us to connect a database client to our MindsDB server. For now, I would be using DBeaver to do the same. You can also connect your local MySQL server following the steps here . NOTE: You should follow the local deployment steps instead of cloud as this is a local instance hosted on the cloud and not the official MindsDB cloud edition. The default username is mindsdb and the default password is empty. We will first need to define the connection settings in Dbeaver before we can connect it. Always make sure you're using MySQL only or MySQL8 or higher from the available options. Click on Next and fill in the following details in the upcoming screen. Hostname : Your Droplet IPv4 Port : 47335 Username : mindsdb Password : Leave it empty. Now hit the Test Connection and once it returns success, click on Finish . NOTE: If you're running it for the first time, it may ask you to download some additional driver files. Check Force Download/Overwrite and hit `Download to proceed.","title":"Deployment on Digital Ocean"},{"location":"Getting%20Started%20with%20MindsDB%20on%20Digital%20Ocean/#tables-in-mindsdb","text":"Once connected to the mindsdb database, it will contain 3 tables predictors , commands and datasources . You can simply run a query show tables; to see these as shown in the snippet above. The Predictors table contains all the newly trained ML models as a new record. Each column in the Predictors table contains information about each of these models. You can always find more information by visiting MindsDB Documentation page. This concludes the tutorial on how to deploy the MindsDB server on a Digital Ocean Droplet and connect to it using a local client. Please drop a like let me know in the comments if you found this useful or have any other suggestions. NOTE: The dashboard images belong to the official platforms of Digital Ocean and MindsDB respectively and are used here only for illustrative purposes. This tutorial has already been published on Dev.to and you find it here","title":"Tables in MindsDB"},{"location":"community/","text":"Join our community \u00b6 If you have questions or you want to chat with the MindsDB core team or other community members, you can join our Slack workspace . MindsDB newsletter \u00b6 To get updates on MindsDB\u2019s latest announcements, releases and events, sign up for our newsletter . Become a MindsDB Beta tester \u00b6 If you want to become a part of our product and get first access to all of our latest updates, join our Beta testers community . Talk to our engineers \u00b6 If you want to use MindsDB or you have more questions, you can schedule a call by clicking on Talk to our Engineers button. Get in touch for collaboration \u00b6 Contact us by submitting this form .","title":"Join our Community"},{"location":"community/#join-our-community","text":"If you have questions or you want to chat with the MindsDB core team or other community members, you can join our Slack workspace .","title":"Join our community"},{"location":"community/#mindsdb-newsletter","text":"To get updates on MindsDB\u2019s latest announcements, releases and events, sign up for our newsletter .","title":"MindsDB newsletter"},{"location":"community/#become-a-mindsdb-beta-tester","text":"If you want to become a part of our product and get first access to all of our latest updates, join our Beta testers community .","title":"Become a MindsDB Beta tester"},{"location":"community/#talk-to-our-engineers","text":"If you want to use MindsDB or you have more questions, you can schedule a call by clicking on Talk to our Engineers button.","title":"Talk to our engineers"},{"location":"community/#get-in-touch-for-collaboration","text":"Contact us by submitting this form .","title":"Get in touch for collaboration"},{"location":"connect/","text":"Connect your data \u00b6 Connect to database \u00b6 From the left navigation menu, select Database Integration. Click on the ADD DATABASE button. In the Connect to Database modal window: Select the Supported Database that you want to connect to. Add the Integration name(how you want to name the integration between your database and MindsDB). Add the Database name. Add the Hostname. Add Port. Add the database user. Add Password for the user. Click on CONNECT . Required inputs Note: For different type of database there could be different required inputs you need to provide. After connecting MindsDB and the database, use your SQL client to connect to MindsDB as a database .","title":"Connect your data"},{"location":"connect/#connect-your-data","text":"","title":"Connect your data"},{"location":"connect/#connect-to-database","text":"From the left navigation menu, select Database Integration. Click on the ADD DATABASE button. In the Connect to Database modal window: Select the Supported Database that you want to connect to. Add the Integration name(how you want to name the integration between your database and MindsDB). Add the Database name. Add the Hostname. Add Port. Add the database user. Add Password for the user. Click on CONNECT . Required inputs Note: For different type of database there could be different required inputs you need to provide. After connecting MindsDB and the database, use your SQL client to connect to MindsDB as a database .","title":"Connect to database"},{"location":"contribute/","text":"Contribute to MindsDB \u00b6 Thank you for your interest in contributing to MindsDB. MindsDB is free, open source software, and all types of contributions are welcome, whether they\u2019re documentation changes, bug reports, bug fixes or new source code changes. Contribution issues \u00b6 Most of the issues that are open for contributions will be tagged with good-first-issue or help-wanted . A great place to start looking will be our GitHub projects for: Community writers dashboard . Community code contributors dashboard . Also, we are always open to suggestions so feel free to open new issues with your ideas and we can give you guidance! After you find the issue that you want to contribute to, follow the fork-and-pull workflow: Fork the MindsDB repository Clone the repository locally Make changes and commit them Push your local branch to your fork Submit a Pull Request so that we can review your changes Write a commit message Make sure that the CI tests are GREEN NOTE: Be sure to merge the latest from \"upstream\" before making a Pull Request! Pull Request reviews are done on a regular basis. Please make sure you respond to our feedback/questions and sign our CLA. Documentation \u00b6 We are always trying to improve our documentation. All Pull Requests that improve our grammar or docs structure or fix typos are welcomed. Check the documentation tagged issues and help us. Write for us \u00b6 Do you find MindsDB useful and want to share your story? Make a PR to this repo with your writing in a markdown file, or just post it on Medium, Dev or your own blog post. We would love to hear from you .","title":"How to Contribute"},{"location":"contribute/#contribute-to-mindsdb","text":"Thank you for your interest in contributing to MindsDB. MindsDB is free, open source software, and all types of contributions are welcome, whether they\u2019re documentation changes, bug reports, bug fixes or new source code changes.","title":"Contribute to MindsDB"},{"location":"contribute/#contribution-issues","text":"Most of the issues that are open for contributions will be tagged with good-first-issue or help-wanted . A great place to start looking will be our GitHub projects for: Community writers dashboard . Community code contributors dashboard . Also, we are always open to suggestions so feel free to open new issues with your ideas and we can give you guidance! After you find the issue that you want to contribute to, follow the fork-and-pull workflow: Fork the MindsDB repository Clone the repository locally Make changes and commit them Push your local branch to your fork Submit a Pull Request so that we can review your changes Write a commit message Make sure that the CI tests are GREEN NOTE: Be sure to merge the latest from \"upstream\" before making a Pull Request! Pull Request reviews are done on a regular basis. Please make sure you respond to our feedback/questions and sign our CLA.","title":"Contribution issues"},{"location":"contribute/#documentation","text":"We are always trying to improve our documentation. All Pull Requests that improve our grammar or docs structure or fix typos are welcomed. Check the documentation tagged issues and help us.","title":"Documentation"},{"location":"contribute/#write-for-us","text":"Do you find MindsDB useful and want to share your story? Make a PR to this repo with your writing in a markdown file, or just post it on Medium, Dev or your own blog post. We would love to hear from you .","title":"Write for us"},{"location":"docs-rules/","text":"Style Guide for MindsDB Documentation \u00b6 Syntax for SQL commands \u00b6 Follow the rules below when writing an SQL command. Add a semi-colon ; at the end of each SQL command. Use all-caps when writing the keywords, such as SELECT , FROM , JOIN , WHERE , GROUP BY , ORDER BY , PREDICT , AS , CREATE TABLE , INSERT INTO , etc. When writing a query, start a new line for the following keywords: SELECT , FROM , JOIN , WHERE , GROUP BY , ORDER BY , PREDICT , USING , AND , OR . It is to avoid the horizontal scrollbar. Example \u00b6 SELECT * FROM table_name_1 a JOIN table_name_2 b WHERE column_name_1 = value_name_1 AND column_name_2 = value_name_2 GROUP BY a . column_name_2 ORDER BY b . column_name_1 ; Syntax for SQL commands along with their output \u00b6 Follow the syntax below when documenting an SQL command and its output. ```sql QUERY GOES HERE ``` On execution , we get: ```sql +---------------+---------------+ | [ column_name ] | [ column_name ] | +---------------+---------------+ | [ value ] | [ value ] | +---------------+---------------+ ``` Where: | Name | Description | | ----------------------------------- | ----------------------------------- | | `VARIABLE NAME GOES HERE` | VARIABLE DESCRIPTION GOES HERE | Note If the output is not a table, remove the output table from above and place your output message there. Example 1 \u00b6 ```sql SELECT * FROM table_name_1 a JOIN table_name_2 b WHERE column_name=value_name; ``` On execution , we get: ```sql +---------------+---------------+ | [ column_name ] | [ column_name ] | +---------------+---------------+ | [ value ] | [ value ] | +---------------+---------------+ ``` Where: | Name | Description | | ----------------------------------- | --------------------------- | | `column_name` | column description | Output of Example 1 \u00b6 SELECT * FROM table_name_1 a JOIN table_name_2 b WHERE column_name = value_name ; On execution, we get: + ---------------+---------------+ | [ column_name ] | [ column_name ] | + ---------------+---------------+ | [ value ] | [ value ] | + ---------------+---------------+ Where: Name Description column_name column description Example 2 \u00b6 ```sql CREATE MODEL mindsdb.predictor_name FROM integration_name (SELECT column_name_1, column_name_2, target_column FROM table_name) PREDICT target_column; ``` On execution, we get: ```sql OUTPUT GOES HERE ``` Output of Example 2 \u00b6 CREATE MODEL mindsdb . predictor_name FROM integration_name ( SELECT column_name_1 , column_name_2 , target_column FROM table_name ) PREDICT target_column ; On execution, we get: OUTPUT GOES HERE","title":"Docs Style Guide"},{"location":"docs-rules/#style-guide-for-mindsdb-documentation","text":"","title":"Style Guide for MindsDB Documentation"},{"location":"docs-rules/#syntax-for-sql-commands","text":"Follow the rules below when writing an SQL command. Add a semi-colon ; at the end of each SQL command. Use all-caps when writing the keywords, such as SELECT , FROM , JOIN , WHERE , GROUP BY , ORDER BY , PREDICT , AS , CREATE TABLE , INSERT INTO , etc. When writing a query, start a new line for the following keywords: SELECT , FROM , JOIN , WHERE , GROUP BY , ORDER BY , PREDICT , USING , AND , OR . It is to avoid the horizontal scrollbar.","title":"Syntax for SQL commands"},{"location":"docs-rules/#example","text":"SELECT * FROM table_name_1 a JOIN table_name_2 b WHERE column_name_1 = value_name_1 AND column_name_2 = value_name_2 GROUP BY a . column_name_2 ORDER BY b . column_name_1 ;","title":"Example"},{"location":"docs-rules/#syntax-for-sql-commands-along-with-their-output","text":"Follow the syntax below when documenting an SQL command and its output. ```sql QUERY GOES HERE ``` On execution , we get: ```sql +---------------+---------------+ | [ column_name ] | [ column_name ] | +---------------+---------------+ | [ value ] | [ value ] | +---------------+---------------+ ``` Where: | Name | Description | | ----------------------------------- | ----------------------------------- | | `VARIABLE NAME GOES HERE` | VARIABLE DESCRIPTION GOES HERE | Note If the output is not a table, remove the output table from above and place your output message there.","title":"Syntax for SQL commands along with their output"},{"location":"docs-rules/#example-1","text":"```sql SELECT * FROM table_name_1 a JOIN table_name_2 b WHERE column_name=value_name; ``` On execution , we get: ```sql +---------------+---------------+ | [ column_name ] | [ column_name ] | +---------------+---------------+ | [ value ] | [ value ] | +---------------+---------------+ ``` Where: | Name | Description | | ----------------------------------- | --------------------------- | | `column_name` | column description |","title":"Example 1"},{"location":"docs-rules/#output-of-example-1","text":"SELECT * FROM table_name_1 a JOIN table_name_2 b WHERE column_name = value_name ; On execution, we get: + ---------------+---------------+ | [ column_name ] | [ column_name ] | + ---------------+---------------+ | [ value ] | [ value ] | + ---------------+---------------+ Where: Name Description column_name column description","title":"Output of Example 1"},{"location":"docs-rules/#example-2","text":"```sql CREATE MODEL mindsdb.predictor_name FROM integration_name (SELECT column_name_1, column_name_2, target_column FROM table_name) PREDICT target_column; ``` On execution, we get: ```sql OUTPUT GOES HERE ```","title":"Example 2"},{"location":"docs-rules/#output-of-example-2","text":"CREATE MODEL mindsdb . predictor_name FROM integration_name ( SELECT column_name_1 , column_name_2 , target_column FROM table_name ) PREDICT target_column ; On execution, we get: OUTPUT GOES HERE","title":"Output of Example 2"},{"location":"documentation-link/","text":"Link for all the Documentation in mindsdb which can help in the QuickStarts, tutorials, learn about API's, etc. https://docs.mindsdb.com/ Hope this was helpfull for the biginners","title":"Documentation link"},{"location":"getting-started/","text":"Getting Started \u00b6 MindsDB can be integrated with the most popular databases, as well as with the DBT and MLFlow workflows. To try out MindsDB right away without bringing in your own data or models, follow our Quickstart Guide . Choose your MindsDB installation path. MindsDB Cloud Docker pip Create your free MindsDB Cloud account . To get started with a Docker installation, follow the MindsDB installation instructions using Docker . You can also install MindsDB from source using pip Windows , Mac , Linux , Open your SQL client and connect to MindsDB. If you do not have a preferred SQL client yet, we recommend using the MindsDB SQL Editor or DBeaver Community Edition . Follow this guide to set up your MindsDB SQL Editor. And here , you'll find how to connect to MindsDB from DBeaver. MindsDB Cloud Docker Create a new MySQL connection. Configure it using the following parameters, as well as the username and password you created above. Host: mysql.mindsdb.com Port: 3306 Database: mindsdb Create a new MySQL connection. Configure it using the following parameters: Host: localhost Port: 47335 Database: mindsdb Username: mindsdb Password: [leave it empty] Connect your data to MindsDB using CREATE DATABASE . You can now preview the available data with a standard SELECT statement. Now you are ready to create your model, using CREATE MODEL . If you already have a model in MLFlow, you can connect to your model. MindsDB is creating my model My model is in MLflow <div id=\"create-predictor\"> <style> #create-predictor code { background-color: #353535; color: #f5f5f5 } </style> mysql> CREATE MODEL mindsdb.home_rentals_predictor -> FROM example_data (select * from demo_data.home_rentals) -> PREDICT rental_price -> USING url.predict='http://host.docker.internal:1234/invocations', -> format='mlflow', -> dtype_dict={\"alcohol\": \"integer\", \"chlorides\": \"integer\", \"citric acid\": \"integer\", \"density\": \"integer\", \"fixed acidity\": \"integer\", \"free sulfur dioxide\": \"integer\", \"pH\": \"integer\", \"residual sugar\": \"integer\", \"sulphates\": \"integer\", \"total sulfur dioxide\": \"integer\", \"volatile acidity\": \"integer\"}; Query OK, 0 rows affected (0.21 sec) </div> The SELECT statement allows you to make predictions based on features. To integrate your predictions into your DBT workflow, you will need to make four changes: profiles.yml schema.yml predicted_rentals.sql dbt_project.yml mindsdb: type: mysql host: mysql.mindsdb.com user: mindsdb.user@example.com password: mindsdbpassword port: 3306 dbname: mindsdb schema: example_data threads: 1 keepalives_idle: 0 # default 0, indicating the system default connect_timeout: 10 # default 10 seconds version: 2 models: - name: predicted_rentals description: \"Integrating MindsDB predictions and historical data\" with predictions as ( SELECT hrp.rental_price as predicted_price, hr.rental_price as actual_price FROM mindsdb.home_rentals_predictor hrp JOIN exampleData.demo_data.home_rentals hr WHERE hr.number_of_bathrooms=2 AND hr.sqft=1000; ) select * from predictions; models: home_rentals: +materialized: view","title":"Getting Started"},{"location":"getting-started/#getting-started","text":"MindsDB can be integrated with the most popular databases, as well as with the DBT and MLFlow workflows. To try out MindsDB right away without bringing in your own data or models, follow our Quickstart Guide . Choose your MindsDB installation path. MindsDB Cloud Docker pip Create your free MindsDB Cloud account . To get started with a Docker installation, follow the MindsDB installation instructions using Docker . You can also install MindsDB from source using pip Windows , Mac , Linux , Open your SQL client and connect to MindsDB. If you do not have a preferred SQL client yet, we recommend using the MindsDB SQL Editor or DBeaver Community Edition . Follow this guide to set up your MindsDB SQL Editor. And here , you'll find how to connect to MindsDB from DBeaver. MindsDB Cloud Docker Create a new MySQL connection. Configure it using the following parameters, as well as the username and password you created above. Host: mysql.mindsdb.com Port: 3306 Database: mindsdb Create a new MySQL connection. Configure it using the following parameters: Host: localhost Port: 47335 Database: mindsdb Username: mindsdb Password: [leave it empty] Connect your data to MindsDB using CREATE DATABASE . You can now preview the available data with a standard SELECT statement. Now you are ready to create your model, using CREATE MODEL . If you already have a model in MLFlow, you can connect to your model. MindsDB is creating my model My model is in MLflow <div id=\"create-predictor\"> <style> #create-predictor code { background-color: #353535; color: #f5f5f5 } </style> mysql> CREATE MODEL mindsdb.home_rentals_predictor -> FROM example_data (select * from demo_data.home_rentals) -> PREDICT rental_price -> USING url.predict='http://host.docker.internal:1234/invocations', -> format='mlflow', -> dtype_dict={\"alcohol\": \"integer\", \"chlorides\": \"integer\", \"citric acid\": \"integer\", \"density\": \"integer\", \"fixed acidity\": \"integer\", \"free sulfur dioxide\": \"integer\", \"pH\": \"integer\", \"residual sugar\": \"integer\", \"sulphates\": \"integer\", \"total sulfur dioxide\": \"integer\", \"volatile acidity\": \"integer\"}; Query OK, 0 rows affected (0.21 sec) </div> The SELECT statement allows you to make predictions based on features. To integrate your predictions into your DBT workflow, you will need to make four changes: profiles.yml schema.yml predicted_rentals.sql dbt_project.yml mindsdb: type: mysql host: mysql.mindsdb.com user: mindsdb.user@example.com password: mindsdbpassword port: 3306 dbname: mindsdb schema: example_data threads: 1 keepalives_idle: 0 # default 0, indicating the system default connect_timeout: 10 # default 10 seconds version: 2 models: - name: predicted_rentals description: \"Integrating MindsDB predictions and historical data\" with predictions as ( SELECT hrp.rental_price as predicted_price, hr.rental_price as actual_price FROM mindsdb.home_rentals_predictor hrp JOIN exampleData.demo_data.home_rentals hr WHERE hr.number_of_bathrooms=2 AND hr.sqft=1000; ) select * from predictions; models: home_rentals: +materialized: view","title":"Getting Started"},{"location":"hacktoberfest-facebook-post/","text":"Show your \ud83d\udc9a for MindsDB on Facebook \u00b6 A Facebook Post by Bilal Aamer A Facebook Post by Teslim Odumuyiwa A Facebook Post by Michael Lantz A Facebook post by Abdulrahman Olamilekan A Facebook post by Dishant Zaveri A Facebook Post by Emmanuel Aiyenigba A Facebook Post by Chemistryloverts(Tushar Sublaik) A Facebook Post by Hritik Dangi A Facebook Post by Alissa Troiano","title":"Show your \ud83d\udc9a for MindsDB on Facebook"},{"location":"hacktoberfest-facebook-post/#show-your-for-mindsdb-on-facebook","text":"A Facebook Post by Bilal Aamer A Facebook Post by Teslim Odumuyiwa A Facebook Post by Michael Lantz A Facebook post by Abdulrahman Olamilekan A Facebook post by Dishant Zaveri A Facebook Post by Emmanuel Aiyenigba A Facebook Post by Chemistryloverts(Tushar Sublaik) A Facebook Post by Hritik Dangi A Facebook Post by Alissa Troiano","title":"Show your \ud83d\udc9a for MindsDB on Facebook"},{"location":"hacktoberfest-linkedin-post/","text":"Show your \ud83d\udc9a for MindsDB on LinkedIn \u00b6 A LinkedIn Post by Akhil Teja A LinkedIn Post by Arman Chand A LinkedIn Post by Bilal Aamer A LinkedIn Post by Michael Lantz A LinkedIn Post by Alissa Troiano A LinkedIn post by Abdulrahman Olamilekan A LinkedIn post by Hell Fire A LinkedIn post by Shrey Parekh A LinkedIn post by Emmanuel Aiyenigba A LinkedIn post by Jeremy Zabala A LinkedIn post by Munyoudoum Pav A LinkedIn post by Angelo Ganaden Jr A LinkedIn post by [Fiona Murugi] (https://www.linkedin.com/posts/fiona-murugi-594012174_introduce-ai-to-your-database-activity-6989693493744963584-oGSO?utm_source=share&utm_medium=member_desktop) A LinkedIn Post by Syed Zubeen A LinkedIn Post by Sarvesh S. Kulkarni A LinkedIn post by Gloria Okeke E.J A LinkedIn Post by Hritik Dangi A LinkedIn Post by Ferdi Gueran A LinkedIn Post by Rutam Prita Mishra","title":"Show your \ud83d\udc9a for MindsDB on LinkedIn"},{"location":"hacktoberfest-linkedin-post/#show-your-for-mindsdb-on-linkedin","text":"A LinkedIn Post by Akhil Teja A LinkedIn Post by Arman Chand A LinkedIn Post by Bilal Aamer A LinkedIn Post by Michael Lantz A LinkedIn Post by Alissa Troiano A LinkedIn post by Abdulrahman Olamilekan A LinkedIn post by Hell Fire A LinkedIn post by Shrey Parekh A LinkedIn post by Emmanuel Aiyenigba A LinkedIn post by Jeremy Zabala A LinkedIn post by Munyoudoum Pav A LinkedIn post by Angelo Ganaden Jr A LinkedIn post by [Fiona Murugi] (https://www.linkedin.com/posts/fiona-murugi-594012174_introduce-ai-to-your-database-activity-6989693493744963584-oGSO?utm_source=share&utm_medium=member_desktop) A LinkedIn Post by Syed Zubeen A LinkedIn Post by Sarvesh S. Kulkarni A LinkedIn post by Gloria Okeke E.J A LinkedIn Post by Hritik Dangi A LinkedIn Post by Ferdi Gueran A LinkedIn Post by Rutam Prita Mishra","title":"Show your \ud83d\udc9a for MindsDB on LinkedIn"},{"location":"hacktoberfest-twitter-post/","text":"Show your \ud83d\udc9a for MindsDB on Twitter \u00b6 A Twitter Thread by Aditya A Twitter Thread by @akhilcoder A Twitter Thread by Arman Chand A Twitter Thread by @TheBilalAamer A Twitter Thread by Don Heshanthaka A Twitter Thread by Michael Lantz A Twitter Thread by reachtoana A Twitter Thread by Mateus Elias A Twitter Thread by Aditya Das A Twitter Thread by Haseeb Siddiqui A Twitter Thread by @DevHellFire A Twitter Thread by Craig Gehrig A Twitter Thread by @talkshrey A Twitter Thread by @alissatroiano A Twitter Thread by @EmmanuelCoder A Twitter Thread by Atharva Shirdhankar A Twitter Thread by @UnrevealRich A Twitter Thread by Tushar Sublaik A Twitter Thread by @FionaMurugi1 A Twitter Thread by @zavbala A Twitter Thread by Syed Zubeen A Twitter Thread by Syed Zubeen A Twitter Thread by Anusha A Twitter Thread by Prakash Gatiyala A Twitter Thread by Prathik Shetty A Twitter Thread by Rutam Prita Mishra","title":"Show your \ud83d\udc9a for MindsDB on Twitter"},{"location":"hacktoberfest-twitter-post/#show-your-for-mindsdb-on-twitter","text":"A Twitter Thread by Aditya A Twitter Thread by @akhilcoder A Twitter Thread by Arman Chand A Twitter Thread by @TheBilalAamer A Twitter Thread by Don Heshanthaka A Twitter Thread by Michael Lantz A Twitter Thread by reachtoana A Twitter Thread by Mateus Elias A Twitter Thread by Aditya Das A Twitter Thread by Haseeb Siddiqui A Twitter Thread by @DevHellFire A Twitter Thread by Craig Gehrig A Twitter Thread by @talkshrey A Twitter Thread by @alissatroiano A Twitter Thread by @EmmanuelCoder A Twitter Thread by Atharva Shirdhankar A Twitter Thread by @UnrevealRich A Twitter Thread by Tushar Sublaik A Twitter Thread by @FionaMurugi1 A Twitter Thread by @zavbala A Twitter Thread by Syed Zubeen A Twitter Thread by Syed Zubeen A Twitter Thread by Anusha A Twitter Thread by Prakash Gatiyala A Twitter Thread by Prathik Shetty A Twitter Thread by Rutam Prita Mishra","title":"Show your \ud83d\udc9a for MindsDB on Twitter"},{"location":"info/","text":"Follow the below steps to get up and running with MindsDB. Steps: \u00b6 Deploy MindsDB: Docker OR Pip OR MindsDB Cloud Connect your Data to MindsDB: Connect to MindsDB like it was a Database using your preferred DB Client Using MindsDB Cloud OR Using a Local Deployment CREATE a predictor (model) trained on your data Make predictions using your model","title":"Getting started"},{"location":"info/#steps","text":"Deploy MindsDB: Docker OR Pip OR MindsDB Cloud Connect your Data to MindsDB: Connect to MindsDB like it was a Database using your preferred DB Client Using MindsDB Cloud OR Using a Local Deployment CREATE a predictor (model) trained on your data Make predictions using your model","title":"Steps:"},{"location":"tutorials/","text":"Community Tutorials \u00b6 Deploying MindsDB on Scaleway Cloud by Rutam Prita Mishra Predict Tesla Stock Prices using MindsDB by Rutam Prita Mishra Creating Views with MindsDB by Rutam Prita Mishra Predicting Logistics Quality using MindsDB by Arman Chand Predict Wine Quality using MindsDB by Rutam Prita Mishra Predicting Employee Attrition & Performance using MindsDB Cloud by Arman Chand Deploying MindsDB on Oracle Cloud by Rutam Prita Mishra Predict Water Quality using MindsDB by Rutam Prita Mishra Predicting Life Expectancy using MindsDB Cloud by Arman Chand Predict Purchasing Power per Country using MindsDB by Rutam Prita Mishra Predicting the Genre of Books with MindsDB by Dipti Sengupta Predicting Data Science Job Salaries using MindsDB Cloud by Rutam Prita Mishra Predicting Used Car Prices using MindsDB by Rutam Prita Mishra Deploying MindsDB on a Digital Ocean Droplet by Rutam Prita Mishra Deploying MindsDB on a Vultr Cloud Instance by Rutam Prita Mishra Deploying MindsDB on Civo Compute by Rutam Prita Mishra Deploying MindsDB on Google Cloud Platform by Rutam Prita Mishra How To Maintain ML-Analytics Cycle With Headless BI by Jan Kadlec How To Visualize MindsDB Predictions with Tableau by Ephraimx Predicting Supermarket Future Sales Using Machine Learning with MindsDB by Ephraimx Predicting Home Rental Prices with MindsDB in Python by Temidayo MindsDB: In-Database Machine Learning by Katrine Gorczany Easy machine learning with MindsDB by Liron Navon MindsDB: Your Introduction to Creating Machine Learning Predictive Models by Chandre Van Der Westhuizen Self-Service Machine Learning with Intelligent Databases by MindsDB Team How To Forecast Purchase Orders for Shopify Stores Using Open-Source by John Lafleur Easy machine learning with MindsDB by Liron Navon MindsDB: Your introduction to creating machine learning predictive models. by Chandre Van Der Westhuizen Machine Learning in 5 Lines with MindsDB by Aman Kharwal Predicting Loan Default using Machine Learning with MindsDB by Eliel Godsent MindsDB - The Superpowers of Machine learning in your regular DataBase by Pratham Prasoon Multivariate time series forecasts inside databases with MindsDB and PyTorch by Patricio Cerda Mardini How to bring your own machine learning model to databases by Patricio Cerda Mardini Introduction to AI Tables by Javi S\u00e1nchez Model agnostic confidence estimation with conformal predictors for AutoML by Patricio Cerda Mardini Predict insurance cost using MindsDB by Kinie K Kusuma Forecast bitcoin price using MindsDB by Kinie K Kusuma Predicting car prices using MindsDB by Marcel Coetzee Predict Gold Prices Using MindsDB by Alissa Troiano Predicting Type of Glass using MindsDB by Ketan Singh MindsDB: Predicting Supply Chain Demand with Machine Learning using SQL by Chandre Van Der Westhuizen Predicting Poker Hand Strength with MindsDB by Teslim Odumuyiwa Why Your Database Needs a Machine Learning Brain by James Wilson Machine Learning for a Shopify store \u2014 a step-by-step guide by James Wilson Predicting Home Rental Prices with MindsDB in Java by Temidayo Predicting Mobile Phone Price using MindsDB Cloud by Arman Chand Predicting Machine Failure Rates using MindsDB by Rutam Prita Mishra Predicting Turbine Yield Energy using MindsDB by DhaneshRagu Predicting Store Sales Data using MindsDB by Kevin Heng Using MindsDB to Predict Future Retail Sales by Alissa Troiano Predicting the Rating of Cars using Mindsdb by Atharva Shirdhankar Predicting & Visualizing Hourly Electricity Demand in the US with MindsDB and Tableau by Teslim Odumuyiwa Bank Customer Churn Prediction using MindsDB by Ayush Sharma Let's connect Mindsdb with Mariadb shell Locally and Predict the Mobile Price by Atharva Shirdhankar Gold price prediction using MindsDB by Pritish Sinha What is MindsDB? by Gloria Okeke E.J Forecasting Honey Production with MindsDB by Adithya Narayan Predicting the Genre of Books using MongoDB with MindsDB by Sarvesh S. Kulkarni Predicting gold prices with MindsDB and MongoDB by yagueto Predicting the Genre of Books using MongoDB with MindsDB by Sarvesh S. Kulkarni Predict Diamond prices with SQL Alchemy by Teslim Odumuyiwa Predict Municipal Debt Risk Analysis by Teslim Odumuyiwa What is MindsDB? How to get started with it by Hritik Dangi How to Design a custom mixer model in Lightwood by Teslim Odumuyiwa Predicting & Visualizing Petroleum Production with MindsDB and Tableau by Teslim Odumuyiwa Predicting & Visualizing Gas Prices with MindsDB and Tableau by Teslim Odumuyiwa Predicting Environment Impact of Food Production caused by C02 Emission by Teslim Odumuyiwa Using MindsDB to Predict Amazon Rainforest Degradation by Alissa Troiano How to Design a custom mixer model in Lightwood by Teslim Odumuyiwa Using MindsDB to Predict Microsoft Stock Prices by Vivek Shah Tutorial to Predict the Weather Using MindsDB and MongoDB by Nupoor Shetye Tutorial to Predict Amsterdam Housing Prices Using MindsDB and MongoDB by Nupoor Shetye Community Video Guides \u00b6 Heart Disease Predictor using #MindsDB by @akhilcoder Video: What is MindsDB? by Alissa Troiano Connecting MindsDB to Tableau by Alissa Troiano Connect Postgres Database to MindsDB by @akhilcoder Exploring Learning Hub on MindsDB by predicting house rental prices and forecasting house sales by @akhilcoder Easily connect to MindsDB Cloud from MongoShell by @akhilcoder Singing Up with MindsDB Cloud uploaded on ExploringTech by Rutam Prita Mishra Toyota Car Price Prediction using MindsDB uploaded on ExploringTech by Rutam Prita Mishra Exploring Data Insights in MindsDB uploaded on ExploringTech by Rutam Prita Mishra Creating a View in MindsDB uploaded on ExploringTech by Rutam Prita Mishra Water Quality Prediction using MindsDB uploaded on ExploringTech by Rutam Prita Mishra Forecasting Microsoft Stock Prices using MindsDB uploaded on ExploringTech by Rutam Prita Mishra Forecasting Tesla Stock Prices using MindsDB uploaded on ExploringTech by Rutam Prita Mishra Wine Quality Prediction using MindsDB uploaded on ExploringTech by Rutam Prita Mishra Deploying MindsDB on IBM Cloud uploaded on ExploringTech by Rutam Prita Mishra Deploying MindsDB on Google Cloud Platform uploaded on ExploringTech by Rutam Prita Mishra Deploying MindsDB on Oracle uploaded on ExploringTech by Rutam Prita Mishra Deploying MindsDB on Vultr Cloud uploaded on ExploringTech by Rutam Prita Mishra Deploying MindsDB on Scaleway Cloud uploaded on ExploringTech by Rutam Prita Mishra Deploying MindsDB on a DigitalOcean Droplet uploaded on ExploringTech by Rutam Prita Mishra How to connect to MindsDB with SQL Alchemy(Python) by Nishant Sapkota What is MindsDB | How to Get Started | A Cloud/AI Enabled Database by Arman Chand How to deploy MindsDB on your local machine (Docker, PIP) by Munyoudoum Pav What are AI Tables and How to use MindsDB ? by @akhilcoder How to Create an Account on MindsDB under 2 minutes \ud83d\udc7e by Chirag8023 What is MindsDB and How to Get Started by Hritik Dangi Setup MindsDB Free Cloud Account in 2 minutes by @akhilcoder What is MindsDB? uploaded on ExploringTech by Rutam Prita Mishra Deploying MindsDB on Civo Cloud Compute uploaded on ExploringTech by Rutam Prita Mishra Tutorial: Create a Free MindsDB Cloud Account by Alissa Troiano What is MindsDB - AI Database Prediction by Bhavesh Mishra How to create an account on MindsDB cloud by HellFire Setting Up Docker for MindsDB by Alissa Troiano How to create a free account on MindsDB Cloud by Anamika How to create a free account on MindsDB Cloud by MichaelLantz Simple starting guide video tutorial for MindsDB by Posterizedsoul How to connect mongo compass to MindsDB by HellFire MindsDB - Predict data with machine learning by HellFire Visualizing prediction result in Tableau by Teslim Odumuyiwa MindsDB Data Insights by @akhilcoder CREATE Predictor with USING statement using Cement Manufacturing Dataset Civil Engineering by Teslim Odumuyiwa What is MindsDB ? by Syed Zubeen Honey Production Prediction Time Series by Narayan Forecasting Quarterly House Sales with MindsDB by Hritik Dangi Forecasting Quarterly House Sales with MindsDB by Adithya Narayan How to access MindsDB by Syed Zubeen Predict house prices with MindsDB by HellFire Deploying an instance of MindsDB on Google Cloud Platform by Syed Zubeen Connecting MindsDB to Tableau by Alissa Troiano Deploy MindsDB on the Google Cloud Platform by Alissa Troiano Regression Tutorial - Home Rentals by Teslim Odumuyiwa Setting up data sources with mindsDB by Syed Zubeen Integrating your MindsDB instance into MongoDB by Syed Zubeen Predicting car prices with MindsDB by Syed Zubeen Predicting predicting Residuary resistance displacement using MindsDB by Dipti Sengupta Forcasting quarterly house sales with MindsDB by HellFire How Mindsdb is impacting the world of Machine Learning by Prathik Shetty","title":"Tutorials List"},{"location":"tutorials/#community-tutorials","text":"Deploying MindsDB on Scaleway Cloud by Rutam Prita Mishra Predict Tesla Stock Prices using MindsDB by Rutam Prita Mishra Creating Views with MindsDB by Rutam Prita Mishra Predicting Logistics Quality using MindsDB by Arman Chand Predict Wine Quality using MindsDB by Rutam Prita Mishra Predicting Employee Attrition & Performance using MindsDB Cloud by Arman Chand Deploying MindsDB on Oracle Cloud by Rutam Prita Mishra Predict Water Quality using MindsDB by Rutam Prita Mishra Predicting Life Expectancy using MindsDB Cloud by Arman Chand Predict Purchasing Power per Country using MindsDB by Rutam Prita Mishra Predicting the Genre of Books with MindsDB by Dipti Sengupta Predicting Data Science Job Salaries using MindsDB Cloud by Rutam Prita Mishra Predicting Used Car Prices using MindsDB by Rutam Prita Mishra Deploying MindsDB on a Digital Ocean Droplet by Rutam Prita Mishra Deploying MindsDB on a Vultr Cloud Instance by Rutam Prita Mishra Deploying MindsDB on Civo Compute by Rutam Prita Mishra Deploying MindsDB on Google Cloud Platform by Rutam Prita Mishra How To Maintain ML-Analytics Cycle With Headless BI by Jan Kadlec How To Visualize MindsDB Predictions with Tableau by Ephraimx Predicting Supermarket Future Sales Using Machine Learning with MindsDB by Ephraimx Predicting Home Rental Prices with MindsDB in Python by Temidayo MindsDB: In-Database Machine Learning by Katrine Gorczany Easy machine learning with MindsDB by Liron Navon MindsDB: Your Introduction to Creating Machine Learning Predictive Models by Chandre Van Der Westhuizen Self-Service Machine Learning with Intelligent Databases by MindsDB Team How To Forecast Purchase Orders for Shopify Stores Using Open-Source by John Lafleur Easy machine learning with MindsDB by Liron Navon MindsDB: Your introduction to creating machine learning predictive models. by Chandre Van Der Westhuizen Machine Learning in 5 Lines with MindsDB by Aman Kharwal Predicting Loan Default using Machine Learning with MindsDB by Eliel Godsent MindsDB - The Superpowers of Machine learning in your regular DataBase by Pratham Prasoon Multivariate time series forecasts inside databases with MindsDB and PyTorch by Patricio Cerda Mardini How to bring your own machine learning model to databases by Patricio Cerda Mardini Introduction to AI Tables by Javi S\u00e1nchez Model agnostic confidence estimation with conformal predictors for AutoML by Patricio Cerda Mardini Predict insurance cost using MindsDB by Kinie K Kusuma Forecast bitcoin price using MindsDB by Kinie K Kusuma Predicting car prices using MindsDB by Marcel Coetzee Predict Gold Prices Using MindsDB by Alissa Troiano Predicting Type of Glass using MindsDB by Ketan Singh MindsDB: Predicting Supply Chain Demand with Machine Learning using SQL by Chandre Van Der Westhuizen Predicting Poker Hand Strength with MindsDB by Teslim Odumuyiwa Why Your Database Needs a Machine Learning Brain by James Wilson Machine Learning for a Shopify store \u2014 a step-by-step guide by James Wilson Predicting Home Rental Prices with MindsDB in Java by Temidayo Predicting Mobile Phone Price using MindsDB Cloud by Arman Chand Predicting Machine Failure Rates using MindsDB by Rutam Prita Mishra Predicting Turbine Yield Energy using MindsDB by DhaneshRagu Predicting Store Sales Data using MindsDB by Kevin Heng Using MindsDB to Predict Future Retail Sales by Alissa Troiano Predicting the Rating of Cars using Mindsdb by Atharva Shirdhankar Predicting & Visualizing Hourly Electricity Demand in the US with MindsDB and Tableau by Teslim Odumuyiwa Bank Customer Churn Prediction using MindsDB by Ayush Sharma Let's connect Mindsdb with Mariadb shell Locally and Predict the Mobile Price by Atharva Shirdhankar Gold price prediction using MindsDB by Pritish Sinha What is MindsDB? by Gloria Okeke E.J Forecasting Honey Production with MindsDB by Adithya Narayan Predicting the Genre of Books using MongoDB with MindsDB by Sarvesh S. Kulkarni Predicting gold prices with MindsDB and MongoDB by yagueto Predicting the Genre of Books using MongoDB with MindsDB by Sarvesh S. Kulkarni Predict Diamond prices with SQL Alchemy by Teslim Odumuyiwa Predict Municipal Debt Risk Analysis by Teslim Odumuyiwa What is MindsDB? How to get started with it by Hritik Dangi How to Design a custom mixer model in Lightwood by Teslim Odumuyiwa Predicting & Visualizing Petroleum Production with MindsDB and Tableau by Teslim Odumuyiwa Predicting & Visualizing Gas Prices with MindsDB and Tableau by Teslim Odumuyiwa Predicting Environment Impact of Food Production caused by C02 Emission by Teslim Odumuyiwa Using MindsDB to Predict Amazon Rainforest Degradation by Alissa Troiano How to Design a custom mixer model in Lightwood by Teslim Odumuyiwa Using MindsDB to Predict Microsoft Stock Prices by Vivek Shah Tutorial to Predict the Weather Using MindsDB and MongoDB by Nupoor Shetye Tutorial to Predict Amsterdam Housing Prices Using MindsDB and MongoDB by Nupoor Shetye","title":"Community Tutorials"},{"location":"tutorials/#community-video-guides","text":"Heart Disease Predictor using #MindsDB by @akhilcoder Video: What is MindsDB? by Alissa Troiano Connecting MindsDB to Tableau by Alissa Troiano Connect Postgres Database to MindsDB by @akhilcoder Exploring Learning Hub on MindsDB by predicting house rental prices and forecasting house sales by @akhilcoder Easily connect to MindsDB Cloud from MongoShell by @akhilcoder Singing Up with MindsDB Cloud uploaded on ExploringTech by Rutam Prita Mishra Toyota Car Price Prediction using MindsDB uploaded on ExploringTech by Rutam Prita Mishra Exploring Data Insights in MindsDB uploaded on ExploringTech by Rutam Prita Mishra Creating a View in MindsDB uploaded on ExploringTech by Rutam Prita Mishra Water Quality Prediction using MindsDB uploaded on ExploringTech by Rutam Prita Mishra Forecasting Microsoft Stock Prices using MindsDB uploaded on ExploringTech by Rutam Prita Mishra Forecasting Tesla Stock Prices using MindsDB uploaded on ExploringTech by Rutam Prita Mishra Wine Quality Prediction using MindsDB uploaded on ExploringTech by Rutam Prita Mishra Deploying MindsDB on IBM Cloud uploaded on ExploringTech by Rutam Prita Mishra Deploying MindsDB on Google Cloud Platform uploaded on ExploringTech by Rutam Prita Mishra Deploying MindsDB on Oracle uploaded on ExploringTech by Rutam Prita Mishra Deploying MindsDB on Vultr Cloud uploaded on ExploringTech by Rutam Prita Mishra Deploying MindsDB on Scaleway Cloud uploaded on ExploringTech by Rutam Prita Mishra Deploying MindsDB on a DigitalOcean Droplet uploaded on ExploringTech by Rutam Prita Mishra How to connect to MindsDB with SQL Alchemy(Python) by Nishant Sapkota What is MindsDB | How to Get Started | A Cloud/AI Enabled Database by Arman Chand How to deploy MindsDB on your local machine (Docker, PIP) by Munyoudoum Pav What are AI Tables and How to use MindsDB ? by @akhilcoder How to Create an Account on MindsDB under 2 minutes \ud83d\udc7e by Chirag8023 What is MindsDB and How to Get Started by Hritik Dangi Setup MindsDB Free Cloud Account in 2 minutes by @akhilcoder What is MindsDB? uploaded on ExploringTech by Rutam Prita Mishra Deploying MindsDB on Civo Cloud Compute uploaded on ExploringTech by Rutam Prita Mishra Tutorial: Create a Free MindsDB Cloud Account by Alissa Troiano What is MindsDB - AI Database Prediction by Bhavesh Mishra How to create an account on MindsDB cloud by HellFire Setting Up Docker for MindsDB by Alissa Troiano How to create a free account on MindsDB Cloud by Anamika How to create a free account on MindsDB Cloud by MichaelLantz Simple starting guide video tutorial for MindsDB by Posterizedsoul How to connect mongo compass to MindsDB by HellFire MindsDB - Predict data with machine learning by HellFire Visualizing prediction result in Tableau by Teslim Odumuyiwa MindsDB Data Insights by @akhilcoder CREATE Predictor with USING statement using Cement Manufacturing Dataset Civil Engineering by Teslim Odumuyiwa What is MindsDB ? by Syed Zubeen Honey Production Prediction Time Series by Narayan Forecasting Quarterly House Sales with MindsDB by Hritik Dangi Forecasting Quarterly House Sales with MindsDB by Adithya Narayan How to access MindsDB by Syed Zubeen Predict house prices with MindsDB by HellFire Deploying an instance of MindsDB on Google Cloud Platform by Syed Zubeen Connecting MindsDB to Tableau by Alissa Troiano Deploy MindsDB on the Google Cloud Platform by Alissa Troiano Regression Tutorial - Home Rentals by Teslim Odumuyiwa Setting up data sources with mindsDB by Syed Zubeen Integrating your MindsDB instance into MongoDB by Syed Zubeen Predicting car prices with MindsDB by Syed Zubeen Predicting predicting Residuary resistance displacement using MindsDB by Dipti Sengupta Forcasting quarterly house sales with MindsDB by HellFire How Mindsdb is impacting the world of Machine Learning by Prathik Shetty","title":"Community Video Guides"},{"location":"what-is-mindsdb/","text":"What is MindsDB? \u00b6 Data that lives in your database is a valuable asset. MindsDB enables you to use your data and make forecasts. It speeds up the ML development process by bringing machine learning into the database. With MindsDB, you can build, train, optimize, and deploy your ML models without the need for other platforms. And to get the forecasts, simply query your data and ML models. Read along to see some examples. What are AI Tables? \u00b6 MindsDB brings machine learning into databases by employing the concept of AI Tables. AI Tables are machine learning models stored as virtual tables inside a database. They facilitate making predictions based on your data. You can perform the time series, regression, and classification predictions within your database and get the output almost instantly by querying an AI Table with simple SQL statements. Deep Dive into the AI Tables \u00b6 Current Challenges \u00b6 Let\u2019s consider the following income_table table that stores the income and debt values. SELECT income , debt FROM income_table ; On execution, we get: + ------+-----+ | income | debt | + ------+-----+ | 60000 | 20000 | | 80000 | 25100 | | 100000 | 30040 | | 120000 | 36010 | + ------+-----+ A simple visualization of the data present in the income_table table is as follows: Querying the income table to get the debt value for a particular income value results in the following: SELECT income , debt FROM income_table WHERE income = 80000 ; On execution, we get: + ------+-----+ | income | debt | + ------+-----+ | 80000 | 25100 | + ------+-----+ And here is what we get: But what happens when querying the table for an income value that is not present there? SELECT income , debt FROM income_table WHERE income = 90000 ; On execution, we get: Empty set ( 0 . 00 sec ) When the WHERE clause condition is not fulfilled for any of the rows, no value is returned. When a table doesn\u2019t have an exact match, the query returns an empty set or null value. This is where the AI Tables come into play! Solution Offered by MindsDB \u00b6 Let\u2019s create a debt_model model that allows us to approximate the debt value for any income value. We train the debt_model model using the data from the income_table table. CREATE MODEL mindsdb . debt_model FROM income_table PREDICT debt ; On execution, we get: Query OK , 0 rows affected ( x . xxx sec ) MindsDB provides the CREATE MODEL statement. On execution of this statement, the predictive model works in the background, automatically creating a vector representation of the data that can be visualized as follows: Let\u2019s now look for the debt value of some random income value. To get the approximated debt value, we query the mindsdb . debt_model model instead of the income_table table. SELECT income , debt FROM mindsdb . debt_model WHERE income = 90000 ; On execution, we get: + ------+-----+ | income | debt | + ------+-----+ | 90000 | 27820 | + ------+-----+ And here is how it looks: Why Choose MindsDB? \u00b6 Shift to Data Analysis Paradigm \u00b6 There is an ongoing transformational shift within the modern business world from the \u201cwhat happened and why\u201d based on historical data analysis to the \u201cwhat will happen and how can we make it happen\u201d based on machine learning predictive modeling. The success of your predictions depends both on the data you have available and the models trained with the data. Data Scientists and Data Engineers require efficient and easy-to-use tools to prepare the data for feature engineering, then training the models, and finally, deploying, monitoring, and managing these implementations for optimal prediction confidence. The Machine Learning Lifecycle \u00b6 The ML lifecycle is a process that consists of the data preparation phase, modeling phase, and deployment phase. The diagram below presents all the steps included in each of the stages. Current solutions for implementing machine learning encounter various challenges, such as time-consuming preparation, cleaning, and labeling of substantial amouts of data, and difficulties in finding qualified ML/AI data scientists. The processes that must be followed by the ML/AI data scientists to implement machine learning include the following: - feature engineering, - building, training, and optimizing models, - assembling, verifying, and deploying models to production, - continuously monitoring and improving the models, - continuously training the models, as they require multiple training iterations with existing data, - extracting, transforming, and loading (ETL) data from one system to another, which is complicated and may lead to multiple copies of information. A recent study has shown it takes 64% of companies a month up to over a year to deploy a machine learning model into production. Leveraging existing databases and automating all the aforementioned processes is called AutoML. AutoML has been gaining traction within enterprises for enabling non-experts to use machine learning models for practical applications. Why MindsDB? \u00b6 Well, as with most names, we needed one. We like science fiction and The Culture series, where the AI super-smart entities are called Minds . So that's for the first part of our name. As for the second part - the DB , it is quite self-explanatory. Although we will support all kinds of data in the future, but currently, our objective is to add intelligence to existing data stores and databases. Hence, the term DB comes along. So there we have it, MindsDB. And why the bear? We wanted to honor the open-source tradition of animals related to projects. We went for a bear because MindsDB was born at UC Berkeley, where the first codes were written. Then, we went a step further and decided for a polar bear. How to Help Democratize Machine Learning? \u00b6 Here is what you can do: Go ahead and try out MindsDB by following our tutorials, and in case of problems, you can always report an issue here . Are you familiar with Python? You can then help us out in resolving open issues. At first, have a look at issues labeled with the good first issue tag , as these should be easy to start. You can also help us with documentation and tutorials. Here is how you can contribute by writing documentation and tutorials . Don't forget to follow the style guide . Share with your friends and spread the word about MindsDB. Join our team! We are a fast-growing company, so we always have a few open positions .","title":"What is MindsDB?"},{"location":"what-is-mindsdb/#what-is-mindsdb","text":"Data that lives in your database is a valuable asset. MindsDB enables you to use your data and make forecasts. It speeds up the ML development process by bringing machine learning into the database. With MindsDB, you can build, train, optimize, and deploy your ML models without the need for other platforms. And to get the forecasts, simply query your data and ML models. Read along to see some examples.","title":"What is MindsDB?"},{"location":"what-is-mindsdb/#what-are-ai-tables","text":"MindsDB brings machine learning into databases by employing the concept of AI Tables. AI Tables are machine learning models stored as virtual tables inside a database. They facilitate making predictions based on your data. You can perform the time series, regression, and classification predictions within your database and get the output almost instantly by querying an AI Table with simple SQL statements.","title":"What are AI Tables?"},{"location":"what-is-mindsdb/#deep-dive-into-the-ai-tables","text":"","title":"Deep Dive into the AI Tables"},{"location":"what-is-mindsdb/#current-challenges","text":"Let\u2019s consider the following income_table table that stores the income and debt values. SELECT income , debt FROM income_table ; On execution, we get: + ------+-----+ | income | debt | + ------+-----+ | 60000 | 20000 | | 80000 | 25100 | | 100000 | 30040 | | 120000 | 36010 | + ------+-----+ A simple visualization of the data present in the income_table table is as follows: Querying the income table to get the debt value for a particular income value results in the following: SELECT income , debt FROM income_table WHERE income = 80000 ; On execution, we get: + ------+-----+ | income | debt | + ------+-----+ | 80000 | 25100 | + ------+-----+ And here is what we get: But what happens when querying the table for an income value that is not present there? SELECT income , debt FROM income_table WHERE income = 90000 ; On execution, we get: Empty set ( 0 . 00 sec ) When the WHERE clause condition is not fulfilled for any of the rows, no value is returned. When a table doesn\u2019t have an exact match, the query returns an empty set or null value. This is where the AI Tables come into play!","title":"Current Challenges"},{"location":"what-is-mindsdb/#solution-offered-by-mindsdb","text":"Let\u2019s create a debt_model model that allows us to approximate the debt value for any income value. We train the debt_model model using the data from the income_table table. CREATE MODEL mindsdb . debt_model FROM income_table PREDICT debt ; On execution, we get: Query OK , 0 rows affected ( x . xxx sec ) MindsDB provides the CREATE MODEL statement. On execution of this statement, the predictive model works in the background, automatically creating a vector representation of the data that can be visualized as follows: Let\u2019s now look for the debt value of some random income value. To get the approximated debt value, we query the mindsdb . debt_model model instead of the income_table table. SELECT income , debt FROM mindsdb . debt_model WHERE income = 90000 ; On execution, we get: + ------+-----+ | income | debt | + ------+-----+ | 90000 | 27820 | + ------+-----+ And here is how it looks:","title":"Solution Offered by MindsDB"},{"location":"what-is-mindsdb/#why-choose-mindsdb","text":"","title":"Why Choose MindsDB?"},{"location":"what-is-mindsdb/#shift-to-data-analysis-paradigm","text":"There is an ongoing transformational shift within the modern business world from the \u201cwhat happened and why\u201d based on historical data analysis to the \u201cwhat will happen and how can we make it happen\u201d based on machine learning predictive modeling. The success of your predictions depends both on the data you have available and the models trained with the data. Data Scientists and Data Engineers require efficient and easy-to-use tools to prepare the data for feature engineering, then training the models, and finally, deploying, monitoring, and managing these implementations for optimal prediction confidence.","title":"Shift to Data Analysis Paradigm"},{"location":"what-is-mindsdb/#the-machine-learning-lifecycle","text":"The ML lifecycle is a process that consists of the data preparation phase, modeling phase, and deployment phase. The diagram below presents all the steps included in each of the stages. Current solutions for implementing machine learning encounter various challenges, such as time-consuming preparation, cleaning, and labeling of substantial amouts of data, and difficulties in finding qualified ML/AI data scientists. The processes that must be followed by the ML/AI data scientists to implement machine learning include the following: - feature engineering, - building, training, and optimizing models, - assembling, verifying, and deploying models to production, - continuously monitoring and improving the models, - continuously training the models, as they require multiple training iterations with existing data, - extracting, transforming, and loading (ETL) data from one system to another, which is complicated and may lead to multiple copies of information. A recent study has shown it takes 64% of companies a month up to over a year to deploy a machine learning model into production. Leveraging existing databases and automating all the aforementioned processes is called AutoML. AutoML has been gaining traction within enterprises for enabling non-experts to use machine learning models for practical applications.","title":"The Machine Learning Lifecycle"},{"location":"what-is-mindsdb/#why-mindsdb","text":"Well, as with most names, we needed one. We like science fiction and The Culture series, where the AI super-smart entities are called Minds . So that's for the first part of our name. As for the second part - the DB , it is quite self-explanatory. Although we will support all kinds of data in the future, but currently, our objective is to add intelligence to existing data stores and databases. Hence, the term DB comes along. So there we have it, MindsDB. And why the bear? We wanted to honor the open-source tradition of animals related to projects. We went for a bear because MindsDB was born at UC Berkeley, where the first codes were written. Then, we went a step further and decided for a polar bear.","title":"Why MindsDB?"},{"location":"what-is-mindsdb/#how-to-help-democratize-machine-learning","text":"Here is what you can do: Go ahead and try out MindsDB by following our tutorials, and in case of problems, you can always report an issue here . Are you familiar with Python? You can then help us out in resolving open issues. At first, have a look at issues labeled with the good first issue tag , as these should be easy to start. You can also help us with documentation and tutorials. Here is how you can contribute by writing documentation and tutorials . Don't forget to follow the style guide . Share with your friends and spread the word about MindsDB. Join our team! We are a fast-growing company, so we always have a few open positions .","title":"How to Help Democratize Machine Learning?"},{"location":"connect/connect-mariadb-skysql/","text":"MariaDB SkySQL Setup Guide \u00b6 1. Select your service for MindsDB \u00b6 If you haven\u2019t already, identify the service to be enabled with MindsDB and make sure it is running. Otherwise skip to step 2. 2. Add MindsDB to your service Allowlist \u00b6 Access to MariaDB SkySQL services is restricted on a per-service basis . Add the following IP addresses to allow MindsDB to connect to your MariaDB service, do this by clicking on the cog icon and navigating to Security Access. In the dialog, input as prompted \u2013 one by one \u2013 the following IPs: 18.220.205.95 3.19.152.46 52.14.91.162 3. Download your service .pem file \u00b6 A certificate authority chain (.pem file) must be provided for proper TLS certificate validation. From your selected service, click on the world globe icon (Connect to service). In the Login Credentials section, click Download. The aws_skysql_chain.pem file will download onto your machine. 4. Publically Expose your service .pem File \u00b6 Select secure storage for the aws_skysql_chain.pem file that allows a working public URL or localpath. 5. Link MindsDB to your MariaDB SkySQL Service \u00b6 To print the query template, select Add Data in either the top or side navigation and choose MariaDB SkySQL from the list. Fill in the values and run query to complete the setup. Template Example for MariaDB SkySQL Service CREATE DATABASE maria_datasource --- display name for the database WITH ENGINE = 'MariaDB' , --- name of the MindsDB handler PARAMETERS = { \"host\" : \" \" , --- host IP address or URL \"port\" : , --- port used to make TCP/IP connection \"database\" : \" \" , --- database name \"user\" : \" \" , --- database user \"password\" : \" \" , --- database password \"ssl\" : True / False , --- optional, the `ssl` parameter value indicates whether SSL is enabled (`True`) or disabled (`False`) \"ssl_ca\" : { --- optional, SSL Certificate Authority \"path\" : \" \" --- either \"path\" or \"url\" } , \"ssl_cert\" : { --- optional, SSL certificates \"url\" : \" \" --- either \"path\" or \"url\" } , \"ssl_key\" : { --- optional, SSL keys \"path\" : \" \" --- either \"path\" or \"url\" } } ; CREATE DATABASE skysql_datasource WITH ENGINE = 'MariaDB' , PARAMETERS = { \"host\" : \"mindsdbtest.mdb0002956.db1.skysql.net\" , \"port\" : 5001 , \"database\" : \"mindsdb_data\" , \"user\" : \"DB00007539\" , \"password\" : \"password\" , --- here, the SSL certificate is required \"ssl-ca\" : { \"url\" : \"https://mindsdb-web-builds.s3.amazonaws.com/aws_skysql_chain.pem\" } } ; What's Next? \u00b6 Now that you are all set, we recommend you check out our Tutorials and Community Tutorials sections, where you'll find various examples of regression, classification, and time series predictions with MindsDB. To learn more about MindsDB itself, follow the guide on MindsDB database structure . Also, don't miss out on the remaining pages from the SQL API section, as they explain a common SQL syntax with examples. Have fun!","title":"MariaDB SkySQL"},{"location":"connect/connect-mariadb-skysql/#mariadb-skysql-setup-guide","text":"","title":"MariaDB SkySQL Setup Guide"},{"location":"connect/connect-mariadb-skysql/#1-select-your-service-for-mindsdb","text":"If you haven\u2019t already, identify the service to be enabled with MindsDB and make sure it is running. Otherwise skip to step 2.","title":"1. Select your service for MindsDB"},{"location":"connect/connect-mariadb-skysql/#2-add-mindsdb-to-your-service-allowlist","text":"Access to MariaDB SkySQL services is restricted on a per-service basis . Add the following IP addresses to allow MindsDB to connect to your MariaDB service, do this by clicking on the cog icon and navigating to Security Access. In the dialog, input as prompted \u2013 one by one \u2013 the following IPs: 18.220.205.95 3.19.152.46 52.14.91.162","title":"2. Add MindsDB to your service Allowlist"},{"location":"connect/connect-mariadb-skysql/#3-download-your-service-pem-file","text":"A certificate authority chain (.pem file) must be provided for proper TLS certificate validation. From your selected service, click on the world globe icon (Connect to service). In the Login Credentials section, click Download. The aws_skysql_chain.pem file will download onto your machine.","title":"3. Download your service .pem file"},{"location":"connect/connect-mariadb-skysql/#4-publically-expose-your-service-pem-file","text":"Select secure storage for the aws_skysql_chain.pem file that allows a working public URL or localpath.","title":"4. Publically Expose your service .pem File"},{"location":"connect/connect-mariadb-skysql/#5-link-mindsdb-to-your-mariadb-skysql-service","text":"To print the query template, select Add Data in either the top or side navigation and choose MariaDB SkySQL from the list. Fill in the values and run query to complete the setup. Template Example for MariaDB SkySQL Service CREATE DATABASE maria_datasource --- display name for the database WITH ENGINE = 'MariaDB' , --- name of the MindsDB handler PARAMETERS = { \"host\" : \" \" , --- host IP address or URL \"port\" : , --- port used to make TCP/IP connection \"database\" : \" \" , --- database name \"user\" : \" \" , --- database user \"password\" : \" \" , --- database password \"ssl\" : True / False , --- optional, the `ssl` parameter value indicates whether SSL is enabled (`True`) or disabled (`False`) \"ssl_ca\" : { --- optional, SSL Certificate Authority \"path\" : \" \" --- either \"path\" or \"url\" } , \"ssl_cert\" : { --- optional, SSL certificates \"url\" : \" \" --- either \"path\" or \"url\" } , \"ssl_key\" : { --- optional, SSL keys \"path\" : \" \" --- either \"path\" or \"url\" } } ; CREATE DATABASE skysql_datasource WITH ENGINE = 'MariaDB' , PARAMETERS = { \"host\" : \"mindsdbtest.mdb0002956.db1.skysql.net\" , \"port\" : 5001 , \"database\" : \"mindsdb_data\" , \"user\" : \"DB00007539\" , \"password\" : \"password\" , --- here, the SSL certificate is required \"ssl-ca\" : { \"url\" : \"https://mindsdb-web-builds.s3.amazonaws.com/aws_skysql_chain.pem\" } } ;","title":"5. Link MindsDB to your MariaDB SkySQL Service"},{"location":"connect/connect-mariadb-skysql/#whats-next","text":"Now that you are all set, we recommend you check out our Tutorials and Community Tutorials sections, where you'll find various examples of regression, classification, and time series predictions with MindsDB. To learn more about MindsDB itself, follow the guide on MindsDB database structure . Also, don't miss out on the remaining pages from the SQL API section, as they explain a common SQL syntax with examples. Have fun!","title":"What's Next?"},{"location":"connect/dbeaver/","text":"MindsDB and DBeaver \u00b6 DBeaver is a database tool that allows you to connect to and work with various database engines. You can download it here . Data Setup \u00b6 First, create a new database connection in DBeaver by clicking the icon, as shown below. Next, choose the MySQL database engine and click the Next button. Now it's time to fill in the connection details. There are two options, as below. Connecting to your MindsDB Cloud account Connecting to your local MindsDB You can connect to your MindsDB Cloud account. To do that, please use the connection details below: Hostname: `cloud.mindsdb.com` Port: `3306` Username: *your MindsDB Cloud username* # your Mindsdb Cloud email address is your username Password: *your MindsDB Cloud password* Database: *leave it empty* You can connect to your local MindsDB. To do that, please use the connection details below: Hostname: `127.0.0.1` Port: `47334` Username: `mindsdb` Password: *leave it empty* Database: *leave it empty* Now we are ready to test the connection. Testing the Connection \u00b6 Click on the Test Connection... button to check if all the provided data allows you to connect to MindsDB. On success, you should see the message, as below. Let's Run Some Queries \u00b6 To finally make sure that our MinsdDB database connection works, let's run some queries. SELECT * FROM mindsdb . models ; On execution, we get: + -------------------------+--------+------------------+------------------+-------------+---------------+-----+-----------------+----------------+ | name | status | accuracy | predict | update_status | mindsdb_version | error | select_data_query | training_options | + -------------------------+--------+------------------+------------------+-------------+---------------+-----+-----------------+----------------+ | house_sales_model | complete | 0 . 4658770134240238 | ma | up_to_date | 22 . 7 . 5 . 1 | | | | | process_quality_predictor | complete | 1 . 0 | silica_concentrate | up_to_date | 22 . 7 . 5 . 1 | | | | | home_rentals_model | complete | 0 . 9991920992432087 | rental_price | up_to_date | 22 . 7 . 4 . 0 | | | | + -------------------------+--------+------------------+------------------+-------------+---------------+-----+-----------------+----------------+ Here is how it looks in DBeaver: Whitelist MindsDB Cloud IP address If you need to whitelist the MindsDB Cloud IP address to gain access to your database, reach out to the MindsDB team, and we'll share the MindsDB Cloud static IP address with you. What's Next? \u00b6 Now that you are all set, we recommend you check out our Tutorials and Community Tutorials sections, where you'll find various examples of regression, classification, and time series predictions with MindsDB. To learn more about MindsDB itself, follow the guide on MindsDB database structure . Also, don't miss out on the remaining pages from the SQL API section, as they explain a common SQL syntax with examples. Have fun!","title":"DBeaver"},{"location":"connect/dbeaver/#mindsdb-and-dbeaver","text":"DBeaver is a database tool that allows you to connect to and work with various database engines. You can download it here .","title":"MindsDB and DBeaver"},{"location":"connect/dbeaver/#data-setup","text":"First, create a new database connection in DBeaver by clicking the icon, as shown below. Next, choose the MySQL database engine and click the Next button. Now it's time to fill in the connection details. There are two options, as below. Connecting to your MindsDB Cloud account Connecting to your local MindsDB You can connect to your MindsDB Cloud account. To do that, please use the connection details below: Hostname: `cloud.mindsdb.com` Port: `3306` Username: *your MindsDB Cloud username* # your Mindsdb Cloud email address is your username Password: *your MindsDB Cloud password* Database: *leave it empty* You can connect to your local MindsDB. To do that, please use the connection details below: Hostname: `127.0.0.1` Port: `47334` Username: `mindsdb` Password: *leave it empty* Database: *leave it empty* Now we are ready to test the connection.","title":"Data Setup"},{"location":"connect/dbeaver/#testing-the-connection","text":"Click on the Test Connection... button to check if all the provided data allows you to connect to MindsDB. On success, you should see the message, as below.","title":"Testing the Connection"},{"location":"connect/dbeaver/#lets-run-some-queries","text":"To finally make sure that our MinsdDB database connection works, let's run some queries. SELECT * FROM mindsdb . models ; On execution, we get: + -------------------------+--------+------------------+------------------+-------------+---------------+-----+-----------------+----------------+ | name | status | accuracy | predict | update_status | mindsdb_version | error | select_data_query | training_options | + -------------------------+--------+------------------+------------------+-------------+---------------+-----+-----------------+----------------+ | house_sales_model | complete | 0 . 4658770134240238 | ma | up_to_date | 22 . 7 . 5 . 1 | | | | | process_quality_predictor | complete | 1 . 0 | silica_concentrate | up_to_date | 22 . 7 . 5 . 1 | | | | | home_rentals_model | complete | 0 . 9991920992432087 | rental_price | up_to_date | 22 . 7 . 4 . 0 | | | | + -------------------------+--------+------------------+------------------+-------------+---------------+-----+-----------------+----------------+ Here is how it looks in DBeaver: Whitelist MindsDB Cloud IP address If you need to whitelist the MindsDB Cloud IP address to gain access to your database, reach out to the MindsDB team, and we'll share the MindsDB Cloud static IP address with you.","title":"Let's Run Some Queries"},{"location":"connect/dbeaver/#whats-next","text":"Now that you are all set, we recommend you check out our Tutorials and Community Tutorials sections, where you'll find various examples of regression, classification, and time series predictions with MindsDB. To learn more about MindsDB itself, follow the guide on MindsDB database structure . Also, don't miss out on the remaining pages from the SQL API section, as they explain a common SQL syntax with examples. Have fun!","title":"What's Next?"},{"location":"connect/dbt/","text":"MindsDB and DBT \u00b6 DBT is a development framework that facilitates data transformation processes. You can read more here . Usage \u00b6 Installing Adapter for DBT \u00b6 MindsDB provides an adapter to integrate your predictions into the DBT workflow. To find out more about the dbt-mindsdb adapter, follow the instructions here . You can install the dbt-mindsdb adapter by executing this command: pip install dbt-mindsdb . Profile Setup \u00b6 Create a DBT project: dbt init [ project_name ] Configure your profiles.yml file: What MindsDB Supports Currently, MindsDB supports only user/password authentication. Please see below ~/.dbt/profiles.yml file for details. Self-Hosted Local Deployment MindsDB Cloud mindsdb: outputs: dev: type: mindsdb database: 'mindsdb' host: '127.0.0.1' port: 47335 schema: 'mindsdb' username: 'mindsdb' password: '' target: dev mindsdb: outputs: dev: type: mindsdb database: 'mindsdb' host: 'cloud.mindsdb.com' port: 3306 schema: '[dbt schema]' username: '[mindsdb cloud username]' # your Mindsdb Cloud email address is your username password: '[mindsdb cloud password]' target: dev Let's list the required data fields. Key Required Description Example type \u2714\ufe0f The adapter name mindsdb database \u2714\ufe0f The database name mindsdb host \u2714\ufe0f The MindsDB (hostname) to connect to cloud.mindsdb.com port \u2714\ufe0f The port to be used 47335 or 3306 schema \u2714\ufe0f The schema (database) where models are built The MindsDB data source username \u2714\ufe0f The username to be used to connect to the server mindsdb or MindsDB Cloud user password \u2714\ufe0f The password to be used for authentication local password or MindsDB Cloud password Creating a Predictor \u00b6 Create a table_name.sql file where table_name is the name of the predictor. {{ config ( materialized = 'predictor' , integration = 'photorep' , predict = 'name' , predict_alias = 'predicted_name' , using = { 'encoders.location.module' : 'CategoricalAutoEncoder' , 'encoders.rental_price.module' : 'NumericEncoder' } ) }} SELECT * FROM stores ; Let's list the required and optional data fields. Parameter Required Description Example materialized \u2714\ufe0f Use always the value predictor predictor integration \u2714\ufe0f Create an integration in MindsDB that is used to get the data from and save the result to photorep predict \u2714\ufe0f Field to be predicted name predict_alias Alias for predicted field predicted_name using Configuration options for trained model ... Creating Predictions Table \u00b6 Create a table_name.sql file where table_name is used as the name of the predictor. Or you can create a schema_name.table_name.sql file where schema_name is the name of the integration and table_name is the name of the predictor. {{ config ( materialized = 'table' , predictor_name = 'store_predictor' , integration = 'photorep' ) }} SELECT a , bc FROM ddd WHERE name > latest ; Let's list the required data fields. Parameter Required Description Example materialized \u2714\ufe0f Use always the value table table predictor_name \u2714\ufe0f Name of the predictor model from CREATE MODEL store_predictor integration \u2714\ufe0f Create an integration in MindsDB that is used to get the data from and save the result to photorep Note that each time DBT is run, the results table is overwritten. Testing \u00b6 Install dev requirements: pip install -r dev_requirements.txt Run tests: python -m pytest tests/ What's Next? \u00b6 Now that you are all set, we recommend you check out our Tutorials and Community Tutorials sections, where you'll find various examples of regression, classification, and time series predictions with MindsDB. To learn more about MindsDB itself, follow the guide on MindsDB database structure . Also, don't miss out on the remaining pages from the SQL API section, as they explain a common SQL syntax with examples. Have fun!","title":"DBT"},{"location":"connect/dbt/#mindsdb-and-dbt","text":"DBT is a development framework that facilitates data transformation processes. You can read more here .","title":"MindsDB and DBT"},{"location":"connect/dbt/#usage","text":"","title":"Usage"},{"location":"connect/dbt/#installing-adapter-for-dbt","text":"MindsDB provides an adapter to integrate your predictions into the DBT workflow. To find out more about the dbt-mindsdb adapter, follow the instructions here . You can install the dbt-mindsdb adapter by executing this command: pip install dbt-mindsdb .","title":"Installing Adapter for DBT"},{"location":"connect/dbt/#profile-setup","text":"Create a DBT project: dbt init [ project_name ] Configure your profiles.yml file: What MindsDB Supports Currently, MindsDB supports only user/password authentication. Please see below ~/.dbt/profiles.yml file for details. Self-Hosted Local Deployment MindsDB Cloud mindsdb: outputs: dev: type: mindsdb database: 'mindsdb' host: '127.0.0.1' port: 47335 schema: 'mindsdb' username: 'mindsdb' password: '' target: dev mindsdb: outputs: dev: type: mindsdb database: 'mindsdb' host: 'cloud.mindsdb.com' port: 3306 schema: '[dbt schema]' username: '[mindsdb cloud username]' # your Mindsdb Cloud email address is your username password: '[mindsdb cloud password]' target: dev Let's list the required data fields. Key Required Description Example type \u2714\ufe0f The adapter name mindsdb database \u2714\ufe0f The database name mindsdb host \u2714\ufe0f The MindsDB (hostname) to connect to cloud.mindsdb.com port \u2714\ufe0f The port to be used 47335 or 3306 schema \u2714\ufe0f The schema (database) where models are built The MindsDB data source username \u2714\ufe0f The username to be used to connect to the server mindsdb or MindsDB Cloud user password \u2714\ufe0f The password to be used for authentication local password or MindsDB Cloud password","title":"Profile Setup"},{"location":"connect/dbt/#creating-a-predictor","text":"Create a table_name.sql file where table_name is the name of the predictor. {{ config ( materialized = 'predictor' , integration = 'photorep' , predict = 'name' , predict_alias = 'predicted_name' , using = { 'encoders.location.module' : 'CategoricalAutoEncoder' , 'encoders.rental_price.module' : 'NumericEncoder' } ) }} SELECT * FROM stores ; Let's list the required and optional data fields. Parameter Required Description Example materialized \u2714\ufe0f Use always the value predictor predictor integration \u2714\ufe0f Create an integration in MindsDB that is used to get the data from and save the result to photorep predict \u2714\ufe0f Field to be predicted name predict_alias Alias for predicted field predicted_name using Configuration options for trained model ...","title":"Creating a Predictor"},{"location":"connect/dbt/#creating-predictions-table","text":"Create a table_name.sql file where table_name is used as the name of the predictor. Or you can create a schema_name.table_name.sql file where schema_name is the name of the integration and table_name is the name of the predictor. {{ config ( materialized = 'table' , predictor_name = 'store_predictor' , integration = 'photorep' ) }} SELECT a , bc FROM ddd WHERE name > latest ; Let's list the required data fields. Parameter Required Description Example materialized \u2714\ufe0f Use always the value table table predictor_name \u2714\ufe0f Name of the predictor model from CREATE MODEL store_predictor integration \u2714\ufe0f Create an integration in MindsDB that is used to get the data from and save the result to photorep Note that each time DBT is run, the results table is overwritten.","title":"Creating Predictions Table"},{"location":"connect/dbt/#testing","text":"Install dev requirements: pip install -r dev_requirements.txt Run tests: python -m pytest tests/","title":"Testing"},{"location":"connect/dbt/#whats-next","text":"Now that you are all set, we recommend you check out our Tutorials and Community Tutorials sections, where you'll find various examples of regression, classification, and time series predictions with MindsDB. To learn more about MindsDB itself, follow the guide on MindsDB database structure . Also, don't miss out on the remaining pages from the SQL API section, as they explain a common SQL syntax with examples. Have fun!","title":"What's Next?"},{"location":"connect/deepnote/","text":"MindsDB and Deepnote \u00b6 Deepnote integration We have worked with the team at Deepnote, and built native integration to Deepnote notebooks. Please check: Deepnote Demo Guide Deepnote Integration Docs What's Next? \u00b6 Now that you are all set, we recommend you check out our Tutorials and Community Tutorials sections, where you'll find various examples of regression, classification, and time series predictions with MindsDB. To learn more about MindsDB itself, follow the guide on MindsDB database structure . Also, don't miss out on the remaining pages from the SQL API section, as they explain a common SQL syntax with examples. Have fun!","title":"Deepnote"},{"location":"connect/deepnote/#mindsdb-and-deepnote","text":"Deepnote integration We have worked with the team at Deepnote, and built native integration to Deepnote notebooks. Please check: Deepnote Demo Guide Deepnote Integration Docs","title":"MindsDB and Deepnote"},{"location":"connect/deepnote/#whats-next","text":"Now that you are all set, we recommend you check out our Tutorials and Community Tutorials sections, where you'll find various examples of regression, classification, and time series predictions with MindsDB. To learn more about MindsDB itself, follow the guide on MindsDB database structure . Also, don't miss out on the remaining pages from the SQL API section, as they explain a common SQL syntax with examples. Have fun!","title":"What's Next?"},{"location":"connect/kafka/","text":"MindsDB and Kafka \u00b6 MindsDB provides the Kafka connector plugin to connect to the Kafka cluster. Please visit the Kafka Connect MindsDB page on the official Confluent site. It contains all the instructions on how to install the connector from the Confluent hub. Setting up Kafka in Docker \u00b6 Let's review the instructions here as well. You may use the official connector docker image: docker pull mindsdb/mindsdb-kafka-connector The source code of the Kafka connector is in the kafka_connector GitHub repository . Please read the instructions before building the connector from scratch. It is possible to integrate and use the Kafka connector as part of your own Kafka cluster, or you may use our test docker environment. To try out our test docker environment , follow the steps below. Before you bring the docker container up, please note that there are two types of connector configuration: For MindsDB Cloud For a separate MindsDB installation No matter which option you choose, these files require real values to be set in place of a username, password, Kafka connection details, etc. The SASL mechanism details are optional, as local Kafka installation may not have this mechanism configured - or you can use this data for the SASL username and password. Now that your config files store real data, you can execute the command below from the root of the kafka_connector GitHub repository to build the connector and launch it in the test environment locally. docker-compose up -d Let's go over some examples. Examples \u00b6 Prerequisites \u00b6 Launch MindsDB instance where HTTP API interface runs on docker network interface inet ip . Usually, the IP address is 172.17.0.1 , and the port is 47334 . You can modify the HTTP API interface details in the MindsDB config file . Now, to launch your MindsDB, run the following command: python -m mindsdb --config = /path-to-the-config-file/your-config-file.json Please note that the config file must be in JSON format. It must include this part of the config file from the MindsDB repository in a proper JSON format. Train a new model. You may use this tutorial as an example. Run the test environment as instructed in the Setting up Kafka in Docker section above. Create the Connector Instance \u00b6 To create a connector, you need to send a POST request to a specific CONNECTORS_URL with connector configuration in JSON format, as below. import requests MINDSDB_URL = \"http://172.17.0.1:47334\" CONNECTOR_NAME = \"MindsDBConnector\" INTEGRATION_NAME = 'test_kafka' KAFKA_PORT = 9092 KAFKA_HOST = \"127.0.0.1\" CONNECTOR_NAME = \"MindsDBConnector\" CONNECTORS_URL = \"http://127.0.0.1:9021/api/connect/connect-default/connectors\" STREAM_IN = \"topic_in\" STREAM_OUT = \"topic_out\" PREDICTOR_NAME = \"YOUR_MODEL_NAME\" # set actual model name here params = { \"name\" : CONNECTOR_NAME , \"config\" : { \"connector.class\" : \"com.mindsdb.kafka.connect.MindsDBConnector\" , \"topics\" : STREAM_IN , \"mindsdb.url\" : MINDSDB_URL , \"kafka.api.host\" : KAFKA_HOST , \"kafka.api.port\" : KAFKA_PORT , \"kafka.api.name\" : INTEGRATION_NAME , \"predictor.name\" : PREDICTOR_NAME , \"output.forecast.topic\" : STREAM_OUT , \"security.protocol\" : \"SASL_PLAINTEXT\" , \"sasl.mechanism\" : \"PLAIN\" , \"sasl.plain.username\" : \"admin\" , \"sasl.plain.password\" : \"admin-secret\" , } } headers = { \"Content-Type\" : \"application/json\" } res = requests . post ( CONNECTORS_URL , json = params , headers = headers ) This code creates a MindsDB Kafka connector that uses the PREDICTOR_NAME model, receives source data from the STREAM_IN Kafka topic, and sends prediction results to the STREAM_OUT Kafka topic. Send Source Data and Receive Prediction Results \u00b6 There are many Kafka client implementations - choose the most suitable one depending on your goals. The code below generates and sends the source records to topic_in by default. You can use any other Kafka topic by providing its name as a CMD parameter. import sys import json import kafka connection_info = { \"bootstrap_servers\" : \"127.0.0.1:9092\" , \"security_protocol\" : \"SASL_PLAINTEXT\" , \"sasl_mechanism\" : \"PLAIN\" , \"sasl_plain_username\" : \"admin\" , \"sasl_plain_password\" : \"admin-secret\" } producer = kafka . KafkaProducer ( ** connection_info ) if __name__ == '__main__' : print ( json . dumps ( connection_info )) if len ( sys . argv ) == 1 : topic = \"topic_in\" else : topic = sys . argv [ 1 ] for x in range ( 1 , 4 ): data = { \"Age\" : x + 20 , \"Weight\" : x * x * 0.8 + 200 , \"Height\" : x * x * 0.5 + 65 } to_send = json . dumps ( data ) producer . send ( topic , to_send . encode ( 'utf-8' )) producer . close () And the following code shows how to read prediction results from topic_out by default. Again, you can use any other Kafka topic by providing its name as a CMD parameter. import sys import json import kafka connection_info = { \"bootstrap_servers\" : \"127.0.0.1:9092\" , \"security_protocol\" : \"SASL_PLAINTEXT\" , \"sasl_mechanism\" : \"PLAIN\" , \"sasl_plain_username\" : \"admin\" , \"sasl_plain_password\" : \"admin-secret\" , \"auto_offset_reset\" : 'earliest' , \"consumer_timeout_ms\" : 1000 } consumer = kafka . KafkaConsumer ( ** connection_info ) if __name__ == '__main__' : print ( json . dumps ( connection_info )) if len ( sys . argv ) == 1 : topic = \"topic_out\" else : topic = sys . argv [ 1 ] consumer . subscribe ( topics = [ topic ]) for msg in consumer : print ( msg . value ) print ( \"-\" * 100 ) What's Next? \u00b6 Now that you are all set, we recommend you check out our Tutorials and Community Tutorials sections, where you'll find various examples of regression, classification, and time series predictions with MindsDB. To learn more about MindsDB itself, follow the guide on MindsDB database structure . Also, don't miss out on the remaining pages from the SQL API section, as they explain a common SQL syntax with examples. Have fun!","title":"Kafka"},{"location":"connect/kafka/#mindsdb-and-kafka","text":"MindsDB provides the Kafka connector plugin to connect to the Kafka cluster. Please visit the Kafka Connect MindsDB page on the official Confluent site. It contains all the instructions on how to install the connector from the Confluent hub.","title":"MindsDB and Kafka"},{"location":"connect/kafka/#setting-up-kafka-in-docker","text":"Let's review the instructions here as well. You may use the official connector docker image: docker pull mindsdb/mindsdb-kafka-connector The source code of the Kafka connector is in the kafka_connector GitHub repository . Please read the instructions before building the connector from scratch. It is possible to integrate and use the Kafka connector as part of your own Kafka cluster, or you may use our test docker environment. To try out our test docker environment , follow the steps below. Before you bring the docker container up, please note that there are two types of connector configuration: For MindsDB Cloud For a separate MindsDB installation No matter which option you choose, these files require real values to be set in place of a username, password, Kafka connection details, etc. The SASL mechanism details are optional, as local Kafka installation may not have this mechanism configured - or you can use this data for the SASL username and password. Now that your config files store real data, you can execute the command below from the root of the kafka_connector GitHub repository to build the connector and launch it in the test environment locally. docker-compose up -d Let's go over some examples.","title":"Setting up Kafka in Docker"},{"location":"connect/kafka/#examples","text":"","title":"Examples"},{"location":"connect/kafka/#prerequisites","text":"Launch MindsDB instance where HTTP API interface runs on docker network interface inet ip . Usually, the IP address is 172.17.0.1 , and the port is 47334 . You can modify the HTTP API interface details in the MindsDB config file . Now, to launch your MindsDB, run the following command: python -m mindsdb --config = /path-to-the-config-file/your-config-file.json Please note that the config file must be in JSON format. It must include this part of the config file from the MindsDB repository in a proper JSON format. Train a new model. You may use this tutorial as an example. Run the test environment as instructed in the Setting up Kafka in Docker section above.","title":"Prerequisites"},{"location":"connect/kafka/#create-the-connector-instance","text":"To create a connector, you need to send a POST request to a specific CONNECTORS_URL with connector configuration in JSON format, as below. import requests MINDSDB_URL = \"http://172.17.0.1:47334\" CONNECTOR_NAME = \"MindsDBConnector\" INTEGRATION_NAME = 'test_kafka' KAFKA_PORT = 9092 KAFKA_HOST = \"127.0.0.1\" CONNECTOR_NAME = \"MindsDBConnector\" CONNECTORS_URL = \"http://127.0.0.1:9021/api/connect/connect-default/connectors\" STREAM_IN = \"topic_in\" STREAM_OUT = \"topic_out\" PREDICTOR_NAME = \"YOUR_MODEL_NAME\" # set actual model name here params = { \"name\" : CONNECTOR_NAME , \"config\" : { \"connector.class\" : \"com.mindsdb.kafka.connect.MindsDBConnector\" , \"topics\" : STREAM_IN , \"mindsdb.url\" : MINDSDB_URL , \"kafka.api.host\" : KAFKA_HOST , \"kafka.api.port\" : KAFKA_PORT , \"kafka.api.name\" : INTEGRATION_NAME , \"predictor.name\" : PREDICTOR_NAME , \"output.forecast.topic\" : STREAM_OUT , \"security.protocol\" : \"SASL_PLAINTEXT\" , \"sasl.mechanism\" : \"PLAIN\" , \"sasl.plain.username\" : \"admin\" , \"sasl.plain.password\" : \"admin-secret\" , } } headers = { \"Content-Type\" : \"application/json\" } res = requests . post ( CONNECTORS_URL , json = params , headers = headers ) This code creates a MindsDB Kafka connector that uses the PREDICTOR_NAME model, receives source data from the STREAM_IN Kafka topic, and sends prediction results to the STREAM_OUT Kafka topic.","title":"Create the Connector Instance"},{"location":"connect/kafka/#send-source-data-and-receive-prediction-results","text":"There are many Kafka client implementations - choose the most suitable one depending on your goals. The code below generates and sends the source records to topic_in by default. You can use any other Kafka topic by providing its name as a CMD parameter. import sys import json import kafka connection_info = { \"bootstrap_servers\" : \"127.0.0.1:9092\" , \"security_protocol\" : \"SASL_PLAINTEXT\" , \"sasl_mechanism\" : \"PLAIN\" , \"sasl_plain_username\" : \"admin\" , \"sasl_plain_password\" : \"admin-secret\" } producer = kafka . KafkaProducer ( ** connection_info ) if __name__ == '__main__' : print ( json . dumps ( connection_info )) if len ( sys . argv ) == 1 : topic = \"topic_in\" else : topic = sys . argv [ 1 ] for x in range ( 1 , 4 ): data = { \"Age\" : x + 20 , \"Weight\" : x * x * 0.8 + 200 , \"Height\" : x * x * 0.5 + 65 } to_send = json . dumps ( data ) producer . send ( topic , to_send . encode ( 'utf-8' )) producer . close () And the following code shows how to read prediction results from topic_out by default. Again, you can use any other Kafka topic by providing its name as a CMD parameter. import sys import json import kafka connection_info = { \"bootstrap_servers\" : \"127.0.0.1:9092\" , \"security_protocol\" : \"SASL_PLAINTEXT\" , \"sasl_mechanism\" : \"PLAIN\" , \"sasl_plain_username\" : \"admin\" , \"sasl_plain_password\" : \"admin-secret\" , \"auto_offset_reset\" : 'earliest' , \"consumer_timeout_ms\" : 1000 } consumer = kafka . KafkaConsumer ( ** connection_info ) if __name__ == '__main__' : print ( json . dumps ( connection_info )) if len ( sys . argv ) == 1 : topic = \"topic_out\" else : topic = sys . argv [ 1 ] consumer . subscribe ( topics = [ topic ]) for msg in consumer : print ( msg . value ) print ( \"-\" * 100 )","title":"Send Source Data and Receive Prediction Results"},{"location":"connect/kafka/#whats-next","text":"Now that you are all set, we recommend you check out our Tutorials and Community Tutorials sections, where you'll find various examples of regression, classification, and time series predictions with MindsDB. To learn more about MindsDB itself, follow the guide on MindsDB database structure . Also, don't miss out on the remaining pages from the SQL API section, as they explain a common SQL syntax with examples. Have fun!","title":"What's Next?"},{"location":"connect/metabase/","text":"MindsDB and Metabase \u00b6 Metabase is open-source software that facilitates data analysis. It lets you visualize your data easily and intuitively. Now that MindsDB supports the MySQL binary protocol, you can connect it to Metabase and see the forecasts by creating and training the models. For more information, visit Metabase . Setup \u00b6 MindsDB \u00b6 Let's set up the MindsDB following one of the guides from the Deployment section. Please note that the MindsDB Cloud is not yet supported. Currently, only local and on-premise installations are stable. You can choose one of the following approaches to set up the MindsDB locally: Docker Windows via pip Linux via pip MacOS via pip Sourcecode via pip Here, we use the Docker setup for MindsDB. Metabase \u00b6 Now, let's set up the Metabase by following one of the approaches presented on the Metabase Open Source Edition page . Here, we use the .jar approach for Metabase. How to Connect \u00b6 Follow the steps below to connect your MindsDB to Metabase. Open your Metabase and navigate to the Admin settings by clicking the cog in the bottom left corner. Once there, click on Databases in the top navigation bar. Click on Add database in the top right corner. Fill in the form using the following data: Database type: `MySQL` Display name: `MindsDB` Host: `localhost` Port: `47335` Database name: `mindsdb` Username: `mindsdb` Password: *leave it empty* Click on Save . Now you're connected! Example \u00b6 Now that the connection between MindsDB and Metabase is established, let's do some examples. Most of the SQL statements that you usually run in your MindsDB SQL Editor can be run in Metabase as well. Let's start with something easy. On your Metabase's home page, click on New > SQL query in the top right corner and then, select your MindsDB database. Let's execute the following command in the editor. SHOW TABLES ; On execution, we get: Please note that creating a database connection using the CREATE DATABASE statement fails because of the curly braces ( {} ) being used by JDBC as the escape sequences. CREATE DATABASE example_db WITH ENGINE = \"postgres\" , PARAMETERS = { \"user\" : \"demo_user\" , \"password\" : \"demo_password\" , \"host\" : \"3.220.66.106\" , \"port\" : \"5432\" , \"database\" : \"demo\" } ; On execution, we get: You can overcome this issue using the MindsDB SQL Editor to create a database. Now, getting back to the Metabase, let's run some queries on the database created with the help of the MindsDB SQL Editor . SELECT * FROM example_db . demo_data . home_rentals LIMIT 10 ; On execution, we get: What's Next? \u00b6 Now that you are all set, we recommend you check out our Tutorials and Community Tutorials sections, where you'll find various examples of regression, classification, and time series predictions with MindsDB. To learn more about MindsDB itself, follow the guide on MindsDB database structure . Also, don't miss out on the remaining pages from the SQL API section, as they explain a common SQL syntax with examples. Have fun!","title":"Metabase"},{"location":"connect/metabase/#mindsdb-and-metabase","text":"Metabase is open-source software that facilitates data analysis. It lets you visualize your data easily and intuitively. Now that MindsDB supports the MySQL binary protocol, you can connect it to Metabase and see the forecasts by creating and training the models. For more information, visit Metabase .","title":"MindsDB and Metabase"},{"location":"connect/metabase/#setup","text":"","title":"Setup"},{"location":"connect/metabase/#mindsdb","text":"Let's set up the MindsDB following one of the guides from the Deployment section. Please note that the MindsDB Cloud is not yet supported. Currently, only local and on-premise installations are stable. You can choose one of the following approaches to set up the MindsDB locally: Docker Windows via pip Linux via pip MacOS via pip Sourcecode via pip Here, we use the Docker setup for MindsDB.","title":"MindsDB"},{"location":"connect/metabase/#metabase","text":"Now, let's set up the Metabase by following one of the approaches presented on the Metabase Open Source Edition page . Here, we use the .jar approach for Metabase.","title":"Metabase"},{"location":"connect/metabase/#how-to-connect","text":"Follow the steps below to connect your MindsDB to Metabase. Open your Metabase and navigate to the Admin settings by clicking the cog in the bottom left corner. Once there, click on Databases in the top navigation bar. Click on Add database in the top right corner. Fill in the form using the following data: Database type: `MySQL` Display name: `MindsDB` Host: `localhost` Port: `47335` Database name: `mindsdb` Username: `mindsdb` Password: *leave it empty* Click on Save . Now you're connected!","title":"How to Connect"},{"location":"connect/metabase/#example","text":"Now that the connection between MindsDB and Metabase is established, let's do some examples. Most of the SQL statements that you usually run in your MindsDB SQL Editor can be run in Metabase as well. Let's start with something easy. On your Metabase's home page, click on New > SQL query in the top right corner and then, select your MindsDB database. Let's execute the following command in the editor. SHOW TABLES ; On execution, we get: Please note that creating a database connection using the CREATE DATABASE statement fails because of the curly braces ( {} ) being used by JDBC as the escape sequences. CREATE DATABASE example_db WITH ENGINE = \"postgres\" , PARAMETERS = { \"user\" : \"demo_user\" , \"password\" : \"demo_password\" , \"host\" : \"3.220.66.106\" , \"port\" : \"5432\" , \"database\" : \"demo\" } ; On execution, we get: You can overcome this issue using the MindsDB SQL Editor to create a database. Now, getting back to the Metabase, let's run some queries on the database created with the help of the MindsDB SQL Editor . SELECT * FROM example_db . demo_data . home_rentals LIMIT 10 ; On execution, we get:","title":"Example"},{"location":"connect/metabase/#whats-next","text":"Now that you are all set, we recommend you check out our Tutorials and Community Tutorials sections, where you'll find various examples of regression, classification, and time series predictions with MindsDB. To learn more about MindsDB itself, follow the guide on MindsDB database structure . Also, don't miss out on the remaining pages from the SQL API section, as they explain a common SQL syntax with examples. Have fun!","title":"What's Next?"},{"location":"connect/mindsdb_editor/","text":"MindsDB SQL Editor \u00b6 MindsDB provides a SQL Editor, so you don't need to download additional SQL clients to connect to MindsDB. How to Use the MindsDB SQL Editor \u00b6 There are two ways you can use the Editor, as below. Self-Hosted Local Deployment MindsDB Cloud After setting up the MindsDB using Docker , or pip on Linux / Windows / MacOs , or pip via source code , go to your terminal and execute the following: python -m mindsdb On execution, we get: ... 2022 -05-06 14 :07:04,599 - INFO - - GUI available at http://127.0.0.1:47334/ ... Immediately after, your browser automatically opens the MindsDB SQL Editor. In case if it doesn't, visit the URL http://127.0.0.1:47334/ in your browser of preference. Go to the MindsDB Cloud and log in to your account. The Editor is the first page you see after logging in. Here is a sneak peek of the MindsDB SQL Editor: What's Next? \u00b6 Now that you are all set, we recommend you check out our Tutorials and Community Tutorials sections, where you'll find various examples of regression, classification, and time series predictions with MindsDB. To learn more about MindsDB itself, follow the guide on MindsDB database structure . Also, don't miss out on the remaining pages from the SQL API section, as they explain a common SQL syntax with examples. Have fun!","title":"MindsDB SQL Editor"},{"location":"connect/mindsdb_editor/#mindsdb-sql-editor","text":"MindsDB provides a SQL Editor, so you don't need to download additional SQL clients to connect to MindsDB.","title":"MindsDB SQL Editor"},{"location":"connect/mindsdb_editor/#how-to-use-the-mindsdb-sql-editor","text":"There are two ways you can use the Editor, as below. Self-Hosted Local Deployment MindsDB Cloud After setting up the MindsDB using Docker , or pip on Linux / Windows / MacOs , or pip via source code , go to your terminal and execute the following: python -m mindsdb On execution, we get: ... 2022 -05-06 14 :07:04,599 - INFO - - GUI available at http://127.0.0.1:47334/ ... Immediately after, your browser automatically opens the MindsDB SQL Editor. In case if it doesn't, visit the URL http://127.0.0.1:47334/ in your browser of preference. Go to the MindsDB Cloud and log in to your account. The Editor is the first page you see after logging in. Here is a sneak peek of the MindsDB SQL Editor:","title":"How to Use the MindsDB SQL Editor"},{"location":"connect/mindsdb_editor/#whats-next","text":"Now that you are all set, we recommend you check out our Tutorials and Community Tutorials sections, where you'll find various examples of regression, classification, and time series predictions with MindsDB. To learn more about MindsDB itself, follow the guide on MindsDB database structure . Also, don't miss out on the remaining pages from the SQL API section, as they explain a common SQL syntax with examples. Have fun!","title":"What's Next?"},{"location":"connect/mongo-compass/","text":"MindsDB and MongoDB Compass \u00b6 MongoDB Compass is the GUI for MongoDB. Compass provides detailed schema visualizations, real-time performance metrics, sophisticated querying abilities, and much more. You can download it here . Connect \u00b6 First, create a new connection in Compass by clicking the New Connection from the left navigation panel. Next, paste the connection string: Or, fill in connection fields individualy: In the above images we've connected to local MindsDB. Check the option bellow to connect to MindsDB cloud. Connecting to your MindsDB Cloud account You can connect to your MindsDB Cloud account. To do that, please use the connection details below: Hostname: `cloud.mindsdb.com` Port: `272018` Authentication: `Username / Password` Username: *your MindsDB Cloud email* Password: *your MindsDB Cloud password* What's Next? \u00b6 Now that you are all set, we recommend you check out our Tutorials and Community Tutorials sections, where you'll find various examples of regression, classification, and time series predictions with MindsDB. To learn more about MindsDB itself, follow the guide on MindsDB collection structure . Also, don't miss out on the remaining pages from the MONGO API section, as they explain a common MQL syntax with examples. Have fun!","title":"Mongo Compass"},{"location":"connect/mongo-compass/#mindsdb-and-mongodb-compass","text":"MongoDB Compass is the GUI for MongoDB. Compass provides detailed schema visualizations, real-time performance metrics, sophisticated querying abilities, and much more. You can download it here .","title":"MindsDB and MongoDB Compass"},{"location":"connect/mongo-compass/#connect","text":"First, create a new connection in Compass by clicking the New Connection from the left navigation panel. Next, paste the connection string: Or, fill in connection fields individualy: In the above images we've connected to local MindsDB. Check the option bellow to connect to MindsDB cloud. Connecting to your MindsDB Cloud account You can connect to your MindsDB Cloud account. To do that, please use the connection details below: Hostname: `cloud.mindsdb.com` Port: `272018` Authentication: `Username / Password` Username: *your MindsDB Cloud email* Password: *your MindsDB Cloud password*","title":"Connect"},{"location":"connect/mongo-compass/#whats-next","text":"Now that you are all set, we recommend you check out our Tutorials and Community Tutorials sections, where you'll find various examples of regression, classification, and time series predictions with MindsDB. To learn more about MindsDB itself, follow the guide on MindsDB collection structure . Also, don't miss out on the remaining pages from the MONGO API section, as they explain a common MQL syntax with examples. Have fun!","title":"What's Next?"},{"location":"connect/mongo-shell/","text":"MindsDB and MongoDB Shell \u00b6 MindsDB provides a powerful MongoDB API that allows users to connect to it using the MongoDB Shell . MongoDB Shell is the quickest way to connect and work with MongoDB. Please note that connection to MindsDB's MongoDB API is the same as connection to a MongoDB database. How to Connect \u00b6 To connect to MindsDB, run the below command in your Mongo Shell and provide the connection details, such as host, port, username, and password. mongo --host [ hostname ] --port [ TCP/IP port number ] -u [ user ] -p [ password ] Here are the commands that allow you to connect to either a local MindsDB installation or a MindsDB Cloud instance. Self-Hosted Local Deployment MindsDB Cloud mongo --host 127 .0.0.1 --port 47336 mongo --host cloud.mindsdb.com -u [ mindsdb_cloud_username ] -p [ mindsdb_cloud_password ] On execution, we get: MongoDB shell version v4.0.28 Enter password: connecting to: mongodb://cloud.mindsdb.com:27017/?gssapiServiceName = mongodb Implicit session: session { \"id\" : UUID ( \"b2afbba1-ddfd-4379-944f-ca2eadsdas\" ) } MongoDB server version: 4 .0.28 > Example \u00b6 In this example, we connect to the MindsDB Cloud instance, as below. mongo --host cloud.mindsdb.com -u zoran@mindsdb.com -p On execution, we get: MongoDB shell version v4.0.28 Enter password: connecting to: mongodb://cloud.mindsdb.com:27017/?gssapiServiceName = mongodb Implicit session: session { \"id\" : UUID ( \"b2afbba1-ddfd-4379-944f-asdasaasd\" ) } MongoDB server version: 4 .0.28 What's Next? \u00b6 Now that you are all set, we recommend you check out our Tutorials and Community Tutorials sections, where you'll find various examples of regression, classification, and time series predictions with MindsDB. To learn more about MindsDB itself, follow the guide on MindsDB collection structure . Also, don't miss out on the remaining pages from the Mongo API section, as they explain a common MQL syntax with examples. Have fun!","title":"Mongo Shell"},{"location":"connect/mongo-shell/#mindsdb-and-mongodb-shell","text":"MindsDB provides a powerful MongoDB API that allows users to connect to it using the MongoDB Shell . MongoDB Shell is the quickest way to connect and work with MongoDB. Please note that connection to MindsDB's MongoDB API is the same as connection to a MongoDB database.","title":"MindsDB and MongoDB Shell"},{"location":"connect/mongo-shell/#how-to-connect","text":"To connect to MindsDB, run the below command in your Mongo Shell and provide the connection details, such as host, port, username, and password. mongo --host [ hostname ] --port [ TCP/IP port number ] -u [ user ] -p [ password ] Here are the commands that allow you to connect to either a local MindsDB installation or a MindsDB Cloud instance. Self-Hosted Local Deployment MindsDB Cloud mongo --host 127 .0.0.1 --port 47336 mongo --host cloud.mindsdb.com -u [ mindsdb_cloud_username ] -p [ mindsdb_cloud_password ] On execution, we get: MongoDB shell version v4.0.28 Enter password: connecting to: mongodb://cloud.mindsdb.com:27017/?gssapiServiceName = mongodb Implicit session: session { \"id\" : UUID ( \"b2afbba1-ddfd-4379-944f-ca2eadsdas\" ) } MongoDB server version: 4 .0.28 >","title":"How to Connect"},{"location":"connect/mongo-shell/#example","text":"In this example, we connect to the MindsDB Cloud instance, as below. mongo --host cloud.mindsdb.com -u zoran@mindsdb.com -p On execution, we get: MongoDB shell version v4.0.28 Enter password: connecting to: mongodb://cloud.mindsdb.com:27017/?gssapiServiceName = mongodb Implicit session: session { \"id\" : UUID ( \"b2afbba1-ddfd-4379-944f-asdasaasd\" ) } MongoDB server version: 4 .0.28","title":"Example"},{"location":"connect/mongo-shell/#whats-next","text":"Now that you are all set, we recommend you check out our Tutorials and Community Tutorials sections, where you'll find various examples of regression, classification, and time series predictions with MindsDB. To learn more about MindsDB itself, follow the guide on MindsDB collection structure . Also, don't miss out on the remaining pages from the Mongo API section, as they explain a common MQL syntax with examples. Have fun!","title":"What's Next?"},{"location":"connect/mysql-client/","text":"MindsDB and MySQL CLI \u00b6 MindsDB provides a powerful MySQL API that allows users to connect to it using the MySQL Command Line Client . Please note that connecting to MindsDB's MySQL API is the same as connecting to a MySQL database. How to Connect \u00b6 To connect to MindsDB, run the below command in your MySQL CLI and provide the connection details, such as host, port, username, and password. mysql -h [ hostname ] --port [ TCP/IP port number ] -u [ user ] -p [ password ] Here are the commands that allow you to connect to either a local MindsDB installation or a MindsDB Cloud instance. Self-Hosted Local Deployment MindsDB Cloud mysql -h 127 .0.0.1 --port 47335 -u mindsdb mysql -h cloud.mindsdb.com --port 3306 -u [ mindsdb_cloud_username ] -p [ mindsdb_cloud_password ] On execution, we get: Welcome to the MariaDB monitor. Commands end with ; or \\g . Server version: 5 .7.1-MindsDB-1.0 ( MindsDB ) Type 'help;' or '\\h' for help. Type '\\c' to clear the current input statement. MySQL [( none )] > Example \u00b6 In this example, we connect to the MindsDB Cloud instance, as below. mysql -h cloud.mindsdb.com --port 3306 -u zoran@mindsdb.com -p On execution, we get: Enter password: Welcome to the MariaDB monitor. Commands end with ; or \\g . Server version: 5 .7.1-MindsDB-1.0 ( MindsDB ) Type 'help;' or '\\h' for help. Type '\\c' to clear the current input statement. MySQL [( none )] > What's Next? \u00b6 Now that you are all set, we recommend you check out our Tutorials and Community Tutorials sections, where you'll find various examples of regression, classification, and time series predictions with MindsDB. To learn more about MindsDB itself, follow the guide on MindsDB database structure . Also, don't miss out on the remaining pages from the SQL API section, as they explain a common SQL syntax with examples. Have fun!","title":"MySQL CLI"},{"location":"connect/mysql-client/#mindsdb-and-mysql-cli","text":"MindsDB provides a powerful MySQL API that allows users to connect to it using the MySQL Command Line Client . Please note that connecting to MindsDB's MySQL API is the same as connecting to a MySQL database.","title":"MindsDB and MySQL CLI"},{"location":"connect/mysql-client/#how-to-connect","text":"To connect to MindsDB, run the below command in your MySQL CLI and provide the connection details, such as host, port, username, and password. mysql -h [ hostname ] --port [ TCP/IP port number ] -u [ user ] -p [ password ] Here are the commands that allow you to connect to either a local MindsDB installation or a MindsDB Cloud instance. Self-Hosted Local Deployment MindsDB Cloud mysql -h 127 .0.0.1 --port 47335 -u mindsdb mysql -h cloud.mindsdb.com --port 3306 -u [ mindsdb_cloud_username ] -p [ mindsdb_cloud_password ] On execution, we get: Welcome to the MariaDB monitor. Commands end with ; or \\g . Server version: 5 .7.1-MindsDB-1.0 ( MindsDB ) Type 'help;' or '\\h' for help. Type '\\c' to clear the current input statement. MySQL [( none )] >","title":"How to Connect"},{"location":"connect/mysql-client/#example","text":"In this example, we connect to the MindsDB Cloud instance, as below. mysql -h cloud.mindsdb.com --port 3306 -u zoran@mindsdb.com -p On execution, we get: Enter password: Welcome to the MariaDB monitor. Commands end with ; or \\g . Server version: 5 .7.1-MindsDB-1.0 ( MindsDB ) Type 'help;' or '\\h' for help. Type '\\c' to clear the current input statement. MySQL [( none )] >","title":"Example"},{"location":"connect/mysql-client/#whats-next","text":"Now that you are all set, we recommend you check out our Tutorials and Community Tutorials sections, where you'll find various examples of regression, classification, and time series predictions with MindsDB. To learn more about MindsDB itself, follow the guide on MindsDB database structure . Also, don't miss out on the remaining pages from the SQL API section, as they explain a common SQL syntax with examples. Have fun!","title":"What's Next?"},{"location":"connect/sql-alchemy/","text":"MindsDB and SQL Alchemy \u00b6 SQL Alchemy is a Python package, or a Python SQL toolkit, that provides object-relational mapping features for the Python programming language. It facilitates working with databases and Python. You can download it here or run a pip install sqlalchemy command if you use a Linux system. How to Connect \u00b6 Please follow the instructions below to connect your MindsDB to SQL Alchemy. Connecting MindsDB Cloud to SQL Alchemy Connecting Local MindsDB to SQL Alchemy You can use the Python code below to connect your Cloud MindsDB database to SQL Alchemy. Make sure you have the pymysql module installed before executing the Python code. To install it, run the pip install pymysql command. from sqlalchemy import create_engine user = 'MindsDB Cloud username' # your Mindsdb Cloud email address is your username password = 'MindsDB Cloud password' # replace this value host = 'cloud.mindsdb.com' port = 3306 database = '' def get_connection (): return create_engine ( url = \"mysql+pymysql:// {0} : {1} @ {2} : {3} / {4} \" . format ( user , password , host , port , database ) ) if __name__ == '__main__' : try : engine = get_connection () engine . connect () print ( f \"Connection to the { host } for user { user } created successfully.\" ) except Exception as ex : print ( \"Connection could not be made due to the following error: \\n \" , ex ) Please note that we use the following connection details: Username is your MindsDB Cloud email address Password is your MindsDB Cloud password Host is cloud.mindsdb.com Port is 3306 Database name is left empty To create a database connection, execute the code above. On success, the following output is expected: Connection to the cloud.mindsdb.com for user MindsDB-Cloud-Username created successfully. You can use the Python code below to connect your local MindsDB database to SQL Alchemy. Make sure you have the pymysql module installed before executing the Python code. To install it, run the pip install pymysql command. from sqlalchemy import create_engine user = 'mindsdb' password = '' host = '127.0.0.1' port = 47335 database = '' def get_connection (): return create_engine ( url = \"mysql+pymysql:// {0} : {1} @ {2} : {3} / {4} \" . format ( user , password , host , port , database ) ) if __name__ == '__main__' : try : engine = get_connection () engine . connect () print ( f \"Connection to the { host } for user { user } created successfully.\" ) except Exception as ex : print ( \"Connection could not be made due to the following error: \\n \" , ex ) Please note that we use the following connection details: Username is mindsdb Password is left empty Host is 127.0.0.1 Port is 47335 Database name is left empty To create a database connection, execute the code above. On success, the following output is expected: Connection to the 127 .0.0.1 for user mindsdb created successfully. Note The Sqlachemy create_engine is lazy. This implies any human error when entering the connection details would be undetectable until an action becomes necessary, such as when calling the execute method to execute SQL commands. What's Next? \u00b6 Now that you are all set, we recommend you check out our Tutorials and Community Tutorials sections, where you'll find various examples of regression, classification, and time series predictions with MindsDB. To learn more about MindsDB itself, follow the guide on MindsDB database structure . Also, don't miss out on the remaining pages from the SQL API section, as they explain a common SQL syntax with examples. Have fun!","title":"SQL Alchemy"},{"location":"connect/sql-alchemy/#mindsdb-and-sql-alchemy","text":"SQL Alchemy is a Python package, or a Python SQL toolkit, that provides object-relational mapping features for the Python programming language. It facilitates working with databases and Python. You can download it here or run a pip install sqlalchemy command if you use a Linux system.","title":"MindsDB and SQL Alchemy"},{"location":"connect/sql-alchemy/#how-to-connect","text":"Please follow the instructions below to connect your MindsDB to SQL Alchemy. Connecting MindsDB Cloud to SQL Alchemy Connecting Local MindsDB to SQL Alchemy You can use the Python code below to connect your Cloud MindsDB database to SQL Alchemy. Make sure you have the pymysql module installed before executing the Python code. To install it, run the pip install pymysql command. from sqlalchemy import create_engine user = 'MindsDB Cloud username' # your Mindsdb Cloud email address is your username password = 'MindsDB Cloud password' # replace this value host = 'cloud.mindsdb.com' port = 3306 database = '' def get_connection (): return create_engine ( url = \"mysql+pymysql:// {0} : {1} @ {2} : {3} / {4} \" . format ( user , password , host , port , database ) ) if __name__ == '__main__' : try : engine = get_connection () engine . connect () print ( f \"Connection to the { host } for user { user } created successfully.\" ) except Exception as ex : print ( \"Connection could not be made due to the following error: \\n \" , ex ) Please note that we use the following connection details: Username is your MindsDB Cloud email address Password is your MindsDB Cloud password Host is cloud.mindsdb.com Port is 3306 Database name is left empty To create a database connection, execute the code above. On success, the following output is expected: Connection to the cloud.mindsdb.com for user MindsDB-Cloud-Username created successfully. You can use the Python code below to connect your local MindsDB database to SQL Alchemy. Make sure you have the pymysql module installed before executing the Python code. To install it, run the pip install pymysql command. from sqlalchemy import create_engine user = 'mindsdb' password = '' host = '127.0.0.1' port = 47335 database = '' def get_connection (): return create_engine ( url = \"mysql+pymysql:// {0} : {1} @ {2} : {3} / {4} \" . format ( user , password , host , port , database ) ) if __name__ == '__main__' : try : engine = get_connection () engine . connect () print ( f \"Connection to the { host } for user { user } created successfully.\" ) except Exception as ex : print ( \"Connection could not be made due to the following error: \\n \" , ex ) Please note that we use the following connection details: Username is mindsdb Password is left empty Host is 127.0.0.1 Port is 47335 Database name is left empty To create a database connection, execute the code above. On success, the following output is expected: Connection to the 127 .0.0.1 for user mindsdb created successfully. Note The Sqlachemy create_engine is lazy. This implies any human error when entering the connection details would be undetectable until an action becomes necessary, such as when calling the execute method to execute SQL commands.","title":"How to Connect"},{"location":"connect/sql-alchemy/#whats-next","text":"Now that you are all set, we recommend you check out our Tutorials and Community Tutorials sections, where you'll find various examples of regression, classification, and time series predictions with MindsDB. To learn more about MindsDB itself, follow the guide on MindsDB database structure . Also, don't miss out on the remaining pages from the SQL API section, as they explain a common SQL syntax with examples. Have fun!","title":"What's Next?"},{"location":"connect/tableau/","text":"MindsDB and Tableau \u00b6 Tableau lets you visualize your data easily and intuitively. Now that MindsDB supports the MySQL binary protocol, you can connect it to Tableau and see the forecasts. How to Connect \u00b6 Follow the steps below to connect your MindsDB to Tableau. First, create a new workbook in Tableau and open the Connectors tab in the Connect to Data window. Next, choose MySQL and provide the details of your MindsDB connection, such as the IP, port, and database name. Optionally, you can provide a username and password. Then, click Sign In . Now you're connected! Overview of MindsDB in Tableau \u00b6 The content of your MindsDB is visible in the right-side pane. All the predictors are listed under the Table section. You can also switch between the integrations, such as mindsdb or files , in the Database section using the drop-down. Now, let's run some examples! Examples \u00b6 Example 1 \u00b6 Previewing one of the tables from the mysql integration: Example 2 \u00b6 There is one technical limitation. Namely, we cannot join tables from different databases/integrations in Tableau. To overcome this challenge, you can use either views or custom SQL queries. Previewing a view that joins a data table with a predictor table: Using a custom SQL query by clicking the New Custom SQL button in the right-side pane: What's Next? \u00b6 Now that you are all set, we recommend you check out our Tutorials and Community Tutorials sections, where you'll find various examples of regression, classification, and time series predictions with MindsDB. To learn more about MindsDB itself, follow the guide on MindsDB database structure . Also, don't miss out on the remaining pages from the SQL API section, as they explain a common SQL syntax with examples. Have fun!","title":"Tableau"},{"location":"connect/tableau/#mindsdb-and-tableau","text":"Tableau lets you visualize your data easily and intuitively. Now that MindsDB supports the MySQL binary protocol, you can connect it to Tableau and see the forecasts.","title":"MindsDB and Tableau"},{"location":"connect/tableau/#how-to-connect","text":"Follow the steps below to connect your MindsDB to Tableau. First, create a new workbook in Tableau and open the Connectors tab in the Connect to Data window. Next, choose MySQL and provide the details of your MindsDB connection, such as the IP, port, and database name. Optionally, you can provide a username and password. Then, click Sign In . Now you're connected!","title":"How to Connect"},{"location":"connect/tableau/#overview-of-mindsdb-in-tableau","text":"The content of your MindsDB is visible in the right-side pane. All the predictors are listed under the Table section. You can also switch between the integrations, such as mindsdb or files , in the Database section using the drop-down. Now, let's run some examples!","title":"Overview of MindsDB in Tableau"},{"location":"connect/tableau/#examples","text":"","title":"Examples"},{"location":"connect/tableau/#example-1","text":"Previewing one of the tables from the mysql integration:","title":"Example 1"},{"location":"connect/tableau/#example-2","text":"There is one technical limitation. Namely, we cannot join tables from different databases/integrations in Tableau. To overcome this challenge, you can use either views or custom SQL queries. Previewing a view that joins a data table with a predictor table: Using a custom SQL query by clicking the New Custom SQL button in the right-side pane:","title":"Example 2"},{"location":"connect/tableau/#whats-next","text":"Now that you are all set, we recommend you check out our Tutorials and Community Tutorials sections, where you'll find various examples of regression, classification, and time series predictions with MindsDB. To learn more about MindsDB itself, follow the guide on MindsDB database structure . Also, don't miss out on the remaining pages from the SQL API section, as they explain a common SQL syntax with examples. Have fun!","title":"What's Next?"},{"location":"contribute/docs/","text":"MindsDB Documentation \u00b6 This section will walk you through installing the docs locally. MindsDB documentation is built using MkDocs and use Material theme for MkDocs . The source code is located under docs directory in MindsDB repo. Documentation can always be improved so we don't have a strict guideliness to follow. We accept any sort of documentation and tutorials improvments, adding missing documentation, new tutorials or keep up the documentation up to date. Running the docs locally \u00b6 First install the mkdocs and mkdocs-material theme in your python virtual environment: pip install -r requirements.txt Then, navigate to the /mindsdb-docs directory and start the server: mkdocs serve The documentation website will be available at http://127.0.0.1:8000 Repository structure \u00b6 The mindsdb-docs layout is as follows: docs # Contains documentation source files |__assets/ # Image and icons used in pages \u2502 \u251c\u2500 images/ # Images and icons \u2502__stylesheets/ # CSS |__.md # All of the markdown files used as pages overrides \u251c\u2500 partials/ \u2502 \u251c\u2500 footer.html # Footer bar(empty) \u2502 \u251c\u2500 header.html # Header and navigation bar \u2514\u2500 main.html # Main page used for adding script blocks .mkdocs.yml # Mkdocs configuration file","title":"Writing Documentation"},{"location":"contribute/docs/#mindsdb-documentation","text":"This section will walk you through installing the docs locally. MindsDB documentation is built using MkDocs and use Material theme for MkDocs . The source code is located under docs directory in MindsDB repo. Documentation can always be improved so we don't have a strict guideliness to follow. We accept any sort of documentation and tutorials improvments, adding missing documentation, new tutorials or keep up the documentation up to date.","title":"MindsDB Documentation"},{"location":"contribute/docs/#running-the-docs-locally","text":"First install the mkdocs and mkdocs-material theme in your python virtual environment: pip install -r requirements.txt Then, navigate to the /mindsdb-docs directory and start the server: mkdocs serve The documentation website will be available at http://127.0.0.1:8000","title":"Running the docs locally"},{"location":"contribute/docs/#repository-structure","text":"The mindsdb-docs layout is as follows: docs # Contains documentation source files |__assets/ # Image and icons used in pages \u2502 \u251c\u2500 images/ # Images and icons \u2502__stylesheets/ # CSS |__.md # All of the markdown files used as pages overrides \u251c\u2500 partials/ \u2502 \u251c\u2500 footer.html # Footer bar(empty) \u2502 \u251c\u2500 header.html # Header and navigation bar \u2514\u2500 main.html # Main page used for adding script blocks .mkdocs.yml # Mkdocs configuration file","title":"Repository structure"},{"location":"contribute/install/","text":"Installing MindsDB for Development \u00b6 If you want to contribute to the development of MindsDB, you need to install it first. There are a few installation options that you can choose as follows: from source , using PyPi for Windows , Linux , or MacOS , using Docker image . Our preferred MindsDB installation method for development is the installation from source. Installing MindsDB \u00b6 Here, we recall the steps of MindsDB installation from source. You can either follow the steps below or visit the provided link. Before proceeding, make sure you have Git and Python 3.7.x or Python 3.8.x installed. Fork the MindsDB repository from GitHub . Clone the fork locally: git clone git@github.com:YOUR_USERNAME/mindsdb.git Create a new virtual environment: python3 -m venv mindsdb-venv Activate the virtual environment: source mindsdb-venv/bin/activate Install dependencies: cd mindsdb & pip install -r requirements.txt Install MindsDB: python setup.py develop Start MindsDB: python3 -m mindsdb If everything works as expected, you should see the following message in the console: ... 2022-06-28 16:21:46,942 - INFO - - GUI available at http://127.0.0.1:47334/ 2022-06-28 16:21:47,010 - INFO - Starting MindsDB Mysql proxy server on tcp://127.0.0.1:47335 2022-06-28 16:21:47,015 - INFO - Waiting for incoming connections... mysql API: started on 47335 http API: started on 47334 What's Next? \u00b6 Now that you are all set, we recommend you check out our Tutorials and Community Tutorials sections, where you'll find various examples of regression, classification, and time series predictions with MindsDB. To learn more about MindsDB itself, follow the guide on MindsDB database structure . Also, don't miss out on the remaining pages from the SQL API section, as they explain a common SQL syntax with examples. Have fun!","title":"MindsDB Installation for Development"},{"location":"contribute/install/#installing-mindsdb-for-development","text":"If you want to contribute to the development of MindsDB, you need to install it first. There are a few installation options that you can choose as follows: from source , using PyPi for Windows , Linux , or MacOS , using Docker image . Our preferred MindsDB installation method for development is the installation from source.","title":"Installing MindsDB for Development"},{"location":"contribute/install/#installing-mindsdb","text":"Here, we recall the steps of MindsDB installation from source. You can either follow the steps below or visit the provided link. Before proceeding, make sure you have Git and Python 3.7.x or Python 3.8.x installed. Fork the MindsDB repository from GitHub . Clone the fork locally: git clone git@github.com:YOUR_USERNAME/mindsdb.git Create a new virtual environment: python3 -m venv mindsdb-venv Activate the virtual environment: source mindsdb-venv/bin/activate Install dependencies: cd mindsdb & pip install -r requirements.txt Install MindsDB: python setup.py develop Start MindsDB: python3 -m mindsdb If everything works as expected, you should see the following message in the console: ... 2022-06-28 16:21:46,942 - INFO - - GUI available at http://127.0.0.1:47334/ 2022-06-28 16:21:47,010 - INFO - Starting MindsDB Mysql proxy server on tcp://127.0.0.1:47335 2022-06-28 16:21:47,015 - INFO - Waiting for incoming connections... mysql API: started on 47335 http API: started on 47334","title":"Installing MindsDB"},{"location":"contribute/install/#whats-next","text":"Now that you are all set, we recommend you check out our Tutorials and Community Tutorials sections, where you'll find various examples of regression, classification, and time series predictions with MindsDB. To learn more about MindsDB itself, follow the guide on MindsDB database structure . Also, don't miss out on the remaining pages from the SQL API section, as they explain a common SQL syntax with examples. Have fun!","title":"What's Next?"},{"location":"contribute/integrations/","text":"Building a New Integration \u00b6 In this section, you'll find how to add new integrations to MindsDB, either as data layers or predictive frameworks . Prerequisite \u00b6 You should have the latest staging version of the MindsDB repository installed locally. Follow this guide to learn how to install MindsDB for development. What are Handlers? \u00b6 At the heart of the MindsDB philosophy lies the belief that predictive insights are best leveraged when produced as close as possible to the data layer. Usually, this data layer is a SQL-compatible database, but it could also be a non-SQL database, data stream, or any other tool that interacts with data stored somewhere else. The above description fits an enormous set of tools used across the software industry. The complexity increases further by bringing Machine Learning into the equation, as the set of popular ML tools is similarly huge. We aim to support most technology stacks, requiring a simple integration procedure so that anyone can easily contribute the necessary glue to enable any predictive system for usage within data layers. This motivates the concept of handlers , which is an abstraction for the two types of entities mentioned above: data layers and predictive frameworks . Handlers are meant to enforce a common and sufficient set of behaviors that all MindsDB-compatible entities should support. By creating a handler, the target system is effectively integrated into the wider MindsDB ecosystem. Handlers in the MindsDB Repository \u00b6 The codes for integrations are located in the main MindsDB repository under the /integrations directory. integrations # Contains integrations source codes \u251c\u2500 handlers/ # Each integration has its own handler directory \u2502 \u251c\u2500 mysql_handler/ # MySQL integration code \u2502 \u251c\u2500 lightwood_handler/ # Lightwood integration code \u2502 \u251c\u2500 .../ # Other handlers \u251c\u2500 libs/ # Handler libraries directory \u2502 \u251c\u2500 base_handler.py # Each handler class inherits from this base class \u2502 \u251c\u2500 storage_handler.py # Storage classes used by handlers \u2514\u2500 utilities # Handler utility directory \u2502 \u251c\u2500 install.py # Script that installs all handler dependencies Structure of a Handler \u00b6 In technical terms, a handler is a self-contained Python package having everything required for MindsDB to interact with it. It includes aspects like dependencies, unit tests, and continuous integration logic. It is up to the author to determine the nature of the package, for example, closed or open source, version control, and more. Although, we encourage opening pull requests to expand the default set of supported tools. The entry point is a class definition that should inherit either from the integrations.libs.base.DatabaseHandler class or the integrations.libs.base.PredictiveHandler class, depending on the type of the handler. The integrations.libs.base.BaseHandler class defines all the methods that must be overwritten in order to achieve a functional implementation. Handler's Structure The handler's structure is not enforced, and the package design is up to the author. Core Methods \u00b6 Apart from the __init__() method, there are seven core methods that the Handler class has to implement, these are: connect() , disconnect() , check_connection() , native_query() , query() , get_tables() , get_columns() . We recommend checking actual examples in the codebase to get an idea of what goes into each of these methods, as they can change a bit depending on the nature of the system being integrated. Let's review the purpose of each method. Method Purpose connect() It performs the necessary steps to connect to the underlying system. disconnect() It gracefully closes connections established in the connect() method. check_connection() It evaluates if the connection is alive and healthy. This method is called frequently. native_query() It parses any native statement string and acts upon it (for example, raw SQL commands). query() It takes a parsed SQL command in the form of an abstract syntax tree and executes it. A good example is the CREATE MODEL statement for predictive handlers, which is a non-native syntax as databases have no notion of a MODEL entity. get_tables() It lists and returns all the available tables. Each handler decides what a table means for the underlying system when interacting with it from the data layer. Typically, these are actual tables for data handlers and predictive handlers. get_columns() It returns columns of a table registered in the handler with the respective data type. Authors can opt for adding private methods, new files and folders, or any combination of these to structure all the necessary work that will enable the core methods to work as intended. Other Common Methods Under the mindsdb.integrations.libs.utils library, contributors can find various methods that may be useful while implementing new handlers, especially the predictive handlers. Predictor-Specific Methods \u00b6 For predictive handlers, there is an additional method that must be implemented, the join() method. Method Purpose join() It triggers a specific model to generate predictions when given some input data. This behavior manifests in the SQL API when performing any type of JOIN operations between tables from a predictive handler and tables from a data handler. Parsing SQL \u00b6 Whenever you want to parse a string that contains SQL, we strongly recommend using the mindsdb_sql package. It provides a parser that fully supports the MindsDB SQL dialect and partially the standard SQL dialect. There is also a render feature to map other dialects into the already supported ones. Storing Internal State \u00b6 Most handlers need to store internal metadata ranging from a list of registered tables to implementation-specific details that greatly vary from one case to another. The recommendation for storing these bits of information is to use storage handlers from the integrations.libs.storage_handler library. We currently support two options: an SQLite backend and a Redis backend. In both cases, the premise is the same: a key-value store system is set up such that interfaces are simple and clean, exposing only the get() and set() methods for usage within the data and predictive handlers. ML Frameworks In the case of ML frameworks, opt for storing the path to your model weights inside the KV storage and saving weights in optimized formats preferred by the system, like .h5 . Formatting Output \u00b6 When it comes to building the response of the public methods, the output should be wrapped by the HandlerResponse or HandlerStatusResponse class located in the mindsdb.integrations.libs.response library. These classes are used by the MindsDB executioner to orchestrate and coordinate multiple handler instances in parallel. How to Write a Handler \u00b6 Let's go through all the above information step by step. Extending the Classes Extend the DatabaseHandler class when adding a new data handler. Extend the PredictiveHandler class when adding a new predictive handler. The seven core methods must be implemented, both for a data handler and a predictive handler. The connect() method sets up storage and connection. The disconnect() method terminates the connection. The check_connection() method validates the connection. The native_query() method acts upon a raw SQL command. The query() method acts upon a parsed SQL command. The get_tables() method lists all accessible entities within the handler. The get_columns() method returns columns for a specific table entity. And additionally, for predictive handlers: The join() method calls other handlers to merge data with predictions. Creating a Database Handler \u00b6 Below is the list of entities required to create a database handler. Creating the Handler Class \u00b6 Each database handler should inherit from the DatabaseHandler class. Setting the name class property: MindsDB uses it internally as a name of the handler. For example, the CREATE DATABASE statement uses the handler's name. CREATE DATABASE integration_name WITH ENGINE = 'postgres' , --- here, the handler's name is `postgres` PARAMETERS = { 'host' : '127.0.0.1' , 'user' : 'root' , 'password' : 'password' } ; Implementing the __init__() method: This method initializes the handler. The connection_data argument contains the PARAMETERS from the CREATE DATABASE statement, such as user , password , etc. def __init__ ( self , name : str , connection_data : Optional [ dict ], ** kwargs ) \"\"\" Initialize the handler Args: name (str): name of particular handler instance connection_data (dict): parameters for connecting to the database **kwargs: arbitrary keyword arguments. \"\"\" Implementing the connect() method: The connect() method sets up the connection. def connect ( self ) -> HandlerStatusResponse : \"\"\" Set up any connections required by the handler Should return output of check_connection() method after attempting connection. Should switch self.is_connected. Returns: HandlerStatusResponse \"\"\" Implementing the disconnect() method: The disconnect() method closes the existing connection. def disconnect ( self ): \"\"\" Close any existing connections Should switch self.is_connected. \"\"\" self . is_connected = False return self . is_connected Implementing the check_connection() method: The check_connection() method performs the health check for the connection. def check_connection ( self ) -> HandlerStatusResponse : \"\"\" Check connection to the handler Returns: HandlerStatusResponse \"\"\" Implementing the native_query() method: The native_query() method runs commands of the native database language. def native_query ( self , query : Any ) -> HandlerStatusResponse : \"\"\"Receive raw query and act upon it somehow. Args: query (Any): query in native format (str for sql databases, dict for mongo, etc) Returns: HandlerResponse \"\"\" Implementing the query() method: The query method runs parsed SQL commands. def query ( self , query : ASTNode ) -> HandlerStatusResponse : \"\"\"Receive query as AST (abstract syntax tree) and act upon it somehow. Args: query (ASTNode): sql query represented as AST. May be any kind of query: SELECT, INTSERT, DELETE, etc Returns: HandlerResponse \"\"\" Implementing the get_tables() method: The get_tables() method lists all the available tables. def get_tables ( self ) -> HandlerStatusResponse : \"\"\" Return list of entities Return list of entities that will be accesible as tables. Returns: HandlerResponse: shoud have same columns as information_schema.tables (https://dev.mysql.com/doc/refman/8.0/en/information-schema-tables-table.html) Column 'TABLE_NAME' is mandatory, other is optional. \"\"\" Implementing the get_columns() method: The get_columns() method lists all columns of a specified table. def get_columns ( self , table_name : str ) -> HandlerStatusResponse : \"\"\" Returns a list of entity columns Args: table_name (str): name of one of tables returned by self.get_tables() Returns: HandlerResponse: shoud have same columns as information_schema.columns (https://dev.mysql.com/doc/refman/8.0/en/information-schema-columns-table.html) Column 'COLUMN_NAME' is mandatory, other is optional. Hightly recomended to define also 'DATA_TYPE': it should be one of python data types (by default it str). \"\"\" Creating the connection_args Dictionary \u00b6 The connection_arg dictionary contains all required arguments to establish the connection. Here is an example of the connection_arg dictionary from the MySQL handler . connection_args = OrderedDict ( user = { 'type' : ARG_TYPE . STR , 'description' : 'The user name used to authenticate with the MySQL server.' }, password = { 'type' : ARG_TYPE . STR , 'description' : 'The password to authenticate the user with the MySQL server.' }, database = { 'type' : ARG_TYPE . STR , 'description' : 'The database name to use when connecting with the MySQL server.' }, host = { 'type' : ARG_TYPE . STR , 'description' : 'The host name or IP address of the MySQL server. NOTE: use \\' 127.0.0.1 \\' instead of \\' localhost \\' to connect to local server.' }, port = { 'type' : ARG_TYPE . INT , 'description' : 'The TCP/IP port of the MySQL server. Must be an integer.' }, ssl = { 'type' : ARG_TYPE . BOOL , 'description' : 'Set it to False to disable ssl.' }, ssl_ca = { 'type' : ARG_TYPE . PATH , 'description' : 'Path or URL of the Certificate Authority (CA) certificate file' }, ssl_cert = { 'type' : ARG_TYPE . PATH , 'description' : 'Path name or URL of the server public key certificate file' }, ssl_key = { 'type' : ARG_TYPE . PATH , 'description' : 'The path name or URL of the server private key file' } ) Creating the connection_args_example Dictionary \u00b6 The connection_args_example dictionary contains an example of all required arguments to establish the connection. Here is an example of the connection_args_example dictionary from the MySQL handler . connection_args_example = OrderedDict ( host = '127.0.0.1' , port = 3306 , user = 'root' , password = 'password' , database = 'database' ) Exporting All Required Variables \u00b6 This is what should be exported in the __init__.py file: The Handler class. The version of the handler. The name of the handler. The type of the handler, either DATA handler or ML handler. The icon_path to the file with the database icon. The title of the handler or a short description. The description of the handler. The connection_args dictionary with the connection arguments. The connection_args_example dictionary with an example of the connection arguments. The import_error message that is used if the import of the Handler class fails. Let's look at an example of the __init__.py file. ... title = 'Trino' version = 0.1 description = 'Integration for connection to TrinoDB' name = 'trino' type = HANDLER_TYPE . DATA icon_path = 'icon.png' __all__ = [ 'Handler' , 'version' , 'name' , 'type' , 'title' , 'description' , 'connection_args_example' , 'icon_path' ] Check out our Handlers! \u00b6 To see some integration handlers that are currently in use, we encourage you to check out the following handlers inside the MindsDB repository: MySQL Postgres MLflow Lightwood And here are all the handlers available in the MindsDB repository .","title":"Building a New Integration"},{"location":"contribute/integrations/#building-a-new-integration","text":"In this section, you'll find how to add new integrations to MindsDB, either as data layers or predictive frameworks .","title":"Building a New Integration"},{"location":"contribute/integrations/#prerequisite","text":"You should have the latest staging version of the MindsDB repository installed locally. Follow this guide to learn how to install MindsDB for development.","title":"Prerequisite"},{"location":"contribute/integrations/#what-are-handlers","text":"At the heart of the MindsDB philosophy lies the belief that predictive insights are best leveraged when produced as close as possible to the data layer. Usually, this data layer is a SQL-compatible database, but it could also be a non-SQL database, data stream, or any other tool that interacts with data stored somewhere else. The above description fits an enormous set of tools used across the software industry. The complexity increases further by bringing Machine Learning into the equation, as the set of popular ML tools is similarly huge. We aim to support most technology stacks, requiring a simple integration procedure so that anyone can easily contribute the necessary glue to enable any predictive system for usage within data layers. This motivates the concept of handlers , which is an abstraction for the two types of entities mentioned above: data layers and predictive frameworks . Handlers are meant to enforce a common and sufficient set of behaviors that all MindsDB-compatible entities should support. By creating a handler, the target system is effectively integrated into the wider MindsDB ecosystem.","title":"What are Handlers?"},{"location":"contribute/integrations/#handlers-in-the-mindsdb-repository","text":"The codes for integrations are located in the main MindsDB repository under the /integrations directory. integrations # Contains integrations source codes \u251c\u2500 handlers/ # Each integration has its own handler directory \u2502 \u251c\u2500 mysql_handler/ # MySQL integration code \u2502 \u251c\u2500 lightwood_handler/ # Lightwood integration code \u2502 \u251c\u2500 .../ # Other handlers \u251c\u2500 libs/ # Handler libraries directory \u2502 \u251c\u2500 base_handler.py # Each handler class inherits from this base class \u2502 \u251c\u2500 storage_handler.py # Storage classes used by handlers \u2514\u2500 utilities # Handler utility directory \u2502 \u251c\u2500 install.py # Script that installs all handler dependencies","title":"Handlers in the MindsDB Repository"},{"location":"contribute/integrations/#structure-of-a-handler","text":"In technical terms, a handler is a self-contained Python package having everything required for MindsDB to interact with it. It includes aspects like dependencies, unit tests, and continuous integration logic. It is up to the author to determine the nature of the package, for example, closed or open source, version control, and more. Although, we encourage opening pull requests to expand the default set of supported tools. The entry point is a class definition that should inherit either from the integrations.libs.base.DatabaseHandler class or the integrations.libs.base.PredictiveHandler class, depending on the type of the handler. The integrations.libs.base.BaseHandler class defines all the methods that must be overwritten in order to achieve a functional implementation. Handler's Structure The handler's structure is not enforced, and the package design is up to the author.","title":"Structure of a Handler"},{"location":"contribute/integrations/#core-methods","text":"Apart from the __init__() method, there are seven core methods that the Handler class has to implement, these are: connect() , disconnect() , check_connection() , native_query() , query() , get_tables() , get_columns() . We recommend checking actual examples in the codebase to get an idea of what goes into each of these methods, as they can change a bit depending on the nature of the system being integrated. Let's review the purpose of each method. Method Purpose connect() It performs the necessary steps to connect to the underlying system. disconnect() It gracefully closes connections established in the connect() method. check_connection() It evaluates if the connection is alive and healthy. This method is called frequently. native_query() It parses any native statement string and acts upon it (for example, raw SQL commands). query() It takes a parsed SQL command in the form of an abstract syntax tree and executes it. A good example is the CREATE MODEL statement for predictive handlers, which is a non-native syntax as databases have no notion of a MODEL entity. get_tables() It lists and returns all the available tables. Each handler decides what a table means for the underlying system when interacting with it from the data layer. Typically, these are actual tables for data handlers and predictive handlers. get_columns() It returns columns of a table registered in the handler with the respective data type. Authors can opt for adding private methods, new files and folders, or any combination of these to structure all the necessary work that will enable the core methods to work as intended. Other Common Methods Under the mindsdb.integrations.libs.utils library, contributors can find various methods that may be useful while implementing new handlers, especially the predictive handlers.","title":"Core Methods"},{"location":"contribute/integrations/#predictor-specific-methods","text":"For predictive handlers, there is an additional method that must be implemented, the join() method. Method Purpose join() It triggers a specific model to generate predictions when given some input data. This behavior manifests in the SQL API when performing any type of JOIN operations between tables from a predictive handler and tables from a data handler.","title":"Predictor-Specific Methods"},{"location":"contribute/integrations/#parsing-sql","text":"Whenever you want to parse a string that contains SQL, we strongly recommend using the mindsdb_sql package. It provides a parser that fully supports the MindsDB SQL dialect and partially the standard SQL dialect. There is also a render feature to map other dialects into the already supported ones.","title":"Parsing SQL"},{"location":"contribute/integrations/#storing-internal-state","text":"Most handlers need to store internal metadata ranging from a list of registered tables to implementation-specific details that greatly vary from one case to another. The recommendation for storing these bits of information is to use storage handlers from the integrations.libs.storage_handler library. We currently support two options: an SQLite backend and a Redis backend. In both cases, the premise is the same: a key-value store system is set up such that interfaces are simple and clean, exposing only the get() and set() methods for usage within the data and predictive handlers. ML Frameworks In the case of ML frameworks, opt for storing the path to your model weights inside the KV storage and saving weights in optimized formats preferred by the system, like .h5 .","title":"Storing Internal State"},{"location":"contribute/integrations/#formatting-output","text":"When it comes to building the response of the public methods, the output should be wrapped by the HandlerResponse or HandlerStatusResponse class located in the mindsdb.integrations.libs.response library. These classes are used by the MindsDB executioner to orchestrate and coordinate multiple handler instances in parallel.","title":"Formatting Output"},{"location":"contribute/integrations/#how-to-write-a-handler","text":"Let's go through all the above information step by step. Extending the Classes Extend the DatabaseHandler class when adding a new data handler. Extend the PredictiveHandler class when adding a new predictive handler. The seven core methods must be implemented, both for a data handler and a predictive handler. The connect() method sets up storage and connection. The disconnect() method terminates the connection. The check_connection() method validates the connection. The native_query() method acts upon a raw SQL command. The query() method acts upon a parsed SQL command. The get_tables() method lists all accessible entities within the handler. The get_columns() method returns columns for a specific table entity. And additionally, for predictive handlers: The join() method calls other handlers to merge data with predictions.","title":"How to Write a Handler"},{"location":"contribute/integrations/#creating-a-database-handler","text":"Below is the list of entities required to create a database handler.","title":"Creating a Database Handler"},{"location":"contribute/integrations/#creating-the-handler-class","text":"Each database handler should inherit from the DatabaseHandler class. Setting the name class property: MindsDB uses it internally as a name of the handler. For example, the CREATE DATABASE statement uses the handler's name. CREATE DATABASE integration_name WITH ENGINE = 'postgres' , --- here, the handler's name is `postgres` PARAMETERS = { 'host' : '127.0.0.1' , 'user' : 'root' , 'password' : 'password' } ; Implementing the __init__() method: This method initializes the handler. The connection_data argument contains the PARAMETERS from the CREATE DATABASE statement, such as user , password , etc. def __init__ ( self , name : str , connection_data : Optional [ dict ], ** kwargs ) \"\"\" Initialize the handler Args: name (str): name of particular handler instance connection_data (dict): parameters for connecting to the database **kwargs: arbitrary keyword arguments. \"\"\" Implementing the connect() method: The connect() method sets up the connection. def connect ( self ) -> HandlerStatusResponse : \"\"\" Set up any connections required by the handler Should return output of check_connection() method after attempting connection. Should switch self.is_connected. Returns: HandlerStatusResponse \"\"\" Implementing the disconnect() method: The disconnect() method closes the existing connection. def disconnect ( self ): \"\"\" Close any existing connections Should switch self.is_connected. \"\"\" self . is_connected = False return self . is_connected Implementing the check_connection() method: The check_connection() method performs the health check for the connection. def check_connection ( self ) -> HandlerStatusResponse : \"\"\" Check connection to the handler Returns: HandlerStatusResponse \"\"\" Implementing the native_query() method: The native_query() method runs commands of the native database language. def native_query ( self , query : Any ) -> HandlerStatusResponse : \"\"\"Receive raw query and act upon it somehow. Args: query (Any): query in native format (str for sql databases, dict for mongo, etc) Returns: HandlerResponse \"\"\" Implementing the query() method: The query method runs parsed SQL commands. def query ( self , query : ASTNode ) -> HandlerStatusResponse : \"\"\"Receive query as AST (abstract syntax tree) and act upon it somehow. Args: query (ASTNode): sql query represented as AST. May be any kind of query: SELECT, INTSERT, DELETE, etc Returns: HandlerResponse \"\"\" Implementing the get_tables() method: The get_tables() method lists all the available tables. def get_tables ( self ) -> HandlerStatusResponse : \"\"\" Return list of entities Return list of entities that will be accesible as tables. Returns: HandlerResponse: shoud have same columns as information_schema.tables (https://dev.mysql.com/doc/refman/8.0/en/information-schema-tables-table.html) Column 'TABLE_NAME' is mandatory, other is optional. \"\"\" Implementing the get_columns() method: The get_columns() method lists all columns of a specified table. def get_columns ( self , table_name : str ) -> HandlerStatusResponse : \"\"\" Returns a list of entity columns Args: table_name (str): name of one of tables returned by self.get_tables() Returns: HandlerResponse: shoud have same columns as information_schema.columns (https://dev.mysql.com/doc/refman/8.0/en/information-schema-columns-table.html) Column 'COLUMN_NAME' is mandatory, other is optional. Hightly recomended to define also 'DATA_TYPE': it should be one of python data types (by default it str). \"\"\"","title":"Creating the Handler Class"},{"location":"contribute/integrations/#creating-the-connection_args-dictionary","text":"The connection_arg dictionary contains all required arguments to establish the connection. Here is an example of the connection_arg dictionary from the MySQL handler . connection_args = OrderedDict ( user = { 'type' : ARG_TYPE . STR , 'description' : 'The user name used to authenticate with the MySQL server.' }, password = { 'type' : ARG_TYPE . STR , 'description' : 'The password to authenticate the user with the MySQL server.' }, database = { 'type' : ARG_TYPE . STR , 'description' : 'The database name to use when connecting with the MySQL server.' }, host = { 'type' : ARG_TYPE . STR , 'description' : 'The host name or IP address of the MySQL server. NOTE: use \\' 127.0.0.1 \\' instead of \\' localhost \\' to connect to local server.' }, port = { 'type' : ARG_TYPE . INT , 'description' : 'The TCP/IP port of the MySQL server. Must be an integer.' }, ssl = { 'type' : ARG_TYPE . BOOL , 'description' : 'Set it to False to disable ssl.' }, ssl_ca = { 'type' : ARG_TYPE . PATH , 'description' : 'Path or URL of the Certificate Authority (CA) certificate file' }, ssl_cert = { 'type' : ARG_TYPE . PATH , 'description' : 'Path name or URL of the server public key certificate file' }, ssl_key = { 'type' : ARG_TYPE . PATH , 'description' : 'The path name or URL of the server private key file' } )","title":"Creating the connection_args Dictionary"},{"location":"contribute/integrations/#creating-the-connection_args_example-dictionary","text":"The connection_args_example dictionary contains an example of all required arguments to establish the connection. Here is an example of the connection_args_example dictionary from the MySQL handler . connection_args_example = OrderedDict ( host = '127.0.0.1' , port = 3306 , user = 'root' , password = 'password' , database = 'database' )","title":"Creating the connection_args_example Dictionary"},{"location":"contribute/integrations/#exporting-all-required-variables","text":"This is what should be exported in the __init__.py file: The Handler class. The version of the handler. The name of the handler. The type of the handler, either DATA handler or ML handler. The icon_path to the file with the database icon. The title of the handler or a short description. The description of the handler. The connection_args dictionary with the connection arguments. The connection_args_example dictionary with an example of the connection arguments. The import_error message that is used if the import of the Handler class fails. Let's look at an example of the __init__.py file. ... title = 'Trino' version = 0.1 description = 'Integration for connection to TrinoDB' name = 'trino' type = HANDLER_TYPE . DATA icon_path = 'icon.png' __all__ = [ 'Handler' , 'version' , 'name' , 'type' , 'title' , 'description' , 'connection_args_example' , 'icon_path' ]","title":"Exporting All Required Variables"},{"location":"contribute/integrations/#check-out-our-handlers","text":"To see some integration handlers that are currently in use, we encourage you to check out the following handlers inside the MindsDB repository: MySQL Postgres MLflow Lightwood And here are all the handlers available in the MindsDB repository .","title":"Check out our Handlers!"},{"location":"contribute/issues/","text":"Report an Issue \u00b6 Do you want to report a bug or submit a feature request ? You can do that on the MindsDB GitHub issues page . Before reporting a new issue, whether it is a bug report or a feature request, please make sure that it is not already there. Bug Report \u00b6 Here is how to get started when you want to report a bug. First, go to our GitHub issues page and click on the New issue button. Now you have to choose what kind of issue you want to report. Here, we choose Bug report and click on the Get started button. Now, it's time to fill up the form. In the first textbox, add a meaningful title for your issue. Also, don't forget to check whether a similar issue already exists. If not, mark the checkbox. Please note that these fields are mandatory. Here, you describe the current behavior. Please note that this field is mandatory. If you know what the expected behavior should be, you can add it here. It is helpful for us if you add the steps you followed that led you to the error. Any links, references, logs, screenshots, etc., are welcome! Remember All contributions to our repository should follow the contributing guidelines and code of conduct . Thank you for reporting a bug! It helps us improve MindsDB for you and future users. Feature Request \u00b6 Here is how to get started when you want to submit a feature request. First, go to our GitHub issues page and click on the New issue button. Now you have to choose what kind of issue you want to report. Here, we choose Feature request and click on the Get started button. Now, it's time to fill up the form. In the first textbox, add a meaningful title for your issue. Also, don't forget to check whether a similar issue already exists. If not, mark the checkbox. Please note that these fields are mandatory. Here, you describe the feature request and indicate whether it is related to some error/problem. Please note that this field is mandatory. If you know how the solution should look, you can add it here. If you have an alternate solution, you can add it here. Any links, references, logs, screenshots, etc., are welcome! Remember All contributions to our repository should follow the contributing guidelines and code of conduct . Thank you for submitting a feature request! It helps us improve MindsDB for you and future users. Issue Review \u00b6 Issues are reviewed regularly, usually daily. Depending on the issue type, it will be labeled as Bug or enhancement . Please make sure you respond to our feedback/questions regarding your issue.","title":"Bug Reports and Feature Requests"},{"location":"contribute/issues/#report-an-issue","text":"Do you want to report a bug or submit a feature request ? You can do that on the MindsDB GitHub issues page . Before reporting a new issue, whether it is a bug report or a feature request, please make sure that it is not already there.","title":"Report an Issue"},{"location":"contribute/issues/#bug-report","text":"Here is how to get started when you want to report a bug. First, go to our GitHub issues page and click on the New issue button. Now you have to choose what kind of issue you want to report. Here, we choose Bug report and click on the Get started button. Now, it's time to fill up the form. In the first textbox, add a meaningful title for your issue. Also, don't forget to check whether a similar issue already exists. If not, mark the checkbox. Please note that these fields are mandatory. Here, you describe the current behavior. Please note that this field is mandatory. If you know what the expected behavior should be, you can add it here. It is helpful for us if you add the steps you followed that led you to the error. Any links, references, logs, screenshots, etc., are welcome! Remember All contributions to our repository should follow the contributing guidelines and code of conduct . Thank you for reporting a bug! It helps us improve MindsDB for you and future users.","title":"Bug Report"},{"location":"contribute/issues/#feature-request","text":"Here is how to get started when you want to submit a feature request. First, go to our GitHub issues page and click on the New issue button. Now you have to choose what kind of issue you want to report. Here, we choose Feature request and click on the Get started button. Now, it's time to fill up the form. In the first textbox, add a meaningful title for your issue. Also, don't forget to check whether a similar issue already exists. If not, mark the checkbox. Please note that these fields are mandatory. Here, you describe the feature request and indicate whether it is related to some error/problem. Please note that this field is mandatory. If you know how the solution should look, you can add it here. If you have an alternate solution, you can add it here. Any links, references, logs, screenshots, etc., are welcome! Remember All contributions to our repository should follow the contributing guidelines and code of conduct . Thank you for submitting a feature request! It helps us improve MindsDB for you and future users.","title":"Feature Request"},{"location":"contribute/issues/#issue-review","text":"Issues are reviewed regularly, usually daily. Depending on the issue type, it will be labeled as Bug or enhancement . Please make sure you respond to our feedback/questions regarding your issue.","title":"Issue Review"},{"location":"contribute/ml_handlers/","text":"Building a Machine Learning Handler \u00b6 In this section, you'll find how to create new machine learning (ML) handlers within MindsDB. Prerequisite \u00b6 You should have the latest staging version of the MindsDB repository installed locally. Follow this guide to learn how to install MindsDB for development. What are Machine Learning Handlers? \u00b6 ML handlers act as a bridge to any ML framework. You use ML handlers to create ML engines using the CREATE ML_ENGINE command . So you can expose ML models from any supported ML engine as an AI table. ML Handlers in the MindsDB Repository \u00b6 The source code for ML handlers is located in the main MindsDB repository under the /integrations directory. integrations # Contains ML handlers source codes \u251c\u2500 handlers/ # Each ML engine has its own handler directory \u2502 \u251c\u2500 huggingface_handler/ # HuggingFace code \u2502 \u251c\u2500 lightwood_handler/ # Lightwood code \u2502 \u251c\u2500 .../ # Other handlers \u251c\u2500 libs/ # Handler libraries directory \u2502 \u251c\u2500 base.py # Each ML handler class inherits from the BaseMLEngine base class \u2514\u2500 utilities # Handler utility directory \u2502 \u251c\u2500 install.py # Script that installs all handler dependencies Creating a Machine Learning Handler \u00b6 You can create your own ML handler within MindsDB by inheriting from the BaseMLEngine class . By providing implementation for some or all of the methods contained in the BaseMLEngine class, you can connect with the machine learning library or framework of your choice. Core Methods \u00b6 Apart from the __init__() method, there are five methods, of which two must be implemented. Let's review the purpose of each method. Method Purpose create() It creates a model inside the engine registry. predict() It calls a model and returns prediction data. update() Optional. It updates an existing model without resetting its internal structure. describe() Optional. It provides global models insights. create_engine() Optional. It connects with external sources, such as REST API. Implementing Mandatory Methods \u00b6 Here are the methods that must be implemented while inheriting from the BaseMLEngine class: The create() method saves a model inside the engine registry for later usage. def create ( self , target : str , df : Optional [ pd . DataFrame ] = None , args : Optional [ Dict ] = None ) -> None : \"\"\" Saves a model inside the engine registry for later usage. Normally, an input dataframe is required to train the model. However, some integrations may merely require registering the model instead of training, in which case `df` can be omitted. Any other arguments required to register the model can be passed in an `args` dictionary. \"\"\" The predict() method calls a model with an input dataframe and optionally, arguments to modify model's behaviour. This method returns a dataframe with the predicted values. def predict ( self , df : pd . DataFrame , args : Optional [ Dict ] = None ) -> pd . DataFrame : \"\"\" Calls a model with some input dataframe `df`, and optionally some arguments `args` that may modify the model behavior. The expected output is a dataframe with the predicted values in the target-named column. Additional columns can be present, and will be considered row-wise explanations if their names finish with `_explain`. \"\"\" Implementing Optional Methods \u00b6 Here are the optional methods that you can implement alongside the mandatory ones if your ML framework allows it: The update() method is used to update, fine-tune, or adjust an existing model without resetting its internal state. def update ( self , df : Optional [ pd . DataFrame ] = None , args : Optional [ Dict ] = None ) -> None : \"\"\" Optional. Used to update/fine-tune/adjust a pre-existing model without resetting its internal state (e.g. weights). Availability will depend on underlying integration support, as not all ML models can be partially updated. \"\"\" The describe() method provides global models insights, such as framework-level parameters used in training. def describe ( self , key : Optional [ str ] = None ) -> pd . DataFrame : \"\"\" Optional. When called, this method provides global model insights, e.g. framework-level parameters used in training. \"\"\" The create_engine() method is used to connect with the external sources, such as REST API. def create_engine ( self , connection_args : dict ): \"\"\" Optional. Used to connect with external sources (e.g. a REST API) that the engine will require to use any other methods. \"\"\" Check out our Machine Learning Handlers! \u00b6 To see some ML handlers that are currently in use, we encourage you to check out the following ML handlers inside the MindsDB repository: Lightwood HuggingFace Ludwig And here are all the handlers available in the MindsDB repository .","title":"Building an ML Engine"},{"location":"contribute/ml_handlers/#building-a-machine-learning-handler","text":"In this section, you'll find how to create new machine learning (ML) handlers within MindsDB.","title":"Building a Machine Learning Handler"},{"location":"contribute/ml_handlers/#prerequisite","text":"You should have the latest staging version of the MindsDB repository installed locally. Follow this guide to learn how to install MindsDB for development.","title":"Prerequisite"},{"location":"contribute/ml_handlers/#what-are-machine-learning-handlers","text":"ML handlers act as a bridge to any ML framework. You use ML handlers to create ML engines using the CREATE ML_ENGINE command . So you can expose ML models from any supported ML engine as an AI table.","title":"What are Machine Learning Handlers?"},{"location":"contribute/ml_handlers/#ml-handlers-in-the-mindsdb-repository","text":"The source code for ML handlers is located in the main MindsDB repository under the /integrations directory. integrations # Contains ML handlers source codes \u251c\u2500 handlers/ # Each ML engine has its own handler directory \u2502 \u251c\u2500 huggingface_handler/ # HuggingFace code \u2502 \u251c\u2500 lightwood_handler/ # Lightwood code \u2502 \u251c\u2500 .../ # Other handlers \u251c\u2500 libs/ # Handler libraries directory \u2502 \u251c\u2500 base.py # Each ML handler class inherits from the BaseMLEngine base class \u2514\u2500 utilities # Handler utility directory \u2502 \u251c\u2500 install.py # Script that installs all handler dependencies","title":"ML Handlers in the MindsDB Repository"},{"location":"contribute/ml_handlers/#creating-a-machine-learning-handler","text":"You can create your own ML handler within MindsDB by inheriting from the BaseMLEngine class . By providing implementation for some or all of the methods contained in the BaseMLEngine class, you can connect with the machine learning library or framework of your choice.","title":"Creating a Machine Learning Handler"},{"location":"contribute/ml_handlers/#core-methods","text":"Apart from the __init__() method, there are five methods, of which two must be implemented. Let's review the purpose of each method. Method Purpose create() It creates a model inside the engine registry. predict() It calls a model and returns prediction data. update() Optional. It updates an existing model without resetting its internal structure. describe() Optional. It provides global models insights. create_engine() Optional. It connects with external sources, such as REST API.","title":"Core Methods"},{"location":"contribute/ml_handlers/#implementing-mandatory-methods","text":"Here are the methods that must be implemented while inheriting from the BaseMLEngine class: The create() method saves a model inside the engine registry for later usage. def create ( self , target : str , df : Optional [ pd . DataFrame ] = None , args : Optional [ Dict ] = None ) -> None : \"\"\" Saves a model inside the engine registry for later usage. Normally, an input dataframe is required to train the model. However, some integrations may merely require registering the model instead of training, in which case `df` can be omitted. Any other arguments required to register the model can be passed in an `args` dictionary. \"\"\" The predict() method calls a model with an input dataframe and optionally, arguments to modify model's behaviour. This method returns a dataframe with the predicted values. def predict ( self , df : pd . DataFrame , args : Optional [ Dict ] = None ) -> pd . DataFrame : \"\"\" Calls a model with some input dataframe `df`, and optionally some arguments `args` that may modify the model behavior. The expected output is a dataframe with the predicted values in the target-named column. Additional columns can be present, and will be considered row-wise explanations if their names finish with `_explain`. \"\"\"","title":"Implementing Mandatory Methods"},{"location":"contribute/ml_handlers/#implementing-optional-methods","text":"Here are the optional methods that you can implement alongside the mandatory ones if your ML framework allows it: The update() method is used to update, fine-tune, or adjust an existing model without resetting its internal state. def update ( self , df : Optional [ pd . DataFrame ] = None , args : Optional [ Dict ] = None ) -> None : \"\"\" Optional. Used to update/fine-tune/adjust a pre-existing model without resetting its internal state (e.g. weights). Availability will depend on underlying integration support, as not all ML models can be partially updated. \"\"\" The describe() method provides global models insights, such as framework-level parameters used in training. def describe ( self , key : Optional [ str ] = None ) -> pd . DataFrame : \"\"\" Optional. When called, this method provides global model insights, e.g. framework-level parameters used in training. \"\"\" The create_engine() method is used to connect with the external sources, such as REST API. def create_engine ( self , connection_args : dict ): \"\"\" Optional. Used to connect with external sources (e.g. a REST API) that the engine will require to use any other methods. \"\"\"","title":"Implementing Optional Methods"},{"location":"contribute/ml_handlers/#check-out-our-machine-learning-handlers","text":"To see some ML handlers that are currently in use, we encourage you to check out the following ML handlers inside the MindsDB repository: Lightwood HuggingFace Ludwig And here are all the handlers available in the MindsDB repository .","title":"Check out our Machine Learning Handlers!"},{"location":"contribute/techstack/","text":"Code Contribution for MindsDB Development \u00b6 Wondering what skill set is required to contribute in MindsDB project? What do you need to know to help? \u00b6 If you want to do code contributions, having any of the below skillset can help you get started: Python 3 SQLServer MySQL MongoDB SingleStore MariaDB PostgreSQL ClickHouse Cassandra Oracle Kafka Datastax Astra Snowflake Happy coding!","title":"Code Contribution for MindsDB Development"},{"location":"contribute/techstack/#code-contribution-for-mindsdb-development","text":"Wondering what skill set is required to contribute in MindsDB project?","title":"Code Contribution for MindsDB Development"},{"location":"contribute/techstack/#what-do-you-need-to-know-to-help","text":"If you want to do code contributions, having any of the below skillset can help you get started: Python 3 SQLServer MySQL MongoDB SingleStore MariaDB PostgreSQL ClickHouse Cassandra Oracle Kafka Datastax Astra Snowflake Happy coding!","title":"What do you need to know to help?"},{"location":"contribute/tests/","text":"Work in progress This documentation is WIP.","title":"Testing and Improving Test Coverage"},{"location":"contribute/tutorials/","text":"How to Write a Tutorial \u00b6 This section presents how to write a tutorial with MindsDB. Content of the Tutorial You are free to create your content. However, you should include in your tutorial all the chapters listed here. Introduction \u00b6 Here you introduce the readers to the tutorial. You can describe what dataset you use and what you predict. Data Setup \u00b6 This chapter contains an introduction to the dataset you use. Connecting the Data \u00b6 Let others follow your tutorial by providing information on where to get the data from and how to connect it to MindsDB. Understanding the Data \u00b6 You can briefly introduce the dataset you use. Training a Predictor \u00b6 Here you use the CREATE MODEL command to create a predictor. Status of a Predictor \u00b6 The next step is to check the status of a predictor. If its value is complete , you can proceed to the next chapter. Making Predictions \u00b6 Use the SELECT statement to query for prediction results. It is good to present the output to the readers. Making a Single Prediction \u00b6 In the case of regression and classification predictors, you can make a single prediction. It is good to present the output to the readers. Making Batch Predictions \u00b6 You can make batch predictions using the JOIN clause for all predictor types, such as regression, classification, and time series. It is good to present the output to the readers. What's Next? \u00b6 Submit a PR with your tutorial. We'll review it, and soon you'll see your tutorial in the MinsdDB docs!","title":"Writing Tutorials"},{"location":"contribute/tutorials/#how-to-write-a-tutorial","text":"This section presents how to write a tutorial with MindsDB. Content of the Tutorial You are free to create your content. However, you should include in your tutorial all the chapters listed here.","title":"How to Write a Tutorial"},{"location":"contribute/tutorials/#introduction","text":"Here you introduce the readers to the tutorial. You can describe what dataset you use and what you predict.","title":"Introduction"},{"location":"contribute/tutorials/#data-setup","text":"This chapter contains an introduction to the dataset you use.","title":"Data Setup"},{"location":"contribute/tutorials/#connecting-the-data","text":"Let others follow your tutorial by providing information on where to get the data from and how to connect it to MindsDB.","title":"Connecting the Data"},{"location":"contribute/tutorials/#understanding-the-data","text":"You can briefly introduce the dataset you use.","title":"Understanding the Data"},{"location":"contribute/tutorials/#training-a-predictor","text":"Here you use the CREATE MODEL command to create a predictor.","title":"Training a Predictor"},{"location":"contribute/tutorials/#status-of-a-predictor","text":"The next step is to check the status of a predictor. If its value is complete , you can proceed to the next chapter.","title":"Status of a Predictor"},{"location":"contribute/tutorials/#making-predictions","text":"Use the SELECT statement to query for prediction results. It is good to present the output to the readers.","title":"Making Predictions"},{"location":"contribute/tutorials/#making-a-single-prediction","text":"In the case of regression and classification predictors, you can make a single prediction. It is good to present the output to the readers.","title":"Making a Single Prediction"},{"location":"contribute/tutorials/#making-batch-predictions","text":"You can make batch predictions using the JOIN clause for all predictor types, such as regression, classification, and time series. It is good to present the output to the readers.","title":"Making Batch Predictions"},{"location":"contribute/tutorials/#whats-next","text":"Submit a PR with your tutorial. We'll review it, and soon you'll see your tutorial in the MinsdDB docs!","title":"What's Next?"},{"location":"contribute/loanTutorial/loanTutorial/","text":"Loan Default Prediction \u00b6 Introduction \u00b6 A loan is money borroed to somone (debtor) with the intent to pay back at an agreed date. Ideally, things should go as planned but when the debtor fails to pay the person they borrowed the loan from (creditor), the debtor is said to have defaulted on the loan. This is what loan default is It is the event of a debtor not paying their loan... It is then important for creditors/loan companies to know/predict if a certain debtor will default or not. This is a problem that machine learning solves, this is a classification machine learning problem. As with every machine learning problem, data is the major ingredient to solving it. The dataset for this problem is from the Zindi Loan Default Prediction Challenge for DSN This tutorial is to showcase how MindsDB performs in solveing this problem. We will be exploring how we can use a machine learning model to classify negative and positive cases for predicitng loan default. MindsDB allows you to train your model from a .CSV format directly, and you will: Load your dataset Create a predictor, and, Make predictions Load your dataset \u00b6 Data wrangling \u00b6 if you go through the dataset, you will find that it is split into three different files, you will first wrangle the data to suit your requirements, Here, i use python to wrangle the data, you can however use your desired tool (excel perhaps...) or copy my snippet below... # code snipet to join data # iomport dependencies and read in data files import pandas as pd import functools as ft trainPerf = pd.read_csv('trainperf.csv') trainDemographics = pd.read_csv('traindemographics.csv') # join data together, clean and write to .csv data = trainPerf.merge(trainDemographics, left_on='customerid', right_on='customerid') data.drop_duplicates(inplace=True) data.dropna(axis=1, inplace=True) data.drop(['customerid', 'systemloanid'], axis=1, inplace=True) # remove id columns data.to_csv('loanData.csv') Now that the dataset has been cleaned and stored into a .csv file, you can upload it to MindsDB cloud... Using the MindsDB GUI \u00b6 To use the MindsDB GUI, you will need to access the MindsDB cloud tool and sign uo for an account if you haven't yet, and you will be automatically directed to the editor. While on the editor, you can add your data via several means, but as earlier mentioned, you will upload your dataset as a .csv... On the \"Select your data source\" screen, click on \"Files\" next to \"Databaes\" and you should see the screen below: Click on the \"Import File\" icon and select your .csv file. Give your table a name and hit the \"Save and Con\"tinue\" button. NOTE: Use snake or camel casing when uploading files If you do all things well, you should see a result like this: Now that you have loaded your dataset, you are now to... Create a Predictor \u00b6 Creating a predictor in MindsDB is as easy as four lines of code. Copy and paste the code below into your SQL editor to make a predictor CREATE MODEL mindsdb.loanPredictor FROM files ( SELECT * FROM files.loanData ) PREDICT good_bad_flag; If all is successfull, you should see this screen below: To confirm this and view details of your model like the accuracy, use the following code: SELECT * FROM mindsdb.models WHERE name='loanPredictor'; and you can see details of your model... PS: You can see the accuracy is low, this is probably due to low data... Now that you have created your prdictor model, now is time to... Make a prediction \u00b6 You can use the folowing query to make a prediction... pay attention to use new/unseen/untrained data... PS: Pay attention to highlight your prediction query and run it SELECT good_bad_flag FROM mindsdb.loanPredictor WHERE loannumber=1 AND approveddate='2022-07-04 10:12:66.000000' AND creationdate='2022-07-04 08:10:00.000000' AND loanamount=100000 AND totaldue=110000 AND termdays=30 AND birthdate='1986-11-04 00:00:00.000000' AND bank_account_type='Savings' AND longitude_gps=3.3269784 AND latitude_gps=5.799699 AND bank_name_clients='UBA'; and viola! you just made a prediction! Conclusion \u00b6 And that's it you shining rockstar! you are now an ML Wizard making predictions straight from the data! This is a taste of the awesome power of MindsDB. Some important things to note: More data improves the accuracy of the model Your table names should not contain spaces but should rather be in snake or camel casing Highlight your prediction query and run it to see result MinsDB is fast and very helpfull If you like this tutorial, kindly share and follow us for more.","title":"Loan Default Prediction"},{"location":"contribute/loanTutorial/loanTutorial/#loan-default-prediction","text":"","title":"Loan Default Prediction"},{"location":"contribute/loanTutorial/loanTutorial/#introduction","text":"A loan is money borroed to somone (debtor) with the intent to pay back at an agreed date. Ideally, things should go as planned but when the debtor fails to pay the person they borrowed the loan from (creditor), the debtor is said to have defaulted on the loan. This is what loan default is It is the event of a debtor not paying their loan... It is then important for creditors/loan companies to know/predict if a certain debtor will default or not. This is a problem that machine learning solves, this is a classification machine learning problem. As with every machine learning problem, data is the major ingredient to solving it. The dataset for this problem is from the Zindi Loan Default Prediction Challenge for DSN This tutorial is to showcase how MindsDB performs in solveing this problem. We will be exploring how we can use a machine learning model to classify negative and positive cases for predicitng loan default. MindsDB allows you to train your model from a .CSV format directly, and you will: Load your dataset Create a predictor, and, Make predictions","title":"Introduction"},{"location":"contribute/loanTutorial/loanTutorial/#load-your-dataset","text":"","title":"Load your dataset"},{"location":"contribute/loanTutorial/loanTutorial/#data-wrangling","text":"if you go through the dataset, you will find that it is split into three different files, you will first wrangle the data to suit your requirements, Here, i use python to wrangle the data, you can however use your desired tool (excel perhaps...) or copy my snippet below... # code snipet to join data # iomport dependencies and read in data files import pandas as pd import functools as ft trainPerf = pd.read_csv('trainperf.csv') trainDemographics = pd.read_csv('traindemographics.csv') # join data together, clean and write to .csv data = trainPerf.merge(trainDemographics, left_on='customerid', right_on='customerid') data.drop_duplicates(inplace=True) data.dropna(axis=1, inplace=True) data.drop(['customerid', 'systemloanid'], axis=1, inplace=True) # remove id columns data.to_csv('loanData.csv') Now that the dataset has been cleaned and stored into a .csv file, you can upload it to MindsDB cloud...","title":"Data wrangling"},{"location":"contribute/loanTutorial/loanTutorial/#using-the-mindsdb-gui","text":"To use the MindsDB GUI, you will need to access the MindsDB cloud tool and sign uo for an account if you haven't yet, and you will be automatically directed to the editor. While on the editor, you can add your data via several means, but as earlier mentioned, you will upload your dataset as a .csv... On the \"Select your data source\" screen, click on \"Files\" next to \"Databaes\" and you should see the screen below: Click on the \"Import File\" icon and select your .csv file. Give your table a name and hit the \"Save and Con\"tinue\" button. NOTE: Use snake or camel casing when uploading files If you do all things well, you should see a result like this: Now that you have loaded your dataset, you are now to...","title":"Using the MindsDB GUI"},{"location":"contribute/loanTutorial/loanTutorial/#create-a-predictor","text":"Creating a predictor in MindsDB is as easy as four lines of code. Copy and paste the code below into your SQL editor to make a predictor CREATE MODEL mindsdb.loanPredictor FROM files ( SELECT * FROM files.loanData ) PREDICT good_bad_flag; If all is successfull, you should see this screen below: To confirm this and view details of your model like the accuracy, use the following code: SELECT * FROM mindsdb.models WHERE name='loanPredictor'; and you can see details of your model... PS: You can see the accuracy is low, this is probably due to low data... Now that you have created your prdictor model, now is time to...","title":"Create a Predictor"},{"location":"contribute/loanTutorial/loanTutorial/#make-a-prediction","text":"You can use the folowing query to make a prediction... pay attention to use new/unseen/untrained data... PS: Pay attention to highlight your prediction query and run it SELECT good_bad_flag FROM mindsdb.loanPredictor WHERE loannumber=1 AND approveddate='2022-07-04 10:12:66.000000' AND creationdate='2022-07-04 08:10:00.000000' AND loanamount=100000 AND totaldue=110000 AND termdays=30 AND birthdate='1986-11-04 00:00:00.000000' AND bank_account_type='Savings' AND longitude_gps=3.3269784 AND latitude_gps=5.799699 AND bank_name_clients='UBA'; and viola! you just made a prediction!","title":"Make a prediction"},{"location":"contribute/loanTutorial/loanTutorial/#conclusion","text":"And that's it you shining rockstar! you are now an ML Wizard making predictions straight from the data! This is a taste of the awesome power of MindsDB. Some important things to note: More data improves the accuracy of the model Your table names should not contain spaces but should rather be in snake or camel casing Highlight your prediction query and run it to see result MinsDB is fast and very helpfull If you like this tutorial, kindly share and follow us for more.","title":"Conclusion"},{"location":"custom-model/huggingface/","text":"MindsDB and HuggingFace \u00b6 HuggingFace facilitates building, training, and deploying ML models. How to Bring the HuggingFace Model to MindsDB \u00b6 We use the CREATE PREDICTOR statement to bring the HuggingFace models to MindsDB. Let's go through some sample models. Model 1: Spam Classifier \u00b6 Here is an example of a binary classification. The model determines whether a text string is a spam or not. CREATE MODEL mindsdb . spam_classifier PREDICT PRED USING engine = 'huggingface' , task = 'text-classification' , model_name = 'mrm8488/bert-tiny-finetuned-sms-spam-detection' , input_column = 'text_spammy' , labels = [ 'ham' , 'spam' ]; On execution, we get: Query successfully completed Before querying for predictions, we should verify the status of the spam_classifier model. SELECT * FROM mindsdb . models WHERE name = 'spam_classifier' ; On execution, we get: + ---------------+-------+--------+--------+-------+-------------+---------------+------+-----------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ | NAME | PROJECT | STATUS | ACCURACY | PREDICT | UPDATE_STATUS | MINDSDB_VERSION | ERROR | SELECT_DATA_QUERY | TRAINING_OPTIONS | + ---------------+-------+--------+--------+-------+-------------+---------------+------+-----------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ | spam_classifier | mindsdb | complete | [ NULL ] | PRED | up_to_date | 22 . 10 . 2 . 1 | [ NULL ] | [ NULL ] | { 'target' : 'PRED' , 'using' : { 'engine' : 'huggingface' , 'task' : 'text-classification' , 'model_name' : 'mrm8488/bert-tiny-finetuned-sms-spam-detection' , 'input_column' : 'text_spammy' , 'labels' : [ 'ham' , 'spam' ] }} | + ---------------+-------+--------+--------+-------+-------------+---------------+------+-----------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ Once the status is complete , we can query for predictions. SELECT h . * , t . text_spammy AS input_text FROM example_db . demo_data . hf_test AS t JOIN mindsdb . spam_classifier AS h ; On execution, we get: + ----+---------------------------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------+ | PRED | PRED_explain | input_text | + ----+---------------------------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------+ | spam | { 'spam' : 0 . 9051626920700073 , 'ham' : 0 . 09483727067708969 } | Free entry in 2 a wkly comp to win FA Cup final tkts 21 st May 2005 . Text FA to 87121 to receive entry question ( std txt rate ) T & C 's apply 08452810075over18' s | | ham | { 'ham' : 0 . 9380123615264893 , 'spam' : 0 . 061987683176994324 } | Nah I don 't think he goes to usf, he lives around here though | |spam|{' spam ': 0.9064534902572632, ' ham ' : 0 . 09354648739099503 } | WINNER !! As a valued network customer you have been selected to receivea \u00a3 900 prize reward ! To claim call 09061701461 . Claim code KL341 . Valid 12 hours only . | + ----+---------------------------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------+ Model 2: Sentiment Classifier \u00b6 Here is an example of a multi-value classification. The model determines a sentiment of a text string, where possible values are negative ( neg ), neutral ( neu ), and positive ( pos ). CREATE MODEL mindsdb . sentiment_classifier PREDICT sentiment USING engine = 'huggingface' , task = 'text-classification' , model_name = 'cardiffnlp/twitter-roberta-base-sentiment' , input_column = 'text_short' , labels = [ 'neg' , 'neu' , 'pos' ]; On execution, we get: Query successfully completed Before querying for predictions, we should verify the status of the sentiment_classifier model. SELECT * FROM mindsdb . models WHERE name = 'sentiment_classifier' ; On execution, we get: + --------------------+-------+--------+--------+---------+-------------+---------------+------+-----------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ | NAME | PROJECT | STATUS | ACCURACY | PREDICT | UPDATE_STATUS | MINDSDB_VERSION | ERROR | SELECT_DATA_QUERY | TRAINING_OPTIONS | + --------------------+-------+--------+--------+---------+-------------+---------------+------+-----------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ | sentiment_classifier | mindsdb | complete | [ NULL ] | sentiment | up_to_date | 22 . 10 . 2 . 1 | [ NULL ] | [ NULL ] | { 'target' : 'sentiment' , 'using' : { 'engine' : 'huggingface' , 'task' : 'text-classification' , 'model_name' : 'cardiffnlp/twitter-roberta-base-sentiment' , 'input_column' : 'text_short' , 'labels' : [ 'neg' , 'neu' , 'pos' ] }} | + --------------------+-------+--------+--------+---------+-------------+---------------+------+-----------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ Once the status is complete , we can query for predictions. SELECT h . * , t . text_short AS input_text FROM example_db . demo_data . hf_test AS t JOIN mindsdb . sentiment_classifier AS h ; On execution, we get: + ---------+--------------------------------------------------------------------------------------+-------------------+ | sentiment | sentiment_explain | input_text | + ---------+--------------------------------------------------------------------------------------+-------------------+ | neg | { 'neg' : 0 . 9732725620269775 , 'neu' : 0 . 022419845685362816 , 'pos' : 0 . 004307641182094812 } | I hate Australia | | pos | { 'pos' : 0 . 7607280015945435 , 'neu' : 0 . 2332666665315628 , 'neg' : 0 . 006005281116813421 } | I want to dance | | pos | { 'pos' : 0 . 9835041761398315 , 'neu' : 0 . 014900505542755127 , 'neg' : 0 . 0015953202964738011 } | Baking is the best | + ---------+--------------------------------------------------------------------------------------+-------------------+ Model 3: Zero-Shot Classifier \u00b6 Here is an example of a zero-shot classification. The model determines to which of the defined categories a text string belongs. CREATE MODEL mindsdb . zero_shot_tcd PREDICT topic USING engine = 'huggingface' , task = 'zero-shot-classification' , model_name = 'facebook/bart-large-mnli' , input_column = 'text_short' , candidate_labels = [ 'travel' , 'cooking' , 'dancing' ]; On execution, we get: Query successfully completed Before querying for predictions, we should verify the status of the zero_shot_tcd model. SELECT * FROM mindsdb . models WHERE name = 'zero_shot_tcd' ; On execution, we get: + -------------+-------+--------+--------+--------+-------------+---------------+------+-----------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ | NAME | PROJECT | STATUS | ACCURACY | PREDICT | UPDATE_STATUS | MINDSDB_VERSION | ERROR | SELECT_DATA_QUERY | TRAINING_OPTIONS | + -------------+-------+--------+--------+--------+-------------+---------------+------+-----------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ | zero_shot_tcd | mindsdb | complete | [ NULL ] | topic | up_to_date | 22 . 10 . 2 . 1 | [ NULL ] | [ NULL ] | { 'target' : 'topic' , 'using' : { 'engine' : 'huggingface' , 'task' : 'zero-shot-classification' , 'model_name' : 'facebook/bart-large-mnli' , 'input_column' : 'text_short' , 'candidate_labels' : [ 'travel' , 'cooking' , 'dancing' ] }} | + -------------+-------+--------+--------+--------+-------------+---------------+------+-----------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ Once the status is complete , we can query for predictions. SELECT h . * , t . text_short AS input_text FROM example_db . demo_data . hf_test AS t JOIN mindsdb . zero_shot_tcd AS h ; On execution, we get: + -------+--------------------------------------------------------------------------------------------------+-------------------+ | topic | topic_explain | input_text | + -------+--------------------------------------------------------------------------------------------------+-------------------+ | travel | { 'travel' : 0 . 8343215584754944 , 'dancing' : 0 . 08680055290460587 , 'cooking' : 0 . 07887797057628632 } | I hate Australia | | dancing | { 'dancing' : 0 . 9746809601783752 , 'travel' : 0 . 015539299696683884 , 'cooking' : 0 . 009779711253941059 } | I want to dance | | cooking | { 'cooking' : 0 . 9936348795890808 , 'travel' : 0 . 0034196735359728336 , 'dancing' : 0 . 0029454431496560574 } | Baking is the best | + -------+--------------------------------------------------------------------------------------------------+-------------------+ Model 4: Translation \u00b6 Here is an example of a translation. The model gets an input string in English and translates it into French. CREATE MODEL mindsdb . translator_en_fr PREDICT translated USING engine = 'huggingface' , task = 'translation' , model_name = 't5-base' , input_column = 'text_short' , lang_input = 'en' , lang_output = 'fr' ; On execution, we get: Query successfully completed Before querying for predictions, we should verify the status of the translator_en_fr model. SELECT * FROM mindsdb . models WHERE name = 'translator_en_fr' ; On execution, we get: + ----------------+-------+--------+--------+----------+-------------+---------------+------+-----------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ | NAME | PROJECT | STATUS | ACCURACY | PREDICT | UPDATE_STATUS | MINDSDB_VERSION | ERROR | SELECT_DATA_QUERY | TRAINING_OPTIONS | + ----------------+-------+--------+--------+----------+-------------+---------------+------+-----------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ | translator_en_fr | mindsdb | complete | [ NULL ] | translated | up_to_date | 22 . 10 . 2 . 1 | [ NULL ] | [ NULL ] | { 'target' : 'translated' , 'using' : { 'engine' : 'huggingface' , 'task' : 'translation' , 'model_name' : 't5-base' , 'input_column' : 'text_short' , 'lang_input' : 'en' , 'lang_output' : 'fr' }} | + ----------------+-------+--------+--------+----------+-------------+---------------+------+-----------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ Once the status is complete , we can query for predictions. SELECT h . * , t . text_short AS input_text FROM example_db . demo_data . hf_test AS t JOIN mindsdb . translator_en_fr AS h ; On execution, we get: + -------------------------------+-------------------+ | translated | input_text | + -------------------------------+-------------------+ | Je d\u00e9teste l ' Australie | I hate Australia | | Je veux danser | I want to dance | | La boulangerie est la meilleure | Baking is the best | + -------------------------------+-------------------+ Model 5: Summarisation \u00b6 Here is an example of a summarisation. CREATE MODEL mindsdb . summarizer_10_20 PREDICT text_summary USING engine = 'huggingface' , task = 'summarization' , model_name = 'sshleifer/distilbart-cnn-12-6' , input_column = 'text_long' , min_output_length = 10 , max_output_length = 20 ; On execution, we get: Query successfully completed Before querying for predictions, we should verify the status of the summarizer_10_20 model. SELECT * FROM mindsdb . models WHERE name = 'summarizer_10_20' ; On execution, we get: + ----------------+-------+--------+--------+------------+-------------+---------------+------+-----------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ | NAME | PROJECT | STATUS | ACCURACY | PREDICT | UPDATE_STATUS | MINDSDB_VERSION | ERROR | SELECT_DATA_QUERY | TRAINING_OPTIONS | + ----------------+-------+--------+--------+------------+-------------+---------------+------+-----------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ | summarizer_10_20 | mindsdb | complete | [ NULL ] | text_summary | up_to_date | 22 . 10 . 2 . 1 | [ NULL ] | [ NULL ] | { 'target' : 'text_summary' , 'using' : { 'engine' : 'huggingface' , 'task' : 'summarization' , 'model_name' : 'sshleifer/distilbart-cnn-12-6' , 'input_column' : 'text_long' , 'min_output_length' : 10 , 'max_output_length' : 20 }} | + ----------------+-------+--------+--------+------------+-------------+---------------+------+-----------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ Once the status is complete , we can query for predictions. SELECT h . * , t . text_long AS input_text FROM example_db . demo_data . hf_test AS t JOIN mindsdb . summarizer_10_20 AS h ; On execution, we get: + --------------------------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ | text_summary | input_text | + --------------------------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ | Australia is a sovereign country comprising the mainland of the Australian continent , the island of Tasmania | Australia , officially the Commonwealth of Australia , is a sovereign country comprising the mainland of the Australian continent , the island of Tasmania , and numerous smaller islands . | | Dance is a performing art form consisting of sequences of movement , either improvised or purposefully selected | Dance is a performing art form consisting of sequences of movement , either improvised or purposefully selected . This movement has aesthetic and often symbolic value .[ nb 1 ] Dance can be categorized and described by its choreography , by its repertoire of movements , or by its historical period or place of origin . | | Baking is a method of preparing food that uses dry heat , typically in an oven | Baking is a method of preparing food that uses dry heat , typically in an oven , but can also be done in hot ashes , or on hot stones . The most common baked item is bread but many other types of foods can be baked . Heat is gradually transferred from the surface of cakes , cookies , and pieces of bread to their center . As heat travels through , it transforms batters and doughs into baked goods and more with a firm dry crust and a softer center . Baking can be combined with grilling to produce a hybrid barbecue variant by using both methods simultaneously , or one after the other . Baking is related to barbecuing because the concept of the masonry oven is similar to that of a smoke pit . | + --------------------------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+","title":"HuggingFace"},{"location":"custom-model/huggingface/#mindsdb-and-huggingface","text":"HuggingFace facilitates building, training, and deploying ML models.","title":"MindsDB and HuggingFace"},{"location":"custom-model/huggingface/#how-to-bring-the-huggingface-model-to-mindsdb","text":"We use the CREATE PREDICTOR statement to bring the HuggingFace models to MindsDB. Let's go through some sample models.","title":"How to Bring the HuggingFace Model to MindsDB"},{"location":"custom-model/huggingface/#model-1-spam-classifier","text":"Here is an example of a binary classification. The model determines whether a text string is a spam or not. CREATE MODEL mindsdb . spam_classifier PREDICT PRED USING engine = 'huggingface' , task = 'text-classification' , model_name = 'mrm8488/bert-tiny-finetuned-sms-spam-detection' , input_column = 'text_spammy' , labels = [ 'ham' , 'spam' ]; On execution, we get: Query successfully completed Before querying for predictions, we should verify the status of the spam_classifier model. SELECT * FROM mindsdb . models WHERE name = 'spam_classifier' ; On execution, we get: + ---------------+-------+--------+--------+-------+-------------+---------------+------+-----------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ | NAME | PROJECT | STATUS | ACCURACY | PREDICT | UPDATE_STATUS | MINDSDB_VERSION | ERROR | SELECT_DATA_QUERY | TRAINING_OPTIONS | + ---------------+-------+--------+--------+-------+-------------+---------------+------+-----------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ | spam_classifier | mindsdb | complete | [ NULL ] | PRED | up_to_date | 22 . 10 . 2 . 1 | [ NULL ] | [ NULL ] | { 'target' : 'PRED' , 'using' : { 'engine' : 'huggingface' , 'task' : 'text-classification' , 'model_name' : 'mrm8488/bert-tiny-finetuned-sms-spam-detection' , 'input_column' : 'text_spammy' , 'labels' : [ 'ham' , 'spam' ] }} | + ---------------+-------+--------+--------+-------+-------------+---------------+------+-----------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ Once the status is complete , we can query for predictions. SELECT h . * , t . text_spammy AS input_text FROM example_db . demo_data . hf_test AS t JOIN mindsdb . spam_classifier AS h ; On execution, we get: + ----+---------------------------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------+ | PRED | PRED_explain | input_text | + ----+---------------------------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------+ | spam | { 'spam' : 0 . 9051626920700073 , 'ham' : 0 . 09483727067708969 } | Free entry in 2 a wkly comp to win FA Cup final tkts 21 st May 2005 . Text FA to 87121 to receive entry question ( std txt rate ) T & C 's apply 08452810075over18' s | | ham | { 'ham' : 0 . 9380123615264893 , 'spam' : 0 . 061987683176994324 } | Nah I don 't think he goes to usf, he lives around here though | |spam|{' spam ': 0.9064534902572632, ' ham ' : 0 . 09354648739099503 } | WINNER !! As a valued network customer you have been selected to receivea \u00a3 900 prize reward ! To claim call 09061701461 . Claim code KL341 . Valid 12 hours only . | + ----+---------------------------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------+","title":"Model 1: Spam Classifier"},{"location":"custom-model/huggingface/#model-2-sentiment-classifier","text":"Here is an example of a multi-value classification. The model determines a sentiment of a text string, where possible values are negative ( neg ), neutral ( neu ), and positive ( pos ). CREATE MODEL mindsdb . sentiment_classifier PREDICT sentiment USING engine = 'huggingface' , task = 'text-classification' , model_name = 'cardiffnlp/twitter-roberta-base-sentiment' , input_column = 'text_short' , labels = [ 'neg' , 'neu' , 'pos' ]; On execution, we get: Query successfully completed Before querying for predictions, we should verify the status of the sentiment_classifier model. SELECT * FROM mindsdb . models WHERE name = 'sentiment_classifier' ; On execution, we get: + --------------------+-------+--------+--------+---------+-------------+---------------+------+-----------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ | NAME | PROJECT | STATUS | ACCURACY | PREDICT | UPDATE_STATUS | MINDSDB_VERSION | ERROR | SELECT_DATA_QUERY | TRAINING_OPTIONS | + --------------------+-------+--------+--------+---------+-------------+---------------+------+-----------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ | sentiment_classifier | mindsdb | complete | [ NULL ] | sentiment | up_to_date | 22 . 10 . 2 . 1 | [ NULL ] | [ NULL ] | { 'target' : 'sentiment' , 'using' : { 'engine' : 'huggingface' , 'task' : 'text-classification' , 'model_name' : 'cardiffnlp/twitter-roberta-base-sentiment' , 'input_column' : 'text_short' , 'labels' : [ 'neg' , 'neu' , 'pos' ] }} | + --------------------+-------+--------+--------+---------+-------------+---------------+------+-----------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ Once the status is complete , we can query for predictions. SELECT h . * , t . text_short AS input_text FROM example_db . demo_data . hf_test AS t JOIN mindsdb . sentiment_classifier AS h ; On execution, we get: + ---------+--------------------------------------------------------------------------------------+-------------------+ | sentiment | sentiment_explain | input_text | + ---------+--------------------------------------------------------------------------------------+-------------------+ | neg | { 'neg' : 0 . 9732725620269775 , 'neu' : 0 . 022419845685362816 , 'pos' : 0 . 004307641182094812 } | I hate Australia | | pos | { 'pos' : 0 . 7607280015945435 , 'neu' : 0 . 2332666665315628 , 'neg' : 0 . 006005281116813421 } | I want to dance | | pos | { 'pos' : 0 . 9835041761398315 , 'neu' : 0 . 014900505542755127 , 'neg' : 0 . 0015953202964738011 } | Baking is the best | + ---------+--------------------------------------------------------------------------------------+-------------------+","title":"Model 2: Sentiment Classifier"},{"location":"custom-model/huggingface/#model-3-zero-shot-classifier","text":"Here is an example of a zero-shot classification. The model determines to which of the defined categories a text string belongs. CREATE MODEL mindsdb . zero_shot_tcd PREDICT topic USING engine = 'huggingface' , task = 'zero-shot-classification' , model_name = 'facebook/bart-large-mnli' , input_column = 'text_short' , candidate_labels = [ 'travel' , 'cooking' , 'dancing' ]; On execution, we get: Query successfully completed Before querying for predictions, we should verify the status of the zero_shot_tcd model. SELECT * FROM mindsdb . models WHERE name = 'zero_shot_tcd' ; On execution, we get: + -------------+-------+--------+--------+--------+-------------+---------------+------+-----------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ | NAME | PROJECT | STATUS | ACCURACY | PREDICT | UPDATE_STATUS | MINDSDB_VERSION | ERROR | SELECT_DATA_QUERY | TRAINING_OPTIONS | + -------------+-------+--------+--------+--------+-------------+---------------+------+-----------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ | zero_shot_tcd | mindsdb | complete | [ NULL ] | topic | up_to_date | 22 . 10 . 2 . 1 | [ NULL ] | [ NULL ] | { 'target' : 'topic' , 'using' : { 'engine' : 'huggingface' , 'task' : 'zero-shot-classification' , 'model_name' : 'facebook/bart-large-mnli' , 'input_column' : 'text_short' , 'candidate_labels' : [ 'travel' , 'cooking' , 'dancing' ] }} | + -------------+-------+--------+--------+--------+-------------+---------------+------+-----------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ Once the status is complete , we can query for predictions. SELECT h . * , t . text_short AS input_text FROM example_db . demo_data . hf_test AS t JOIN mindsdb . zero_shot_tcd AS h ; On execution, we get: + -------+--------------------------------------------------------------------------------------------------+-------------------+ | topic | topic_explain | input_text | + -------+--------------------------------------------------------------------------------------------------+-------------------+ | travel | { 'travel' : 0 . 8343215584754944 , 'dancing' : 0 . 08680055290460587 , 'cooking' : 0 . 07887797057628632 } | I hate Australia | | dancing | { 'dancing' : 0 . 9746809601783752 , 'travel' : 0 . 015539299696683884 , 'cooking' : 0 . 009779711253941059 } | I want to dance | | cooking | { 'cooking' : 0 . 9936348795890808 , 'travel' : 0 . 0034196735359728336 , 'dancing' : 0 . 0029454431496560574 } | Baking is the best | + -------+--------------------------------------------------------------------------------------------------+-------------------+","title":"Model 3: Zero-Shot Classifier"},{"location":"custom-model/huggingface/#model-4-translation","text":"Here is an example of a translation. The model gets an input string in English and translates it into French. CREATE MODEL mindsdb . translator_en_fr PREDICT translated USING engine = 'huggingface' , task = 'translation' , model_name = 't5-base' , input_column = 'text_short' , lang_input = 'en' , lang_output = 'fr' ; On execution, we get: Query successfully completed Before querying for predictions, we should verify the status of the translator_en_fr model. SELECT * FROM mindsdb . models WHERE name = 'translator_en_fr' ; On execution, we get: + ----------------+-------+--------+--------+----------+-------------+---------------+------+-----------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ | NAME | PROJECT | STATUS | ACCURACY | PREDICT | UPDATE_STATUS | MINDSDB_VERSION | ERROR | SELECT_DATA_QUERY | TRAINING_OPTIONS | + ----------------+-------+--------+--------+----------+-------------+---------------+------+-----------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ | translator_en_fr | mindsdb | complete | [ NULL ] | translated | up_to_date | 22 . 10 . 2 . 1 | [ NULL ] | [ NULL ] | { 'target' : 'translated' , 'using' : { 'engine' : 'huggingface' , 'task' : 'translation' , 'model_name' : 't5-base' , 'input_column' : 'text_short' , 'lang_input' : 'en' , 'lang_output' : 'fr' }} | + ----------------+-------+--------+--------+----------+-------------+---------------+------+-----------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ Once the status is complete , we can query for predictions. SELECT h . * , t . text_short AS input_text FROM example_db . demo_data . hf_test AS t JOIN mindsdb . translator_en_fr AS h ; On execution, we get: + -------------------------------+-------------------+ | translated | input_text | + -------------------------------+-------------------+ | Je d\u00e9teste l ' Australie | I hate Australia | | Je veux danser | I want to dance | | La boulangerie est la meilleure | Baking is the best | + -------------------------------+-------------------+","title":"Model 4: Translation"},{"location":"custom-model/huggingface/#model-5-summarisation","text":"Here is an example of a summarisation. CREATE MODEL mindsdb . summarizer_10_20 PREDICT text_summary USING engine = 'huggingface' , task = 'summarization' , model_name = 'sshleifer/distilbart-cnn-12-6' , input_column = 'text_long' , min_output_length = 10 , max_output_length = 20 ; On execution, we get: Query successfully completed Before querying for predictions, we should verify the status of the summarizer_10_20 model. SELECT * FROM mindsdb . models WHERE name = 'summarizer_10_20' ; On execution, we get: + ----------------+-------+--------+--------+------------+-------------+---------------+------+-----------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ | NAME | PROJECT | STATUS | ACCURACY | PREDICT | UPDATE_STATUS | MINDSDB_VERSION | ERROR | SELECT_DATA_QUERY | TRAINING_OPTIONS | + ----------------+-------+--------+--------+------------+-------------+---------------+------+-----------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ | summarizer_10_20 | mindsdb | complete | [ NULL ] | text_summary | up_to_date | 22 . 10 . 2 . 1 | [ NULL ] | [ NULL ] | { 'target' : 'text_summary' , 'using' : { 'engine' : 'huggingface' , 'task' : 'summarization' , 'model_name' : 'sshleifer/distilbart-cnn-12-6' , 'input_column' : 'text_long' , 'min_output_length' : 10 , 'max_output_length' : 20 }} | + ----------------+-------+--------+--------+------------+-------------+---------------+------+-----------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ Once the status is complete , we can query for predictions. SELECT h . * , t . text_long AS input_text FROM example_db . demo_data . hf_test AS t JOIN mindsdb . summarizer_10_20 AS h ; On execution, we get: + --------------------------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ | text_summary | input_text | + --------------------------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ | Australia is a sovereign country comprising the mainland of the Australian continent , the island of Tasmania | Australia , officially the Commonwealth of Australia , is a sovereign country comprising the mainland of the Australian continent , the island of Tasmania , and numerous smaller islands . | | Dance is a performing art form consisting of sequences of movement , either improvised or purposefully selected | Dance is a performing art form consisting of sequences of movement , either improvised or purposefully selected . This movement has aesthetic and often symbolic value .[ nb 1 ] Dance can be categorized and described by its choreography , by its repertoire of movements , or by its historical period or place of origin . | | Baking is a method of preparing food that uses dry heat , typically in an oven | Baking is a method of preparing food that uses dry heat , typically in an oven , but can also be done in hot ashes , or on hot stones . The most common baked item is bread but many other types of foods can be baked . Heat is gradually transferred from the surface of cakes , cookies , and pieces of bread to their center . As heat travels through , it transforms batters and doughs into baked goods and more with a firm dry crust and a softer center . Baking can be combined with grilling to produce a hybrid barbecue variant by using both methods simultaneously , or one after the other . Baking is related to barbecuing because the concept of the masonry oven is similar to that of a smoke pit . | + --------------------------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+","title":"Model 5: Summarisation"},{"location":"custom-model/mlflow/","text":"MindsDB and MLflow \u00b6 MLflow allows you to create, train, and serve machine learning models, apart from other features, such as organizing experiments, tracking metrics, and more. Here, we present two usage examples of the MLflow. Simple Example of Logistic Regression \u00b6 Currently, there is no way to train an MLflow-wrapped model using the API. The training of the model takes place outside of MindsDB. The data must be pulled manually, for example, with a script. It is a good idea to use an MLflow run or experiment. Creating the MLflow Model \u00b6 We start by writing a script that creates and trains the model. After that, the script is saved using one of the saving methods offered by MLflow. Here, we use the model from this simple tutorial and the mlflow.sklearn.log_model method . Please note that the model must be a scikit-learn model. Once your model is trained, ensure that it is served and listens for input at a URL of your choice. Please note that your model may run on a different machine than the one where MindsDB runs. Here, we assume the URL to be http://localhost:5000/invocations , as in the tutorial. Let's run the train.py script that provides us with the <run-id> value for the model. $ python examples/sklearn_logistic_regression/train.py Score: 0 .666 Model saved in run <run-id> Now, let's execute the following command from the directory where the model resides, providing the <run-id> value. $ mlflow models serve --model-uri runs:/<run-id>/model We're ready to move to MindsDB. Bringing the MLflow Model to MindsDB \u00b6 We execute the command below to create a predictor in MindsDB based on the created model. CREATE MODEL mindsdb . byom_mlflow PREDICT ` 1 ` -- `1` is the target column name USING url . predict = 'http://localhost:5000/invocations' , format = 'mlflow' , data_dtype = { \"0\" : \"integer\" , \"1\" : \"integer\" } ; Now, you can fetch predictions using the standard MindsDB syntax. Follow the guide on the SELECT statement to learn more. SELECT ` 1 ` FROM byom_mlflow WHERE ` 0 `= 2 ; Advanced Example of Keras NLP Model \u00b6 Before we start, download the natural language processing (NLP) dataset from Kaggle to reproduce the steps of this example. Here, we look at the best practices when your model needs custom data preprocessing, which is quite common. We use the mlflow.pyfunc module to complete the following: Save the model using mlflow.pyfunc.save_model . Subclass mlflow.pyfunc.PythonModel to wrap the model in a way compatible with MLflow that enables our custom inference logic to be called. Creating the MLflow Model \u00b6 In the script that trains the model, like this one or that one , there should be a call to the mlflow.pyfunc.save_model function at the end. It is to save every produced artifact. mlflow . pyfunc . save_model ( path = \"nlp_kaggle\" , python_model = Model (), conda_env = conda_env , artifacts = artifacts ) Here, artifacts is a dictionary storing all the expected output after running the training phase. In this case, we want both a model and a tokenizer to preprocess the input text. And conda_env specifies the environment in which your model is executed when it is served in a self-contained conda environment. It should include all the required packages and dependencies, like below: # these are accessible inside the Model() wrapper artifacts = { 'model' : model_path , 'tokenizer_path' : tokenizer_path , } # specs for an environment that is created when serving the model conda_env = { 'name' : 'nlp_keras_env' , 'channels' : [ 'defaults' ], 'dependencies' : [ 'python=3.8' , 'pip' , { 'pip' : [ 'mlflow' , 'tensorflow' , 'cloudpickle' , 'nltk' , 'pandas' , 'numpy' , 'scikit-learn' , 'tqdm' , ], }, ], } Finally, to store the model, you need to provide the wrapper class that loads all the produced artifacts into an accessible \"context\" and implements all the required inference logic. class Model ( mlflow . pyfunc . PythonModel ): def load_context ( self , context ): # we use paths in the context to load everything self . model_path = context . artifacts [ 'model' ] self . model = load_model ( self . model_path ) with open ( context . artifacts [ 'tokenizer_path' ], 'rb' ) as f : self . tokenizer = pickle . load ( f ) def predict ( self , context , model_input ): # preprocess input, tokenize, pad, and call the model df = preprocess_df ( model_input ) corpus = create_corpus ( df ) sequences = self . tokenizer . texts_to_sequences ( corpus ) tweet_pad = pad_sequences ( sequences , maxlen = MAX_LEN , truncating = 'post' , padding = 'post' ) df = tweet_pad [: df . shape [ 0 ]] y_pre = self . model . predict ( df ) y_pre = np . round ( y_pre ) . astype ( int ) . flatten () . tolist () return list ( y_pre ) Here, we load multiple artifacts and use them to guarantee the input data is in the same format that was used for training. Ideally, you would abstract this even further into a single preprocess method that is called both at the training time and at the inference time. Finally, we start serving by going to the directory where you called the script above and executing the mlflow models serve --model-uri ./nlp_kaggle command. Bringing the MLflow Model to MindsDB \u00b6 We execute the command below to create a predictor in MindsDB based on the created model. CREATE MODEL mindsdb . byom_mlflow_nlp PREDICT ` target ` USING url . predict = 'http://localhost:5000/invocations' , format = 'mlflow' , dtype_dict = { \"text\" : \"rich text\" , \"target\" : \"binary\" } ; Now, you can fetch predictions using the standard MindsDB syntax. Follow the guide on the SELECT statement to learn more. You can directly pass input data in the WHERE clause to get a single prediction. SELECT target FROM mindsdb . byom_mlflow_nlp WHERE text = 'The tsunami is coming, seek high ground' ; Or you can JOIN the model with a data table to get bulk predictions. Here, ensure that the data table exists and the database it belongs to is connected to your MindsDB instance. SELECT ta . text , tb . target AS predicted FROM db_byom . test . nlp_kaggle_test AS ta JOIN mindsdb . byom_mlflow_nlp AS tb ; Full Script \u00b6 For your reference, here is the full script that trains and saves the model. import re import pickle import string import mlflow.pyfunc import nltk import tqdm import sklearn import tensorflow import cloudpickle import numpy as np import pandas as pd from nltk.corpus import stopwords from nltk.tokenize import word_tokenize from sklearn.model_selection import train_test_split from tensorflow.keras.initializers import Constant from tensorflow.keras.layers import Embedding , LSTM , Dense , SpatialDropout1D from tensorflow.keras.models import Sequential from tensorflow.keras.optimizers import Adam from tensorflow.keras.preprocessing.sequence import pad_sequences from tensorflow.keras.preprocessing.text import Tokenizer from tensorflow.keras.models import load_model stop = set ( stopwords . words ( 'english' )) MAX_LEN = 100 GLOVE_DIM = 50 EPOCHS = 10 def preprocess_df ( df ): df = df [[ 'text' ]] funcs = [ remove_URL , remove_html , remove_emoji , remove_punct ] for fn in funcs : df [ 'text' ] = df [ 'text' ] . apply ( lambda x : fn ( x )) return df def remove_URL ( text ): url = re . compile ( r 'https?://\\S+|www\\.\\S+' ) return url . sub ( r '' , text ) def remove_html ( text ): html = re . compile ( r '<.*?>' ) return html . sub ( r '' , text ) def remove_punct ( text ): table = str . maketrans ( '' , '' , string . punctuation ) return text . translate ( table ) def remove_emoji ( text ): emoji_pattern = re . compile ( \"[\" u \" \\U0001F600 - \\U0001F64F \" # emoticons u \" \\U0001F300 - \\U0001F5FF \" # symbols & pictographs u \" \\U0001F680 - \\U0001F6FF \" # transport & map symbols u \" \\U0001F1E0 - \\U0001F1FF \" # flags (iOS) u \" \\U00002702 - \\U000027B0 \" u \" \\U000024C2 - \\U0001F251 \" \"]+\" , flags = re . UNICODE ) return emoji_pattern . sub ( r '' , text ) def create_corpus ( df ): corpus = [] for tweet in tqdm . tqdm ( df [ 'text' ]): words = [ word . lower () for word in word_tokenize ( tweet ) if (( word . isalpha () == 1 ) & ( word not in stop ))] corpus . append ( words ) return corpus class Model ( mlflow . pyfunc . PythonModel ): def load_context ( self , context ): self . model_path = context . artifacts [ 'model' ] with open ( context . artifacts [ 'tokenizer_path' ], 'rb' ) as f : self . tokenizer = pickle . load ( f ) self . model = load_model ( self . model_path ) def predict ( self , context , model_input ): df = preprocess_df ( model_input ) corpus = create_corpus ( df ) sequences = self . tokenizer . texts_to_sequences ( corpus ) tweet_pad = pad_sequences ( sequences , maxlen = MAX_LEN , truncating = 'post' , padding = 'post' ) df = tweet_pad [: df . shape [ 0 ]] y_pre = self . model . predict ( df ) y_pre = np . round ( y_pre ) . astype ( int ) . flatten () . tolist () return list ( y_pre ) if __name__ == '__main__' : train_model = True model_path = './' tokenizer_path = './tokenizer.pkl' run_name = 'test_run' mlflow_pyfunc_model_path = \"nlp_kaggle\" mlflow . set_tracking_uri ( \"sqlite:///mlflow.db\" ) if train_model : # preprocess data df = pd . read_csv ( './train.csv' ) target = df [[ 'target' ]] target_arr = target . values df = preprocess_df ( df ) train_corpus = create_corpus ( df ) # load embeddings embedding_dict = {} with open ( './glove.6B.50d.txt' , 'r' ) as f : for line in f : values = line . split () word = values [ 0 ] vectors = np . asarray ( values [ 1 :], 'float32' ) embedding_dict [ word ] = vectors f . close () # generate and save tokenizer tokenizer_obj = Tokenizer () tokenizer_obj . fit_on_texts ( train_corpus ) with open ( tokenizer_path , 'wb' ) as f : pickle . dump ( tokenizer_obj , f ) # tokenize and pad sequences = tokenizer_obj . texts_to_sequences ( train_corpus ) tweet_pad = pad_sequences ( sequences , maxlen = MAX_LEN , truncating = 'post' , padding = 'post' ) df = tweet_pad [: df . shape [ 0 ]] word_index = tokenizer_obj . word_index num_words = len ( word_index ) + 1 embedding_matrix = np . zeros (( num_words , GLOVE_DIM )) # fill embedding matrix for word , i in tqdm . tqdm ( word_index . items ()): if i > num_words : continue emb_vec = embedding_dict . get ( word ) if emb_vec is not None : embedding_matrix [ i ] = emb_vec X_train , X_test , y_train , y_test = train_test_split ( df , target_arr , test_size = 0.15 ) # generate model model = Sequential () embedding = Embedding ( num_words , GLOVE_DIM , embeddings_initializer = Constant ( embedding_matrix ), input_length = MAX_LEN , trainable = False ) model . add ( embedding ) model . add ( SpatialDropout1D ( 0.2 )) model . add ( LSTM ( 64 , dropout = 0.2 , recurrent_dropout = 0.2 )) model . add ( Dense ( 1 , activation = 'sigmoid' )) optimizer = Adam ( learning_rate = 1e-5 ) model . compile ( loss = 'binary_crossentropy' , optimizer = optimizer , metrics = [ 'accuracy' ]) # train and save model . fit ( X_train , y_train , batch_size = 4 , epochs = EPOCHS , validation_data = ( X_test , y_test ), verbose = 2 ) model . save ( model_path ) # save in mlflow format artifacts = { 'model' : model_path , 'tokenizer_path' : tokenizer_path , } conda_env = { 'channels' : [ 'defaults' ], 'dependencies' : [ 'python=3.8' , 'pip' , { 'pip' : [ 'mlflow' , 'tensorflow== {} ' . format ( tensorflow . __version__ ), 'cloudpickle== {} ' . format ( cloudpickle . __version__ ), 'nltk== {} ' . format ( nltk . __version__ ), 'pandas== {} ' . format ( pd . __version__ ), 'numpy== {} ' . format ( np . __version__ ), 'scikit-learn== {} ' . format ( sklearn . __version__ ), 'tqdm== {} ' . format ( tqdm . __version__ ) ], }, ], 'name' : 'nlp_keras_env' } # Save and register the MLflow Model with mlflow . start_run ( run_name = run_name ) as run : mlflow . pyfunc . save_model ( path = mlflow_pyfunc_model_path , python_model = Model (), conda_env = conda_env , artifacts = artifacts ) result = mlflow . register_model ( f \"runs:/ { run . info . run_id } / { mlflow_pyfunc_model_path } \" , f \" { mlflow_pyfunc_model_path } \" )","title":"MLflow"},{"location":"custom-model/mlflow/#mindsdb-and-mlflow","text":"MLflow allows you to create, train, and serve machine learning models, apart from other features, such as organizing experiments, tracking metrics, and more. Here, we present two usage examples of the MLflow.","title":"MindsDB and MLflow"},{"location":"custom-model/mlflow/#simple-example-of-logistic-regression","text":"Currently, there is no way to train an MLflow-wrapped model using the API. The training of the model takes place outside of MindsDB. The data must be pulled manually, for example, with a script. It is a good idea to use an MLflow run or experiment.","title":"Simple Example of Logistic Regression"},{"location":"custom-model/mlflow/#creating-the-mlflow-model","text":"We start by writing a script that creates and trains the model. After that, the script is saved using one of the saving methods offered by MLflow. Here, we use the model from this simple tutorial and the mlflow.sklearn.log_model method . Please note that the model must be a scikit-learn model. Once your model is trained, ensure that it is served and listens for input at a URL of your choice. Please note that your model may run on a different machine than the one where MindsDB runs. Here, we assume the URL to be http://localhost:5000/invocations , as in the tutorial. Let's run the train.py script that provides us with the <run-id> value for the model. $ python examples/sklearn_logistic_regression/train.py Score: 0 .666 Model saved in run <run-id> Now, let's execute the following command from the directory where the model resides, providing the <run-id> value. $ mlflow models serve --model-uri runs:/<run-id>/model We're ready to move to MindsDB.","title":"Creating the MLflow Model"},{"location":"custom-model/mlflow/#bringing-the-mlflow-model-to-mindsdb","text":"We execute the command below to create a predictor in MindsDB based on the created model. CREATE MODEL mindsdb . byom_mlflow PREDICT ` 1 ` -- `1` is the target column name USING url . predict = 'http://localhost:5000/invocations' , format = 'mlflow' , data_dtype = { \"0\" : \"integer\" , \"1\" : \"integer\" } ; Now, you can fetch predictions using the standard MindsDB syntax. Follow the guide on the SELECT statement to learn more. SELECT ` 1 ` FROM byom_mlflow WHERE ` 0 `= 2 ;","title":"Bringing the MLflow Model to MindsDB"},{"location":"custom-model/mlflow/#advanced-example-of-keras-nlp-model","text":"Before we start, download the natural language processing (NLP) dataset from Kaggle to reproduce the steps of this example. Here, we look at the best practices when your model needs custom data preprocessing, which is quite common. We use the mlflow.pyfunc module to complete the following: Save the model using mlflow.pyfunc.save_model . Subclass mlflow.pyfunc.PythonModel to wrap the model in a way compatible with MLflow that enables our custom inference logic to be called.","title":"Advanced Example of Keras NLP Model"},{"location":"custom-model/mlflow/#creating-the-mlflow-model_1","text":"In the script that trains the model, like this one or that one , there should be a call to the mlflow.pyfunc.save_model function at the end. It is to save every produced artifact. mlflow . pyfunc . save_model ( path = \"nlp_kaggle\" , python_model = Model (), conda_env = conda_env , artifacts = artifacts ) Here, artifacts is a dictionary storing all the expected output after running the training phase. In this case, we want both a model and a tokenizer to preprocess the input text. And conda_env specifies the environment in which your model is executed when it is served in a self-contained conda environment. It should include all the required packages and dependencies, like below: # these are accessible inside the Model() wrapper artifacts = { 'model' : model_path , 'tokenizer_path' : tokenizer_path , } # specs for an environment that is created when serving the model conda_env = { 'name' : 'nlp_keras_env' , 'channels' : [ 'defaults' ], 'dependencies' : [ 'python=3.8' , 'pip' , { 'pip' : [ 'mlflow' , 'tensorflow' , 'cloudpickle' , 'nltk' , 'pandas' , 'numpy' , 'scikit-learn' , 'tqdm' , ], }, ], } Finally, to store the model, you need to provide the wrapper class that loads all the produced artifacts into an accessible \"context\" and implements all the required inference logic. class Model ( mlflow . pyfunc . PythonModel ): def load_context ( self , context ): # we use paths in the context to load everything self . model_path = context . artifacts [ 'model' ] self . model = load_model ( self . model_path ) with open ( context . artifacts [ 'tokenizer_path' ], 'rb' ) as f : self . tokenizer = pickle . load ( f ) def predict ( self , context , model_input ): # preprocess input, tokenize, pad, and call the model df = preprocess_df ( model_input ) corpus = create_corpus ( df ) sequences = self . tokenizer . texts_to_sequences ( corpus ) tweet_pad = pad_sequences ( sequences , maxlen = MAX_LEN , truncating = 'post' , padding = 'post' ) df = tweet_pad [: df . shape [ 0 ]] y_pre = self . model . predict ( df ) y_pre = np . round ( y_pre ) . astype ( int ) . flatten () . tolist () return list ( y_pre ) Here, we load multiple artifacts and use them to guarantee the input data is in the same format that was used for training. Ideally, you would abstract this even further into a single preprocess method that is called both at the training time and at the inference time. Finally, we start serving by going to the directory where you called the script above and executing the mlflow models serve --model-uri ./nlp_kaggle command.","title":"Creating the MLflow Model"},{"location":"custom-model/mlflow/#bringing-the-mlflow-model-to-mindsdb_1","text":"We execute the command below to create a predictor in MindsDB based on the created model. CREATE MODEL mindsdb . byom_mlflow_nlp PREDICT ` target ` USING url . predict = 'http://localhost:5000/invocations' , format = 'mlflow' , dtype_dict = { \"text\" : \"rich text\" , \"target\" : \"binary\" } ; Now, you can fetch predictions using the standard MindsDB syntax. Follow the guide on the SELECT statement to learn more. You can directly pass input data in the WHERE clause to get a single prediction. SELECT target FROM mindsdb . byom_mlflow_nlp WHERE text = 'The tsunami is coming, seek high ground' ; Or you can JOIN the model with a data table to get bulk predictions. Here, ensure that the data table exists and the database it belongs to is connected to your MindsDB instance. SELECT ta . text , tb . target AS predicted FROM db_byom . test . nlp_kaggle_test AS ta JOIN mindsdb . byom_mlflow_nlp AS tb ;","title":"Bringing the MLflow Model to MindsDB"},{"location":"custom-model/mlflow/#full-script","text":"For your reference, here is the full script that trains and saves the model. import re import pickle import string import mlflow.pyfunc import nltk import tqdm import sklearn import tensorflow import cloudpickle import numpy as np import pandas as pd from nltk.corpus import stopwords from nltk.tokenize import word_tokenize from sklearn.model_selection import train_test_split from tensorflow.keras.initializers import Constant from tensorflow.keras.layers import Embedding , LSTM , Dense , SpatialDropout1D from tensorflow.keras.models import Sequential from tensorflow.keras.optimizers import Adam from tensorflow.keras.preprocessing.sequence import pad_sequences from tensorflow.keras.preprocessing.text import Tokenizer from tensorflow.keras.models import load_model stop = set ( stopwords . words ( 'english' )) MAX_LEN = 100 GLOVE_DIM = 50 EPOCHS = 10 def preprocess_df ( df ): df = df [[ 'text' ]] funcs = [ remove_URL , remove_html , remove_emoji , remove_punct ] for fn in funcs : df [ 'text' ] = df [ 'text' ] . apply ( lambda x : fn ( x )) return df def remove_URL ( text ): url = re . compile ( r 'https?://\\S+|www\\.\\S+' ) return url . sub ( r '' , text ) def remove_html ( text ): html = re . compile ( r '<.*?>' ) return html . sub ( r '' , text ) def remove_punct ( text ): table = str . maketrans ( '' , '' , string . punctuation ) return text . translate ( table ) def remove_emoji ( text ): emoji_pattern = re . compile ( \"[\" u \" \\U0001F600 - \\U0001F64F \" # emoticons u \" \\U0001F300 - \\U0001F5FF \" # symbols & pictographs u \" \\U0001F680 - \\U0001F6FF \" # transport & map symbols u \" \\U0001F1E0 - \\U0001F1FF \" # flags (iOS) u \" \\U00002702 - \\U000027B0 \" u \" \\U000024C2 - \\U0001F251 \" \"]+\" , flags = re . UNICODE ) return emoji_pattern . sub ( r '' , text ) def create_corpus ( df ): corpus = [] for tweet in tqdm . tqdm ( df [ 'text' ]): words = [ word . lower () for word in word_tokenize ( tweet ) if (( word . isalpha () == 1 ) & ( word not in stop ))] corpus . append ( words ) return corpus class Model ( mlflow . pyfunc . PythonModel ): def load_context ( self , context ): self . model_path = context . artifacts [ 'model' ] with open ( context . artifacts [ 'tokenizer_path' ], 'rb' ) as f : self . tokenizer = pickle . load ( f ) self . model = load_model ( self . model_path ) def predict ( self , context , model_input ): df = preprocess_df ( model_input ) corpus = create_corpus ( df ) sequences = self . tokenizer . texts_to_sequences ( corpus ) tweet_pad = pad_sequences ( sequences , maxlen = MAX_LEN , truncating = 'post' , padding = 'post' ) df = tweet_pad [: df . shape [ 0 ]] y_pre = self . model . predict ( df ) y_pre = np . round ( y_pre ) . astype ( int ) . flatten () . tolist () return list ( y_pre ) if __name__ == '__main__' : train_model = True model_path = './' tokenizer_path = './tokenizer.pkl' run_name = 'test_run' mlflow_pyfunc_model_path = \"nlp_kaggle\" mlflow . set_tracking_uri ( \"sqlite:///mlflow.db\" ) if train_model : # preprocess data df = pd . read_csv ( './train.csv' ) target = df [[ 'target' ]] target_arr = target . values df = preprocess_df ( df ) train_corpus = create_corpus ( df ) # load embeddings embedding_dict = {} with open ( './glove.6B.50d.txt' , 'r' ) as f : for line in f : values = line . split () word = values [ 0 ] vectors = np . asarray ( values [ 1 :], 'float32' ) embedding_dict [ word ] = vectors f . close () # generate and save tokenizer tokenizer_obj = Tokenizer () tokenizer_obj . fit_on_texts ( train_corpus ) with open ( tokenizer_path , 'wb' ) as f : pickle . dump ( tokenizer_obj , f ) # tokenize and pad sequences = tokenizer_obj . texts_to_sequences ( train_corpus ) tweet_pad = pad_sequences ( sequences , maxlen = MAX_LEN , truncating = 'post' , padding = 'post' ) df = tweet_pad [: df . shape [ 0 ]] word_index = tokenizer_obj . word_index num_words = len ( word_index ) + 1 embedding_matrix = np . zeros (( num_words , GLOVE_DIM )) # fill embedding matrix for word , i in tqdm . tqdm ( word_index . items ()): if i > num_words : continue emb_vec = embedding_dict . get ( word ) if emb_vec is not None : embedding_matrix [ i ] = emb_vec X_train , X_test , y_train , y_test = train_test_split ( df , target_arr , test_size = 0.15 ) # generate model model = Sequential () embedding = Embedding ( num_words , GLOVE_DIM , embeddings_initializer = Constant ( embedding_matrix ), input_length = MAX_LEN , trainable = False ) model . add ( embedding ) model . add ( SpatialDropout1D ( 0.2 )) model . add ( LSTM ( 64 , dropout = 0.2 , recurrent_dropout = 0.2 )) model . add ( Dense ( 1 , activation = 'sigmoid' )) optimizer = Adam ( learning_rate = 1e-5 ) model . compile ( loss = 'binary_crossentropy' , optimizer = optimizer , metrics = [ 'accuracy' ]) # train and save model . fit ( X_train , y_train , batch_size = 4 , epochs = EPOCHS , validation_data = ( X_test , y_test ), verbose = 2 ) model . save ( model_path ) # save in mlflow format artifacts = { 'model' : model_path , 'tokenizer_path' : tokenizer_path , } conda_env = { 'channels' : [ 'defaults' ], 'dependencies' : [ 'python=3.8' , 'pip' , { 'pip' : [ 'mlflow' , 'tensorflow== {} ' . format ( tensorflow . __version__ ), 'cloudpickle== {} ' . format ( cloudpickle . __version__ ), 'nltk== {} ' . format ( nltk . __version__ ), 'pandas== {} ' . format ( pd . __version__ ), 'numpy== {} ' . format ( np . __version__ ), 'scikit-learn== {} ' . format ( sklearn . __version__ ), 'tqdm== {} ' . format ( tqdm . __version__ ) ], }, ], 'name' : 'nlp_keras_env' } # Save and register the MLflow Model with mlflow . start_run ( run_name = run_name ) as run : mlflow . pyfunc . save_model ( path = mlflow_pyfunc_model_path , python_model = Model (), conda_env = conda_env , artifacts = artifacts ) result = mlflow . register_model ( f \"runs:/ { run . info . run_id } / { mlflow_pyfunc_model_path } \" , f \" { mlflow_pyfunc_model_path } \" )","title":"Full Script"},{"location":"custom-model/openai/","text":"MindsDB and OpenAI \u00b6 OpenAI facilitates building and deploying ML models. How to Bring the OpenAI Model to MindsDB \u00b6 To bring your OpenAI model to MindsDB, run the CREATE MODEL statement as below. CREATE MODEL mindsdb . openai_model PREDICT target_text_column USING format = 'openai' , APITOKEN = 'yourapitoken' data_dtype = { \"0\" : \"integer\" , \"1\" : \"integer\" } ; Now you can query the mindsdb.models table to see your model. SELECT * FROM mindsdb . models WHERE name = 'openai_model' ; Check out the guide on the SELECT statement to see how to get the predictions.","title":"OpenAI"},{"location":"custom-model/openai/#mindsdb-and-openai","text":"OpenAI facilitates building and deploying ML models.","title":"MindsDB and OpenAI"},{"location":"custom-model/openai/#how-to-bring-the-openai-model-to-mindsdb","text":"To bring your OpenAI model to MindsDB, run the CREATE MODEL statement as below. CREATE MODEL mindsdb . openai_model PREDICT target_text_column USING format = 'openai' , APITOKEN = 'yourapitoken' data_dtype = { \"0\" : \"integer\" , \"1\" : \"integer\" } ; Now you can query the mindsdb.models table to see your model. SELECT * FROM mindsdb . models WHERE name = 'openai_model' ; Check out the guide on the SELECT statement to see how to get the predictions.","title":"How to Bring the OpenAI Model to MindsDB"},{"location":"custom-model/ray-serve/","text":"MindsDB and Ray Serve \u00b6 Ray Serve is a simple high-throughput model serving library that can wrap around your ML model. Simple Example of Logistic Regression \u00b6 In this example, we train an external scikit-learn model to use for making predictions. Creating the Ray Serve Model \u00b6 Let's look at an actual model wrapped by a class that complies with the requirements. import ray from fastapi import Request , FastAPI from ray import serve import time import pandas as pd import json from sklearn.linear_model import LogisticRegression app = FastAPI () ray . init () serve . start ( detached = True ) async def parse_req ( request : Request ): data = await request . json () target = data . get ( 'target' , None ) di = json . loads ( data [ 'df' ]) df = pd . DataFrame ( di ) return df , target @serve . deployment ( route_prefix = \"/my_model\" ) @serve . ingress ( app ) class MyModel : @app . post ( \"/train\" ) async def train ( self , request : Request ): df , target = await parse_req ( request ) feature_cols = list ( set ( list ( df . columns )) - set ([ target ])) self . feature_cols = feature_cols X = df . loc [:, self . feature_cols ] Y = list ( df [ target ]) self . model = LogisticRegression () self . model . fit ( X , Y ) return { 'status' : 'ok' } @app . post ( \"/predict\" ) async def predict ( self , request : Request ): df , _ = await parse_req ( request ) X = df . loc [:, self . feature_cols ] predictions = self . model . predict ( X ) pred_dict = { 'prediction' : [ float ( x ) for x in predictions ]} return pred_dict MyModel . deploy () while True : time . sleep ( 1 ) It is important to have the /train and /predict endpoints. The /train endpoint accepts two parameters to be sent via POST: df is a serialized dictionary that can be converted into a pandas dataframe. target is the name of the target column to be predicted. It returns a JSON object containing the status key and the ok value. The /predict endpoint requires one parameter to be sent via POST: df is a serialized dictionary that can be converted into a pandas dataframe. It returns a dictionary containing the prediction key. It stores the predictions. Additional keys can be returned for confidence and confidence intervals. Bringing the Ray Serve Model to MindsDB \u00b6 Once you start the RayServe-wrapped model, you can create and train it in MindsDB. CREATE MODEL mindsdb . byom_ray_serve FROM mydb ( SELECT number_of_rooms , initial_price , rental_price FROM test_data . home_rentals ) PREDICT number_of_rooms USING url . train = 'http://127.0.0.1:8000/my_model/train' , url . predict = 'http://127.0.0.1:8000/my_model/predict' , dtype_dict = { \"number_of_rooms\" : \"categorical\" , \"initial_price\" : \"integer\" , \"rental_price\" : \"integer\" } , format = 'ray_server' ; Now, you can fetch predictions using the standard MindsDB syntax. Follow the guide on the SELECT statement to learn more. You can directly pass input data in the WHERE clause to get a single prediction. SELECT * FROM byom_ray_serve WHERE initial_price = 3000 AND rental_price = 3000 ; Or you can JOIN the model wth a data table to get bulk predictions. SELECT tb . number_of_rooms , t . rental_price FROM mydb . test_data . home_rentals AS t JOIN mindsdb . byom_ray_serve AS tb WHERE t . rental_price > 5300 ; Limit for POST Requests Please note that if your model is behind a reverse proxy like nginx, you might have to increase the maximum limit for POST requests in order to receive the training data. MindsDB can send as much as you'd like - it has been stress-tested with over a billion rows. Example of Keras NLP Model \u00b6 Here, we consider a natural language processing (NLP) task where we want to train a neural network using Keras to detect if a tweet is related to a natural disaster, such as fires, earthquakes, etc. Please download this dataset to follow the example. Creating the Ray Serve Model \u00b6 We create a Ray Serve service that wraps around the Kaggle NLP Model that can be trained and used for making predictions. import re import time import json import string import requests from collections import Counter , defaultdict \u200b import ray from ray import serve \u200b import gensim import numpy as np import pandas as pd from tqdm import tqdm from nltk.util import ngrams from nltk.corpus import stopwords from nltk.tokenize import word_tokenize from fastapi import Request , FastAPI from sklearn.model_selection import train_test_split from sklearn.feature_extraction.text import CountVectorizer \u200b from tensorflow.keras.preprocessing.text import Tokenizer from tensorflow.keras.preprocessing.sequence import pad_sequences from tensorflow.keras.models import Sequential from tensorflow.keras.layers import Embedding , LSTM , Dense , SpatialDropout1D from tensorflow.keras.initializers import Constant from tensorflow.keras.optimizers import Adam \u200b app = FastAPI () stop = set ( stopwords . words ( 'english' )) \u200b \u200b async def parse_req ( request : Request ): data = await request . json () target = data . get ( 'target' , None ) di = json . loads ( data [ 'df' ]) df = pd . DataFrame ( di ) return df , target \u200b \u200b @serve . deployment ( route_prefix = \"/nlp_kaggle_model\" ) @serve . ingress ( app ) class Model : MAX_LEN = 100 GLOVE_DIM = 50 EPOCHS = 10 \u200b def __init__ ( self ): self . model = None \u200b @app . post ( \"/train\" ) async def train ( self , request : Request ): df , target = await parse_req ( request ) \u200b target_arr = df . pop ( target ) . values df = self . preprocess_df ( df ) train_corpus = self . create_corpus ( df ) \u200b self . embedding_dict = {} with open ( './glove.6B.50d.txt' , 'r' ) as f : for line in f : values = line . split () word = values [ 0 ] vectors = np . asarray ( values [ 1 :], 'float32' ) self . embedding_dict [ word ] = vectors f . close () \u200b self . tokenizer_obj = Tokenizer () self . tokenizer_obj . fit_on_texts ( train_corpus ) \u200b sequences = self . tokenizer_obj . texts_to_sequences ( train_corpus ) tweet_pad = pad_sequences ( sequences , maxlen = self . __class__ . MAX_LEN , truncating = 'post' , padding = 'post' ) df = tweet_pad [: df . shape [ 0 ]] \u200b word_index = self . tokenizer_obj . word_index num_words = len ( word_index ) + 1 embedding_matrix = np . zeros (( num_words , self . __class__ . GLOVE_DIM )) \u200b for word , i in tqdm ( word_index . items ()): if i > num_words : continue \u200b emb_vec = self . embedding_dict . get ( word ) if emb_vec is not None : embedding_matrix [ i ] = emb_vec \u200b self . model = Sequential () embedding = Embedding ( num_words , self . __class__ . GLOVE_DIM , embeddings_initializer = Constant ( embedding_matrix ), input_length = self . __class__ . MAX_LEN , trainable = False ) self . model . add ( embedding ) self . model . add ( SpatialDropout1D ( 0.2 )) self . model . add ( LSTM ( 64 , dropout = 0.2 , recurrent_dropout = 0.2 )) self . model . add ( Dense ( 1 , activation = 'sigmoid' )) \u200b optimizer = Adam ( learning_rate = 1e-5 ) self . model . compile ( loss = 'binary_crossentropy' , optimizer = optimizer , metrics = [ 'accuracy' ]) \u200b X_train , X_test , y_train , y_test = train_test_split ( df , target_arr , test_size = 0.15 ) self . model . fit ( X_train , y_train , batch_size = 4 , epochs = self . __class__ . EPOCHS , validation_data = ( X_test , y_test ), verbose = 2 ) \u200b return { 'status' : 'ok' } \u200b @app . post ( \"/predict\" ) async def predict ( self , request : Request ): df , _ = await parse_req ( request ) \u200b df = self . preprocess_df ( df ) test_corpus = self . create_corpus ( df ) \u200b sequences = self . tokenizer_obj . texts_to_sequences ( test_corpus ) tweet_pad = pad_sequences ( sequences , maxlen = self . __class__ . MAX_LEN , truncating = 'post' , padding = 'post' ) df = tweet_pad [: df . shape [ 0 ]] \u200b y_pre = self . model . predict ( df ) y_pre = np . round ( y_pre ) . astype ( int ) . flatten () . tolist () sub = pd . DataFrame ({ 'target' : y_pre }) \u200b pred_dict = { 'prediction' : [ float ( x ) for x in sub [ 'target' ] . values ]} return pred_dict \u200b def preprocess_df ( self , df ): df = df [[ 'text' ]] df [ 'text' ] = df [ 'text' ] . apply ( lambda x : self . remove_URL ( x )) df [ 'text' ] = df [ 'text' ] . apply ( lambda x : self . remove_html ( x )) df [ 'text' ] = df [ 'text' ] . apply ( lambda x : self . remove_emoji ( x )) df [ 'text' ] = df [ 'text' ] . apply ( lambda x : self . remove_punct ( x )) return df \u200b def remove_URL ( self , text ): url = re . compile ( r 'https?://\\S+|www\\.\\S+' ) return url . sub ( r '' , text ) \u200b def remove_html ( self , text ): html = re . compile ( r '<.*?>' ) return html . sub ( r '' , text ) \u200b def remove_punct ( self , text ): table = str . maketrans ( '' , '' , string . punctuation ) return text . translate ( table ) \u200b def remove_emoji ( self , text ): emoji_pattern = re . compile ( \"[\" u \" \\U0001F600 - \\U0001F64F \" # emoticons u \" \\U0001F300 - \\U0001F5FF \" # symbols & pictographs u \" \\U0001F680 - \\U0001F6FF \" # transport & map symbols u \" \\U0001F1E0 - \\U0001F1FF \" # flags (iOS) u \" \\U00002702 - \\U000027B0 \" u \" \\U000024C2 - \\U0001F251 \" \"]+\" , flags = re . UNICODE ) return emoji_pattern . sub ( r '' , text ) \u200b def create_corpus ( self , df ): corpus = [] for tweet in tqdm ( df [ 'text' ]): words = [ word . lower () for word in word_tokenize ( tweet ) if (( word . isalpha () == 1 ) & ( word not in stop ))] corpus . append ( words ) return corpus \u200b \u200b if __name__ == '__main__' : \u200b ray . init () serve . start ( detached = True ) \u200b Model . deploy () \u200b while True : time . sleep ( 1 ) Now, we need access to the training data. For that, we create a table called nlp_kaggle_train to load the dataset that the original model uses. The nlp_kaggle_train table contains the following columns: id INT , keyword VARCHAR ( 255 ), location VARCHAR ( 255 ), text VARCHAR ( 5000 ), target INT Please note that the specifics of the schema/table and how to ingest the CSV data vary depending on your database. Bringing the Ray Serve Model to MindsDB \u00b6 Now, we can create and train this custom model in MindsDB. CREATE MODEL mindsdb . byom_ray_serve_nlp FROM maria ( SELECT text , target FROM test . nlp_kaggle_train ) PREDICT target USING url . train = 'http://127.0.0.1:8000/nlp_kaggle_model/train' , url . predict = 'http://127.0.0.1:8000/nlp_kaggle_model/predict' , dtype_dict = { \"text\" : \"rich_text\" , \"target\" : \"integer\" } , format = 'ray_server' ; The training process takes some time, considering that this model is a neural network rather than a simple logistic regression. You can check the model status using this query: SELECT * FROM mindsdb . models WHERE name = 'byom_ray_serve_nlp' ; Once the status of the predictor has a value of trained , you can fetch predictions using the standard MindsDB syntax. Follow the guide on the SELECT statement to learn more. SELECT * FROM mindsdb . byom_ray_serve_nlp WHERE text = 'The tsunami is coming, seek high ground' ; The expected output of the query above is 1 . SELECT * FROM mindsdb . byom_ray_serve_nlp WHERE text = 'This is lovely dear friend' ; The expected output of the query above is 0 . Wrong Results? If your results do not match this example, try training the model for a longer amount of epochs.","title":"Ray Serve"},{"location":"custom-model/ray-serve/#mindsdb-and-ray-serve","text":"Ray Serve is a simple high-throughput model serving library that can wrap around your ML model.","title":"MindsDB and Ray Serve"},{"location":"custom-model/ray-serve/#simple-example-of-logistic-regression","text":"In this example, we train an external scikit-learn model to use for making predictions.","title":"Simple Example of Logistic Regression"},{"location":"custom-model/ray-serve/#creating-the-ray-serve-model","text":"Let's look at an actual model wrapped by a class that complies with the requirements. import ray from fastapi import Request , FastAPI from ray import serve import time import pandas as pd import json from sklearn.linear_model import LogisticRegression app = FastAPI () ray . init () serve . start ( detached = True ) async def parse_req ( request : Request ): data = await request . json () target = data . get ( 'target' , None ) di = json . loads ( data [ 'df' ]) df = pd . DataFrame ( di ) return df , target @serve . deployment ( route_prefix = \"/my_model\" ) @serve . ingress ( app ) class MyModel : @app . post ( \"/train\" ) async def train ( self , request : Request ): df , target = await parse_req ( request ) feature_cols = list ( set ( list ( df . columns )) - set ([ target ])) self . feature_cols = feature_cols X = df . loc [:, self . feature_cols ] Y = list ( df [ target ]) self . model = LogisticRegression () self . model . fit ( X , Y ) return { 'status' : 'ok' } @app . post ( \"/predict\" ) async def predict ( self , request : Request ): df , _ = await parse_req ( request ) X = df . loc [:, self . feature_cols ] predictions = self . model . predict ( X ) pred_dict = { 'prediction' : [ float ( x ) for x in predictions ]} return pred_dict MyModel . deploy () while True : time . sleep ( 1 ) It is important to have the /train and /predict endpoints. The /train endpoint accepts two parameters to be sent via POST: df is a serialized dictionary that can be converted into a pandas dataframe. target is the name of the target column to be predicted. It returns a JSON object containing the status key and the ok value. The /predict endpoint requires one parameter to be sent via POST: df is a serialized dictionary that can be converted into a pandas dataframe. It returns a dictionary containing the prediction key. It stores the predictions. Additional keys can be returned for confidence and confidence intervals.","title":"Creating the Ray Serve Model"},{"location":"custom-model/ray-serve/#bringing-the-ray-serve-model-to-mindsdb","text":"Once you start the RayServe-wrapped model, you can create and train it in MindsDB. CREATE MODEL mindsdb . byom_ray_serve FROM mydb ( SELECT number_of_rooms , initial_price , rental_price FROM test_data . home_rentals ) PREDICT number_of_rooms USING url . train = 'http://127.0.0.1:8000/my_model/train' , url . predict = 'http://127.0.0.1:8000/my_model/predict' , dtype_dict = { \"number_of_rooms\" : \"categorical\" , \"initial_price\" : \"integer\" , \"rental_price\" : \"integer\" } , format = 'ray_server' ; Now, you can fetch predictions using the standard MindsDB syntax. Follow the guide on the SELECT statement to learn more. You can directly pass input data in the WHERE clause to get a single prediction. SELECT * FROM byom_ray_serve WHERE initial_price = 3000 AND rental_price = 3000 ; Or you can JOIN the model wth a data table to get bulk predictions. SELECT tb . number_of_rooms , t . rental_price FROM mydb . test_data . home_rentals AS t JOIN mindsdb . byom_ray_serve AS tb WHERE t . rental_price > 5300 ; Limit for POST Requests Please note that if your model is behind a reverse proxy like nginx, you might have to increase the maximum limit for POST requests in order to receive the training data. MindsDB can send as much as you'd like - it has been stress-tested with over a billion rows.","title":"Bringing the Ray Serve Model to MindsDB"},{"location":"custom-model/ray-serve/#example-of-keras-nlp-model","text":"Here, we consider a natural language processing (NLP) task where we want to train a neural network using Keras to detect if a tweet is related to a natural disaster, such as fires, earthquakes, etc. Please download this dataset to follow the example.","title":"Example of Keras NLP Model"},{"location":"custom-model/ray-serve/#creating-the-ray-serve-model_1","text":"We create a Ray Serve service that wraps around the Kaggle NLP Model that can be trained and used for making predictions. import re import time import json import string import requests from collections import Counter , defaultdict \u200b import ray from ray import serve \u200b import gensim import numpy as np import pandas as pd from tqdm import tqdm from nltk.util import ngrams from nltk.corpus import stopwords from nltk.tokenize import word_tokenize from fastapi import Request , FastAPI from sklearn.model_selection import train_test_split from sklearn.feature_extraction.text import CountVectorizer \u200b from tensorflow.keras.preprocessing.text import Tokenizer from tensorflow.keras.preprocessing.sequence import pad_sequences from tensorflow.keras.models import Sequential from tensorflow.keras.layers import Embedding , LSTM , Dense , SpatialDropout1D from tensorflow.keras.initializers import Constant from tensorflow.keras.optimizers import Adam \u200b app = FastAPI () stop = set ( stopwords . words ( 'english' )) \u200b \u200b async def parse_req ( request : Request ): data = await request . json () target = data . get ( 'target' , None ) di = json . loads ( data [ 'df' ]) df = pd . DataFrame ( di ) return df , target \u200b \u200b @serve . deployment ( route_prefix = \"/nlp_kaggle_model\" ) @serve . ingress ( app ) class Model : MAX_LEN = 100 GLOVE_DIM = 50 EPOCHS = 10 \u200b def __init__ ( self ): self . model = None \u200b @app . post ( \"/train\" ) async def train ( self , request : Request ): df , target = await parse_req ( request ) \u200b target_arr = df . pop ( target ) . values df = self . preprocess_df ( df ) train_corpus = self . create_corpus ( df ) \u200b self . embedding_dict = {} with open ( './glove.6B.50d.txt' , 'r' ) as f : for line in f : values = line . split () word = values [ 0 ] vectors = np . asarray ( values [ 1 :], 'float32' ) self . embedding_dict [ word ] = vectors f . close () \u200b self . tokenizer_obj = Tokenizer () self . tokenizer_obj . fit_on_texts ( train_corpus ) \u200b sequences = self . tokenizer_obj . texts_to_sequences ( train_corpus ) tweet_pad = pad_sequences ( sequences , maxlen = self . __class__ . MAX_LEN , truncating = 'post' , padding = 'post' ) df = tweet_pad [: df . shape [ 0 ]] \u200b word_index = self . tokenizer_obj . word_index num_words = len ( word_index ) + 1 embedding_matrix = np . zeros (( num_words , self . __class__ . GLOVE_DIM )) \u200b for word , i in tqdm ( word_index . items ()): if i > num_words : continue \u200b emb_vec = self . embedding_dict . get ( word ) if emb_vec is not None : embedding_matrix [ i ] = emb_vec \u200b self . model = Sequential () embedding = Embedding ( num_words , self . __class__ . GLOVE_DIM , embeddings_initializer = Constant ( embedding_matrix ), input_length = self . __class__ . MAX_LEN , trainable = False ) self . model . add ( embedding ) self . model . add ( SpatialDropout1D ( 0.2 )) self . model . add ( LSTM ( 64 , dropout = 0.2 , recurrent_dropout = 0.2 )) self . model . add ( Dense ( 1 , activation = 'sigmoid' )) \u200b optimizer = Adam ( learning_rate = 1e-5 ) self . model . compile ( loss = 'binary_crossentropy' , optimizer = optimizer , metrics = [ 'accuracy' ]) \u200b X_train , X_test , y_train , y_test = train_test_split ( df , target_arr , test_size = 0.15 ) self . model . fit ( X_train , y_train , batch_size = 4 , epochs = self . __class__ . EPOCHS , validation_data = ( X_test , y_test ), verbose = 2 ) \u200b return { 'status' : 'ok' } \u200b @app . post ( \"/predict\" ) async def predict ( self , request : Request ): df , _ = await parse_req ( request ) \u200b df = self . preprocess_df ( df ) test_corpus = self . create_corpus ( df ) \u200b sequences = self . tokenizer_obj . texts_to_sequences ( test_corpus ) tweet_pad = pad_sequences ( sequences , maxlen = self . __class__ . MAX_LEN , truncating = 'post' , padding = 'post' ) df = tweet_pad [: df . shape [ 0 ]] \u200b y_pre = self . model . predict ( df ) y_pre = np . round ( y_pre ) . astype ( int ) . flatten () . tolist () sub = pd . DataFrame ({ 'target' : y_pre }) \u200b pred_dict = { 'prediction' : [ float ( x ) for x in sub [ 'target' ] . values ]} return pred_dict \u200b def preprocess_df ( self , df ): df = df [[ 'text' ]] df [ 'text' ] = df [ 'text' ] . apply ( lambda x : self . remove_URL ( x )) df [ 'text' ] = df [ 'text' ] . apply ( lambda x : self . remove_html ( x )) df [ 'text' ] = df [ 'text' ] . apply ( lambda x : self . remove_emoji ( x )) df [ 'text' ] = df [ 'text' ] . apply ( lambda x : self . remove_punct ( x )) return df \u200b def remove_URL ( self , text ): url = re . compile ( r 'https?://\\S+|www\\.\\S+' ) return url . sub ( r '' , text ) \u200b def remove_html ( self , text ): html = re . compile ( r '<.*?>' ) return html . sub ( r '' , text ) \u200b def remove_punct ( self , text ): table = str . maketrans ( '' , '' , string . punctuation ) return text . translate ( table ) \u200b def remove_emoji ( self , text ): emoji_pattern = re . compile ( \"[\" u \" \\U0001F600 - \\U0001F64F \" # emoticons u \" \\U0001F300 - \\U0001F5FF \" # symbols & pictographs u \" \\U0001F680 - \\U0001F6FF \" # transport & map symbols u \" \\U0001F1E0 - \\U0001F1FF \" # flags (iOS) u \" \\U00002702 - \\U000027B0 \" u \" \\U000024C2 - \\U0001F251 \" \"]+\" , flags = re . UNICODE ) return emoji_pattern . sub ( r '' , text ) \u200b def create_corpus ( self , df ): corpus = [] for tweet in tqdm ( df [ 'text' ]): words = [ word . lower () for word in word_tokenize ( tweet ) if (( word . isalpha () == 1 ) & ( word not in stop ))] corpus . append ( words ) return corpus \u200b \u200b if __name__ == '__main__' : \u200b ray . init () serve . start ( detached = True ) \u200b Model . deploy () \u200b while True : time . sleep ( 1 ) Now, we need access to the training data. For that, we create a table called nlp_kaggle_train to load the dataset that the original model uses. The nlp_kaggle_train table contains the following columns: id INT , keyword VARCHAR ( 255 ), location VARCHAR ( 255 ), text VARCHAR ( 5000 ), target INT Please note that the specifics of the schema/table and how to ingest the CSV data vary depending on your database.","title":"Creating the Ray Serve Model"},{"location":"custom-model/ray-serve/#bringing-the-ray-serve-model-to-mindsdb_1","text":"Now, we can create and train this custom model in MindsDB. CREATE MODEL mindsdb . byom_ray_serve_nlp FROM maria ( SELECT text , target FROM test . nlp_kaggle_train ) PREDICT target USING url . train = 'http://127.0.0.1:8000/nlp_kaggle_model/train' , url . predict = 'http://127.0.0.1:8000/nlp_kaggle_model/predict' , dtype_dict = { \"text\" : \"rich_text\" , \"target\" : \"integer\" } , format = 'ray_server' ; The training process takes some time, considering that this model is a neural network rather than a simple logistic regression. You can check the model status using this query: SELECT * FROM mindsdb . models WHERE name = 'byom_ray_serve_nlp' ; Once the status of the predictor has a value of trained , you can fetch predictions using the standard MindsDB syntax. Follow the guide on the SELECT statement to learn more. SELECT * FROM mindsdb . byom_ray_serve_nlp WHERE text = 'The tsunami is coming, seek high ground' ; The expected output of the query above is 1 . SELECT * FROM mindsdb . byom_ray_serve_nlp WHERE text = 'This is lovely dear friend' ; The expected output of the query above is 0 . Wrong Results? If your results do not match this example, try training the model for a longer amount of epochs.","title":"Bringing the Ray Serve Model to MindsDB"},{"location":"mongo/collection-structure/","text":"Collection Structure \u00b6 General Structure \u00b6 On start-up, the MindsDB database consists of 2 collections: databases and predictors . You can verify it by running the following MQL commands: USE mindsdb ; SHOW collections ; On execution, we get: + ---------------------------+ | Collections_in_mindsdb | + ---------------------------+ | databases | | predictors | + ---------------------------+ The predictors Collection \u00b6 All the trained machine learning models are visible as new documents inside the predictors collection. The predictors collection stores information about each model in the JSON format, as shown below. { \"name\" : \"model_name\" , \"status\" : \"status\" , \"accuracy\" : 0.999 , \"predict\" : \"value_to_be_predicted\" , \"update_status\" : \"update_status\" , \"mindsdb_version\" : \"22.8.2.1\" , \"error\" : \"error_info\" , \"select_data_query\" : \"\" , \"training_options\" : \"\" } Where: Name Description \"name\" The name of the model. \"status\" Training status ( generating , or training , or complete , or error ). \"accuracy\" The model accuracy ( 0.999 is a sample accuracy value). \"predict\" The name of the target column to be predicted. \"update_status\" Training update status ( up_to_date , or updating , or available ). \"mindsdb_version\" The MindsDB version used while training ( 22.8.2.1 is a sample version value). \"error\" Error message stores a value in case of an error, otherwise, it is null. \"select_data_query\" It is required for SQL API, otherwise, it is null. \"training_options\" Additional training parameters. The databases Collection \u00b6 All the Mongo database connections are stored inside the databases collection, as shown below. { \"name\" : \"mongo_int\" , \"database_type\" : \"mongodb\" , \"host\" : \"\" , \"port\" : 27017 , \"user\" : null } Where: Name Description \"name\" The name of the integration. \"database_type\" The database type (here, mongodb ). \"host\" The Mongo host. \"port\" The Mongo port. \"user\" The Mongo user.","title":"Collection Structure"},{"location":"mongo/collection-structure/#collection-structure","text":"","title":"Collection Structure"},{"location":"mongo/collection-structure/#general-structure","text":"On start-up, the MindsDB database consists of 2 collections: databases and predictors . You can verify it by running the following MQL commands: USE mindsdb ; SHOW collections ; On execution, we get: + ---------------------------+ | Collections_in_mindsdb | + ---------------------------+ | databases | | predictors | + ---------------------------+","title":"General Structure"},{"location":"mongo/collection-structure/#the-predictors-collection","text":"All the trained machine learning models are visible as new documents inside the predictors collection. The predictors collection stores information about each model in the JSON format, as shown below. { \"name\" : \"model_name\" , \"status\" : \"status\" , \"accuracy\" : 0.999 , \"predict\" : \"value_to_be_predicted\" , \"update_status\" : \"update_status\" , \"mindsdb_version\" : \"22.8.2.1\" , \"error\" : \"error_info\" , \"select_data_query\" : \"\" , \"training_options\" : \"\" } Where: Name Description \"name\" The name of the model. \"status\" Training status ( generating , or training , or complete , or error ). \"accuracy\" The model accuracy ( 0.999 is a sample accuracy value). \"predict\" The name of the target column to be predicted. \"update_status\" Training update status ( up_to_date , or updating , or available ). \"mindsdb_version\" The MindsDB version used while training ( 22.8.2.1 is a sample version value). \"error\" Error message stores a value in case of an error, otherwise, it is null. \"select_data_query\" It is required for SQL API, otherwise, it is null. \"training_options\" Additional training parameters.","title":"The predictors Collection"},{"location":"mongo/collection-structure/#the-databases-collection","text":"All the Mongo database connections are stored inside the databases collection, as shown below. { \"name\" : \"mongo_int\" , \"database_type\" : \"mongodb\" , \"host\" : \"\" , \"port\" : 27017 , \"user\" : null } Where: Name Description \"name\" The name of the integration. \"database_type\" The database type (here, mongodb ). \"host\" The Mongo host. \"port\" The Mongo port. \"user\" The Mongo user.","title":"The databases Collection"},{"location":"mongo/database/","text":"Connecting Databases to MindsDB in Mongo \u00b6 Integrations, or external databases, provide data to be used for making forecasts. Here, we use the databases.insertOne() method to connect the integrations to Mongo. The db.databases.insertOne() Method \u00b6 Description \u00b6 MindsDB enables adding databases to your Mongo instance using the db.databases.insertOne() method. Our MindsDB Mongo API supports creating a connection by passing the database credentials. Syntax \u00b6 Here is the syntax: db . databases . insertOne ( { name : \"mongo_int\" , engine : \"mongodb\" , connection_args : { \"port\" : 27017 , \"host\" : \"mongodb+srv://admin:@localhost\" , \"database\" : \"test_data\" } } ); On execution, we get: { \"acknowledged\" : true , \"insertedId\" : Objec t Id( \"62dff63c6cc2fa93e1d7f12c\" ) } Where: Name Description name Identifier for the data source to be created. engine Database engine to be selected. connection_args { \"key\" : \"value\" } object storing the connection parameters such as port, host, database. Example \u00b6 Creating a New Connection \u00b6 Here is an example of how to connect to the local MongoDB. db . databases . insertOne ( { name : \"mongo_local\" , engine : \"mongodb\" , connection_args : { \"port\" : 27017 , \"host\" : \"mongodb+srv://admin:@localhost\" , \"database\" : \"test_data\" } } ); On execution, we get: { \"acknowledged\" : true , \"insertedId\" : Objec t Id( \"62dff63c6cc2fa93e1d7f12c\" ) } Listing Linked Databases \u00b6 You can list all the linked databases using the following command: SHOW dbs ; On execution, we get: + --------------------+ | Database | + --------------------+ | admin | | files | | information_schema | | mindsdb | | mongo_int | | views | + --------------------+","title":"Databases"},{"location":"mongo/database/#connecting-databases-to-mindsdb-in-mongo","text":"Integrations, or external databases, provide data to be used for making forecasts. Here, we use the databases.insertOne() method to connect the integrations to Mongo.","title":"Connecting Databases to MindsDB in Mongo"},{"location":"mongo/database/#the-dbdatabasesinsertone-method","text":"","title":"The db.databases.insertOne() Method"},{"location":"mongo/database/#description","text":"MindsDB enables adding databases to your Mongo instance using the db.databases.insertOne() method. Our MindsDB Mongo API supports creating a connection by passing the database credentials.","title":"Description"},{"location":"mongo/database/#syntax","text":"Here is the syntax: db . databases . insertOne ( { name : \"mongo_int\" , engine : \"mongodb\" , connection_args : { \"port\" : 27017 , \"host\" : \"mongodb+srv://admin:@localhost\" , \"database\" : \"test_data\" } } ); On execution, we get: { \"acknowledged\" : true , \"insertedId\" : Objec t Id( \"62dff63c6cc2fa93e1d7f12c\" ) } Where: Name Description name Identifier for the data source to be created. engine Database engine to be selected. connection_args { \"key\" : \"value\" } object storing the connection parameters such as port, host, database.","title":"Syntax"},{"location":"mongo/database/#example","text":"","title":"Example"},{"location":"mongo/database/#creating-a-new-connection","text":"Here is an example of how to connect to the local MongoDB. db . databases . insertOne ( { name : \"mongo_local\" , engine : \"mongodb\" , connection_args : { \"port\" : 27017 , \"host\" : \"mongodb+srv://admin:@localhost\" , \"database\" : \"test_data\" } } ); On execution, we get: { \"acknowledged\" : true , \"insertedId\" : Objec t Id( \"62dff63c6cc2fa93e1d7f12c\" ) }","title":"Creating a New Connection"},{"location":"mongo/database/#listing-linked-databases","text":"You can list all the linked databases using the following command: SHOW dbs ; On execution, we get: + --------------------+ | Database | + --------------------+ | admin | | files | | information_schema | | mindsdb | | mongo_int | | views | + --------------------+","title":"Listing Linked Databases"},{"location":"mongo/delete/","text":"Deleting a Predictor \u00b6 The db.predictors.deleteOne() Method \u00b6 Description \u00b6 The db.predictors.deleteOne() method deletes an ML model specified in its argument. Syntax \u00b6 Here is the syntax: db . predictors . deleteOne ( { name : \"predictor_name\" } ); On execution, we get: { \"acknowledged\" : true , \"deletedCount\" : 1 } Where: Name Description name Name of the model to be deleted. Example \u00b6 Listing All the Predictors \u00b6 Before deleting a predictor, let's list all the available predictors using the db.predictors.find() method. db . predictors . find ( {} ); On execution, we get: { \"name\" : \"home_rentals_model\" , \"status\" : \"complete\" , \"accuracy\" : \"1.0\" , \"predict\" : \"rental_price\" , \"update_status\" : \"up_to_date\" , \"mindsdb_version\" : \"22.8.3.1\" , \"error\" : null , \"select_data_query\" : \"\" , \"training_options\" : \"\" }, { \"name\" : \"other_model\" , \"status\" : \"complete\" , \"accuracy\" : \"1.0\" , \"predict\" : \"value_to_be_predicted\" , \"update_status\" : \"up_to_date\" , \"mindsdb_version\" : \"22.8.3.1\" , \"error\" : null , \"select_data_query\" : \"\" , \"training_options\" : \"\" } Dropping a Predictor \u00b6 The db.predictors.deleteOne() method drops the model collection called home_rentals_model . db . predictors . deleteOne ( { name : \"home_rentals_model\" } ); On execution, we get: { \"acknowledged\" : true , \"deletedCount\" : 1 } Validating the Deletion \u00b6 You can validate that the model was removed by listing all the predictors. db . predictors . find ( {} ); On execution, we get: { \"name\" : \"other_model\" , \"status\" : \"complete\" , \"accuracy\" : \"1.0\" , \"predict\" : \"value_to_be_predicted\" , \"update_status\" : \"up_to_date\" , \"mindsdb_version\" : \"22.8.3.1\" , \"error\" : null , \"select_data_query\" : \"\" , \"training_options\" : \"\" }","title":"Delete()"},{"location":"mongo/delete/#deleting-a-predictor","text":"","title":"Deleting a Predictor"},{"location":"mongo/delete/#the-dbpredictorsdeleteone-method","text":"","title":"The db.predictors.deleteOne() Method"},{"location":"mongo/delete/#description","text":"The db.predictors.deleteOne() method deletes an ML model specified in its argument.","title":"Description"},{"location":"mongo/delete/#syntax","text":"Here is the syntax: db . predictors . deleteOne ( { name : \"predictor_name\" } ); On execution, we get: { \"acknowledged\" : true , \"deletedCount\" : 1 } Where: Name Description name Name of the model to be deleted.","title":"Syntax"},{"location":"mongo/delete/#example","text":"","title":"Example"},{"location":"mongo/delete/#listing-all-the-predictors","text":"Before deleting a predictor, let's list all the available predictors using the db.predictors.find() method. db . predictors . find ( {} ); On execution, we get: { \"name\" : \"home_rentals_model\" , \"status\" : \"complete\" , \"accuracy\" : \"1.0\" , \"predict\" : \"rental_price\" , \"update_status\" : \"up_to_date\" , \"mindsdb_version\" : \"22.8.3.1\" , \"error\" : null , \"select_data_query\" : \"\" , \"training_options\" : \"\" }, { \"name\" : \"other_model\" , \"status\" : \"complete\" , \"accuracy\" : \"1.0\" , \"predict\" : \"value_to_be_predicted\" , \"update_status\" : \"up_to_date\" , \"mindsdb_version\" : \"22.8.3.1\" , \"error\" : null , \"select_data_query\" : \"\" , \"training_options\" : \"\" }","title":"Listing All the Predictors"},{"location":"mongo/delete/#dropping-a-predictor","text":"The db.predictors.deleteOne() method drops the model collection called home_rentals_model . db . predictors . deleteOne ( { name : \"home_rentals_model\" } ); On execution, we get: { \"acknowledged\" : true , \"deletedCount\" : 1 }","title":"Dropping a Predictor"},{"location":"mongo/delete/#validating-the-deletion","text":"You can validate that the model was removed by listing all the predictors. db . predictors . find ( {} ); On execution, we get: { \"name\" : \"other_model\" , \"status\" : \"complete\" , \"accuracy\" : \"1.0\" , \"predict\" : \"value_to_be_predicted\" , \"update_status\" : \"up_to_date\" , \"mindsdb_version\" : \"22.8.3.1\" , \"error\" : null , \"select_data_query\" : \"\" , \"training_options\" : \"\" }","title":"Validating the Deletion"},{"location":"mongo/find/","text":"Making Predictions using the ML Models \u00b6 The find() Method \u00b6 Description \u00b6 The find() method is used to get predictions from the model table. The data is not persistent - it is returned on the fly as a result-document. Syntax \u00b6 Here is the syntax: db . predictor_name . find ( { column : \"value\" , column : \"value\" } ); On execution, we get: { \"column_name1\" : \"value\" , \"column_name2\" : \"value\" , ...colum ns \"select_data_query\" : null , \"when_data\" : null , \"<target_variable>_original\" : \"value\" , \"<target_variable>_confidence\" : \"value\" , \"<target_variable>_explain\" : \"{\\\"predicted_value\\\": value, \\\"confidence\\\": value, \\\"anomaly\\\": null, \\\"truth\\\": null, \\\"confidence_lower_bound\\\": value \\\"confidence_upper_bound\\\": value}\" , \"<target_variable>_anomaly\" : \"value\" , \"<target_variable>_min\" : \"value\" , \"<target_variable>_max\" : \"value\" } Where: Expressions Description \"<target_variable>_original\" The real value of the target variable from the collection. \"<target_variable>_confidence\" Model confidence. \"<target_variable>_explain\" JSON object that contains additional information, such as predicted_value , confidence , anomaly , truth , confidence_lower_bound , confidence_upper_bound . \"<[target_variable>_anomaly\" Model anomaly. \"<[target_variable>_min\" Lower bound value. \"<[target_variable>_max\" Upper bound value. Example \u00b6 Making a Single Prediction \u00b6 The following MQL statement fetches the predicted value of the rental_price column from the home_rentals_model model. The predicted value is the rental price of a property with attributes listed as a parameter to the find() method. db . home_rentals_model . find ( { sqft : \"823\" , location : \"good\" , neighborhood : \"downtown\" , days_on_market : \"10\" } ); On execution, we get: { \"sqft\" : 823 , \"location\" : \"good\" , \"neighborhood\" : \"downtown\" , \"days_on_market\" : 10 , \"number_of_rooms\" : null , \"number_of_bathrooms\" : null , \"initial_price\" : null , \"rental_price\" : 1431.323795180614 , \"select_data_query\" : null , \"when_data\" : null , \"rental_price_original\" : null , \"rental_price_confidence\" : 0.99 , \"rental_price_explain\" : \"{\\\"predicted_value\\\": 1431.323795180614, \\\"confidence\\\": 0.99, \\\"anomaly\\\": null, \\\"truth\\\": null, \\\"confidence_lower_bound\\\": 1379.4387560440227, \\\"confidence_upper_bound\\\": 1483.2088343172054}\" , \"rental_price_anomaly\" : null , \"rental_price_min\" : 1379.4387560440227 , \"rental_price_max\" : 1483.2088343172054 } Making Bulk Predictions \u00b6 Bulk Predictions WIP The bulk predictions is a work in progress.","title":"Find()"},{"location":"mongo/find/#making-predictions-using-the-ml-models","text":"","title":"Making Predictions using the ML Models"},{"location":"mongo/find/#the-find-method","text":"","title":"The find() Method"},{"location":"mongo/find/#description","text":"The find() method is used to get predictions from the model table. The data is not persistent - it is returned on the fly as a result-document.","title":"Description"},{"location":"mongo/find/#syntax","text":"Here is the syntax: db . predictor_name . find ( { column : \"value\" , column : \"value\" } ); On execution, we get: { \"column_name1\" : \"value\" , \"column_name2\" : \"value\" , ...colum ns \"select_data_query\" : null , \"when_data\" : null , \"<target_variable>_original\" : \"value\" , \"<target_variable>_confidence\" : \"value\" , \"<target_variable>_explain\" : \"{\\\"predicted_value\\\": value, \\\"confidence\\\": value, \\\"anomaly\\\": null, \\\"truth\\\": null, \\\"confidence_lower_bound\\\": value \\\"confidence_upper_bound\\\": value}\" , \"<target_variable>_anomaly\" : \"value\" , \"<target_variable>_min\" : \"value\" , \"<target_variable>_max\" : \"value\" } Where: Expressions Description \"<target_variable>_original\" The real value of the target variable from the collection. \"<target_variable>_confidence\" Model confidence. \"<target_variable>_explain\" JSON object that contains additional information, such as predicted_value , confidence , anomaly , truth , confidence_lower_bound , confidence_upper_bound . \"<[target_variable>_anomaly\" Model anomaly. \"<[target_variable>_min\" Lower bound value. \"<[target_variable>_max\" Upper bound value.","title":"Syntax"},{"location":"mongo/find/#example","text":"","title":"Example"},{"location":"mongo/find/#making-a-single-prediction","text":"The following MQL statement fetches the predicted value of the rental_price column from the home_rentals_model model. The predicted value is the rental price of a property with attributes listed as a parameter to the find() method. db . home_rentals_model . find ( { sqft : \"823\" , location : \"good\" , neighborhood : \"downtown\" , days_on_market : \"10\" } ); On execution, we get: { \"sqft\" : 823 , \"location\" : \"good\" , \"neighborhood\" : \"downtown\" , \"days_on_market\" : 10 , \"number_of_rooms\" : null , \"number_of_bathrooms\" : null , \"initial_price\" : null , \"rental_price\" : 1431.323795180614 , \"select_data_query\" : null , \"when_data\" : null , \"rental_price_original\" : null , \"rental_price_confidence\" : 0.99 , \"rental_price_explain\" : \"{\\\"predicted_value\\\": 1431.323795180614, \\\"confidence\\\": 0.99, \\\"anomaly\\\": null, \\\"truth\\\": null, \\\"confidence_lower_bound\\\": 1379.4387560440227, \\\"confidence_upper_bound\\\": 1483.2088343172054}\" , \"rental_price_anomaly\" : null , \"rental_price_min\" : 1379.4387560440227 , \"rental_price_max\" : 1483.2088343172054 }","title":"Making a Single Prediction"},{"location":"mongo/find/#making-bulk-predictions","text":"Bulk Predictions WIP The bulk predictions is a work in progress.","title":"Making Bulk Predictions"},{"location":"mongo/insert/","text":"Creating Predictors in Mongo \u00b6 Predictors are the machine learning models that enable us to forecast future data based on the available data. By using the db.predictors.insert() method, we create and train predictors in Mongo. The db.predictors.insert() Method \u00b6 Description \u00b6 The db.predictors.insert() method creates and trains a new model. Syntax \u00b6 Here is the syntax: db . predictors . insert ( { name : \"predictor_name\" , predict : \"target_column\" , connection : \"integration_name\" , select_data_query : \"db.collection_name.find({})\" } ); On execution, we get: Wri te Resul t ( { \"nInserted\" : 1 } ) Where: Expressions Description name The name of the model to be created. predict The name of the target column to be predicted. connection The name of the integration created via the db.databases.insertOne() method or file upload . select_data_query Object that stores the data collection name to be used for training and validation and additional arguments for filtering the data. Checking Predictor Status After running the db.predictors.insert() method, execute the db.predictors.find() method from the mindsdb.models collection to check the status of the model. db . predictors . find ( { name : \"model_name\" } ); Example \u00b6 Creating a Predictor \u00b6 This example shows how you can create and train the home_rentals_model machine learning model to predict the rental prices for real estate properties inside the dataset. db . predictors . insert ( { name : \"home_rentals_model\" , predict : \"rental_price\" , connection : \"mongo_integration\" , select_data_query : \"db.home_rentals.find({})\" } ); On execution, we get: Wri te Resul t ( { \"nInserted\" : 1 } ) Checking Predictor Status \u00b6 To check the predictor status, query the mindsdb.models using the db.predictors.find() command. db . predictors . find ( { name : \"home_rentals_model\" } ); On execution, we get: { \"name\" : \"home_rentals_model\" , \"status\" : \"complete\" , \"accuracy\" : 0.91 , \"predict\" : \"rental_price\" , \"update_status\" : \"up_to_date\" , \"mindsdb_version\" : \"22.8.3.1\" , \"error\" : null , \"select_data_query\" : \"\" , \"training_options\" : \"\" }","title":"Predictors"},{"location":"mongo/insert/#creating-predictors-in-mongo","text":"Predictors are the machine learning models that enable us to forecast future data based on the available data. By using the db.predictors.insert() method, we create and train predictors in Mongo.","title":"Creating Predictors in Mongo"},{"location":"mongo/insert/#the-dbpredictorsinsert-method","text":"","title":"The db.predictors.insert() Method"},{"location":"mongo/insert/#description","text":"The db.predictors.insert() method creates and trains a new model.","title":"Description"},{"location":"mongo/insert/#syntax","text":"Here is the syntax: db . predictors . insert ( { name : \"predictor_name\" , predict : \"target_column\" , connection : \"integration_name\" , select_data_query : \"db.collection_name.find({})\" } ); On execution, we get: Wri te Resul t ( { \"nInserted\" : 1 } ) Where: Expressions Description name The name of the model to be created. predict The name of the target column to be predicted. connection The name of the integration created via the db.databases.insertOne() method or file upload . select_data_query Object that stores the data collection name to be used for training and validation and additional arguments for filtering the data. Checking Predictor Status After running the db.predictors.insert() method, execute the db.predictors.find() method from the mindsdb.models collection to check the status of the model. db . predictors . find ( { name : \"model_name\" } );","title":"Syntax"},{"location":"mongo/insert/#example","text":"","title":"Example"},{"location":"mongo/insert/#creating-a-predictor","text":"This example shows how you can create and train the home_rentals_model machine learning model to predict the rental prices for real estate properties inside the dataset. db . predictors . insert ( { name : \"home_rentals_model\" , predict : \"rental_price\" , connection : \"mongo_integration\" , select_data_query : \"db.home_rentals.find({})\" } ); On execution, we get: Wri te Resul t ( { \"nInserted\" : 1 } )","title":"Creating a Predictor"},{"location":"mongo/insert/#checking-predictor-status","text":"To check the predictor status, query the mindsdb.models using the db.predictors.find() command. db . predictors . find ( { name : \"home_rentals_model\" } ); On execution, we get: { \"name\" : \"home_rentals_model\" , \"status\" : \"complete\" , \"accuracy\" : 0.91 , \"predict\" : \"rental_price\" , \"update_status\" : \"up_to_date\" , \"mindsdb_version\" : \"22.8.3.1\" , \"error\" : null , \"select_data_query\" : \"\" , \"training_options\" : \"\" }","title":"Checking Predictor Status"},{"location":"mongo/mongo/","text":"Train a model from the MongoDB API \u00b6 Note: This is work in progress, please join our slack channel if you have any questions. Train new model \u00b6 To train a new model, you will need to insert() a new document inside the mindsdb.models collection. The object sent to the insert() for training the new model should contain: name (string) -- The name of the model. predict (string) -- The feature you want to predict. To predict multiple features, include a list of features. connection(string) -- The connection string for connecting to MongoDB. If you have used GUI to connect to MongoDB, that connection will be used. select_data_query (object) -- The object that contains info about getting the data to train the model. database(string) - The name of the database collection(string) - The name of the collection find(dict) - The dict that selects the documents from the collection, must be valid JSON format. Same as db.collection.find({...}) training_options (dict) -- Optional value that contains additional training parameters. To train timeseries model you need to provide training_options . db . predictors . insert ( { 'name' : str , 'predict' : str | list of fields , 'connection' : str , # optional 'select_data_query' : { 'database' : str , 'collection' : str , 'find' : dict } , 'training_options' : dict # optional } ) For the timeseries model: db.predictors.insert({ 'name': str, 'predict': str | list of fields, 'connection': str, # optional 'select_data_query':{ 'database': str, 'collection': str, 'find': dict }, 'training_options': { \"timeseries_settings\": { \"order_by\": list of fields, \"group_by\": list of fields, #optional \"horizon\": int, #optional \"use_previous_target\": Boolean, \"window\": int } } }) Train new model example \u00b6 The following example shows you how to train a new model from a mongo client. The collection used for training the model is the Telcom Customer Churn dataset. db . predictors . insert ( { 'name' : 'churn' , 'predict' : 'Churn' , 'select_data_query' : { 'database' : 'test_data' , 'collection' : 'customer_churn' , 'find' : {} } } ) This INSERT query will train a new model called churn that predicts the customer Churn value. Model training status \u00b6 To check that the training finished successfully, you can find() the model status inside mindsdb.models collection e.g.: db . predictors . find () That's it You have successfully trained a new model from a mongo shell. The next step is to get predictions by querying the model . Delete model \u00b6 To delete the model run remove function on predictors collection and send the name of the model to delete as: db.predictors.remove({name: 'model_name'}) Query the model from MongoDB API \u00b6 To get the predictions from the model, you will need to call find() method on the model collection and provide values for which you want to get prediction as an object: db . model_name . find ( { 'key' : 'value' , 'key' : 'value' } ) Note The object provided to find() method must be valid JSON format. Query example \u00b6 The following example shows you how to query the model from a mongo client. The collection used for training the model is the Telcom Customer Churn dataset. MindsDB will predict the customer Churn value based on the object values sent to find() method. db . churn . find ( { 'PhoneService' : 'Yes' , 'InternetService' : 'DSL' , 'OnlineService' : 'No' , 'MonthlyCharges' : 53 . 85 , 'TotalCharges' : 108 . 15 , 'tenure' : 2 , 'PaperlessBilling' : 'Yes' } ) You should get a response from MindsDB similar to: predicted_value confidence info Yes 0.8 Check JSON below { \"Churn\" : \"Yes\" , \"Churn_confidence\" : 0.8 , \"Churn_explain\" : { \"class_distribution\" : { \"No\" : 0.44513007027299717 , \"Yes\" : 0.5548699297270028 }, \"predicted_value\" : \"Yes\" , \"confidence\" : 0.8 , \"prediction_quality\" : \"very confident\" } }","title":"Train a model from the MongoDB API"},{"location":"mongo/mongo/#train-a-model-from-the-mongodb-api","text":"Note: This is work in progress, please join our slack channel if you have any questions.","title":"Train a model from the MongoDB API"},{"location":"mongo/mongo/#train-new-model","text":"To train a new model, you will need to insert() a new document inside the mindsdb.models collection. The object sent to the insert() for training the new model should contain: name (string) -- The name of the model. predict (string) -- The feature you want to predict. To predict multiple features, include a list of features. connection(string) -- The connection string for connecting to MongoDB. If you have used GUI to connect to MongoDB, that connection will be used. select_data_query (object) -- The object that contains info about getting the data to train the model. database(string) - The name of the database collection(string) - The name of the collection find(dict) - The dict that selects the documents from the collection, must be valid JSON format. Same as db.collection.find({...}) training_options (dict) -- Optional value that contains additional training parameters. To train timeseries model you need to provide training_options . db . predictors . insert ( { 'name' : str , 'predict' : str | list of fields , 'connection' : str , # optional 'select_data_query' : { 'database' : str , 'collection' : str , 'find' : dict } , 'training_options' : dict # optional } ) For the timeseries model: db.predictors.insert({ 'name': str, 'predict': str | list of fields, 'connection': str, # optional 'select_data_query':{ 'database': str, 'collection': str, 'find': dict }, 'training_options': { \"timeseries_settings\": { \"order_by\": list of fields, \"group_by\": list of fields, #optional \"horizon\": int, #optional \"use_previous_target\": Boolean, \"window\": int } } })","title":"Train new model"},{"location":"mongo/mongo/#train-new-model-example","text":"The following example shows you how to train a new model from a mongo client. The collection used for training the model is the Telcom Customer Churn dataset. db . predictors . insert ( { 'name' : 'churn' , 'predict' : 'Churn' , 'select_data_query' : { 'database' : 'test_data' , 'collection' : 'customer_churn' , 'find' : {} } } ) This INSERT query will train a new model called churn that predicts the customer Churn value.","title":"Train new model example"},{"location":"mongo/mongo/#model-training-status","text":"To check that the training finished successfully, you can find() the model status inside mindsdb.models collection e.g.: db . predictors . find () That's it You have successfully trained a new model from a mongo shell. The next step is to get predictions by querying the model .","title":"Model training status"},{"location":"mongo/mongo/#delete-model","text":"To delete the model run remove function on predictors collection and send the name of the model to delete as: db.predictors.remove({name: 'model_name'})","title":"Delete model"},{"location":"mongo/mongo/#query-the-model-from-mongodb-api","text":"To get the predictions from the model, you will need to call find() method on the model collection and provide values for which you want to get prediction as an object: db . model_name . find ( { 'key' : 'value' , 'key' : 'value' } ) Note The object provided to find() method must be valid JSON format.","title":"Query the model from MongoDB API"},{"location":"mongo/mongo/#query-example","text":"The following example shows you how to query the model from a mongo client. The collection used for training the model is the Telcom Customer Churn dataset. MindsDB will predict the customer Churn value based on the object values sent to find() method. db . churn . find ( { 'PhoneService' : 'Yes' , 'InternetService' : 'DSL' , 'OnlineService' : 'No' , 'MonthlyCharges' : 53 . 85 , 'TotalCharges' : 108 . 15 , 'tenure' : 2 , 'PaperlessBilling' : 'Yes' } ) You should get a response from MindsDB similar to: predicted_value confidence info Yes 0.8 Check JSON below { \"Churn\" : \"Yes\" , \"Churn_confidence\" : 0.8 , \"Churn_explain\" : { \"class_distribution\" : { \"No\" : 0.44513007027299717 , \"Yes\" : 0.5548699297270028 }, \"predicted_value\" : \"Yes\" , \"confidence\" : 0.8 , \"prediction_quality\" : \"very confident\" } }","title":"Query example"},{"location":"mongo/stats/","text":"Getting the Statistics \u00b6 The stats() Method \u00b6 The stats() method is used to display the attributes of an existing model. It accepts the {scale: \"attribute\"} object as an argument. Here is how to call the stats() method: db . predictor_name . stats ( { scale : \"attribute\" } ); Where: Name Description predictor_name The name of the predictor whose statistics you want to see. {scale: \"attribute\"} The argument of the stats() method defines the type of statistics ( {scale: \"features\"} , or {scale: \"model\"} , or {scale: \"ensemble\"} ). The stats() Method with the {scale: \"features\"} Parameter \u00b6 Description \u00b6 The db.predictor_name.stats({scale: \"features\"}) method is used to display the way the model encoded the data before training. Syntax \u00b6 Here is the syntax: db . predictor_name . stats ( { scale : \"features\" } ); On execution, we get: { \"data\" :[ { \"column\" : \"number_of_rooms\" , \"type\" : \"categorical\" , \"encoder\" : \"OneHotEncoder\" , \"role\" : \"feature\" } ] } Where: Name Description \"column\" The name of the column. \"type\" Type of the inferred data. \"encoder\" Encoder used. \"role\" Role of the column ( feature or target ). Example \u00b6 Let's describe the home_rentals_model model. db . home_rentals_model . stats ( { scale : \"features\" } ); On execution, we get: { \"data\" : [ { \"column\" : \"number_of_rooms\" , \"type\" : \"categorical\" , \"encoder\" : \"OneHotEncoder\" , \"role\" : \"feature\" }, { \"column\" : \"number_of_bathrooms\" , \"type\" : \"binary\" , \"encoder\" : \"BinaryEncoder\" , \"role\" : \"feature\" }, { \"column\" : \"sqft\" , \"type\" : \"float\" , \"encoder\" : \"NumericEncoder\" , \"role\" : \"feature\" }, { \"column\" : \"location\" , \"type\" : \"categorical\" , \"encoder\" : \"OneHotEncoder\" , \"role\" : \"feature\" }, { \"column\" : \"days_on_market\" , \"type\" : \"integer\" , \"encoder\" : \"NumericEncoder\" , \"role\" : \"feature\" }, { \"column\" : \"initial_price\" , \"type\" : \"integer\" , \"encoder\" : \"NumericEncoder\" , \"role\" : \"feature\" }, { \"column\" : \"neighborhood\" , \"type\" : \"categorical\" , \"encoder\" : \"OneHotEncoder\" , \"role\" : \"feature\" }, { \"column\" : \"rental_price\" , \"type\" : \"float\" , \"encoder\" : \"NumericEncoder\" , \"role\" : \"target\" } ], \"ns\" : \"mindsdb.home_rentals_model\" } The stats() Method with the {scale: \"model\"} Parameter \u00b6 Description \u00b6 The db.predictor_name.stats({scale: \"model\"}) method is used to display the performance of the candidate models. Syntax \u00b6 Here is the syntax: db . predictor_name . stats ( { scale : \"model\" } ); On execution, we get: { \"data\" : [ { \"name\" : \"candidate_model\" , \"performance\" : < 0.0 | 1.0 > , \"training_time\" : <seco n ds> , \"selected\" : < 0 | 1 > } ] } Where: Name Description \"name\" Name of the candidate model. \"performance\" Accuracy from 0 to 1 depending on the type of the model. \"training_time\" Time elapsed for the training of the model. \"selected\" 1 for the best performing model and 0 for the rest. Example \u00b6 Let's see the output for the home_rentals_model model. db . home_rentals_model . stats ( { scale : \"model\" } ); On execution, we get: { \"data\" : [ { \"name\" : \"Neural\" , \"performance\" : 0.999 , \"training_time\" : 48.37 , \"selected\" : 0 }, { \"name\" : \"LightGBM\" , \"performance\" : 1 , \"training_time\" : 33 , \"selected\" : 1 }, { \"name\" : \"Regression\" , \"performance\" : 0.999 , \"training_time\" : 0.05 , \"selected\" : 0 } ], \"ns\" : \"mindsdb.home_rentals_model\" } The stats() Method with the {scale: \"ensemble\"} Parameter \u00b6 Description \u00b6 The db.predictor_name.stats({scale: \"ensemble\"}) method is used to display the parameters used to select the best candidate model. Syntax \u00b6 Here is the syntax: db . predictor_name . stats ( { scale : \"ensemble\" } ); On execution, we get: + -----------------+ | ensemble | + -----------------+ | { JSON } | + -----------------+ Where: Name Description ensemble Object of the JSON type describing the parameters used to select the best candidate model. Example \u00b6 Example WIP This example is a work in progress. Need More Info? If you need more information on how to describe your model or understand the results, feel free to ask us on the community Slack workspace .","title":"Stats()"},{"location":"mongo/stats/#getting-the-statistics","text":"","title":"Getting the Statistics"},{"location":"mongo/stats/#the-stats-method","text":"The stats() method is used to display the attributes of an existing model. It accepts the {scale: \"attribute\"} object as an argument. Here is how to call the stats() method: db . predictor_name . stats ( { scale : \"attribute\" } ); Where: Name Description predictor_name The name of the predictor whose statistics you want to see. {scale: \"attribute\"} The argument of the stats() method defines the type of statistics ( {scale: \"features\"} , or {scale: \"model\"} , or {scale: \"ensemble\"} ).","title":"The stats() Method"},{"location":"mongo/stats/#the-stats-method-with-the-scale-features-parameter","text":"","title":"The stats() Method with the {scale: \"features\"} Parameter"},{"location":"mongo/stats/#description","text":"The db.predictor_name.stats({scale: \"features\"}) method is used to display the way the model encoded the data before training.","title":"Description"},{"location":"mongo/stats/#syntax","text":"Here is the syntax: db . predictor_name . stats ( { scale : \"features\" } ); On execution, we get: { \"data\" :[ { \"column\" : \"number_of_rooms\" , \"type\" : \"categorical\" , \"encoder\" : \"OneHotEncoder\" , \"role\" : \"feature\" } ] } Where: Name Description \"column\" The name of the column. \"type\" Type of the inferred data. \"encoder\" Encoder used. \"role\" Role of the column ( feature or target ).","title":"Syntax"},{"location":"mongo/stats/#example","text":"Let's describe the home_rentals_model model. db . home_rentals_model . stats ( { scale : \"features\" } ); On execution, we get: { \"data\" : [ { \"column\" : \"number_of_rooms\" , \"type\" : \"categorical\" , \"encoder\" : \"OneHotEncoder\" , \"role\" : \"feature\" }, { \"column\" : \"number_of_bathrooms\" , \"type\" : \"binary\" , \"encoder\" : \"BinaryEncoder\" , \"role\" : \"feature\" }, { \"column\" : \"sqft\" , \"type\" : \"float\" , \"encoder\" : \"NumericEncoder\" , \"role\" : \"feature\" }, { \"column\" : \"location\" , \"type\" : \"categorical\" , \"encoder\" : \"OneHotEncoder\" , \"role\" : \"feature\" }, { \"column\" : \"days_on_market\" , \"type\" : \"integer\" , \"encoder\" : \"NumericEncoder\" , \"role\" : \"feature\" }, { \"column\" : \"initial_price\" , \"type\" : \"integer\" , \"encoder\" : \"NumericEncoder\" , \"role\" : \"feature\" }, { \"column\" : \"neighborhood\" , \"type\" : \"categorical\" , \"encoder\" : \"OneHotEncoder\" , \"role\" : \"feature\" }, { \"column\" : \"rental_price\" , \"type\" : \"float\" , \"encoder\" : \"NumericEncoder\" , \"role\" : \"target\" } ], \"ns\" : \"mindsdb.home_rentals_model\" }","title":"Example"},{"location":"mongo/stats/#the-stats-method-with-the-scale-model-parameter","text":"","title":"The stats() Method with the {scale: \"model\"} Parameter"},{"location":"mongo/stats/#description_1","text":"The db.predictor_name.stats({scale: \"model\"}) method is used to display the performance of the candidate models.","title":"Description"},{"location":"mongo/stats/#syntax_1","text":"Here is the syntax: db . predictor_name . stats ( { scale : \"model\" } ); On execution, we get: { \"data\" : [ { \"name\" : \"candidate_model\" , \"performance\" : < 0.0 | 1.0 > , \"training_time\" : <seco n ds> , \"selected\" : < 0 | 1 > } ] } Where: Name Description \"name\" Name of the candidate model. \"performance\" Accuracy from 0 to 1 depending on the type of the model. \"training_time\" Time elapsed for the training of the model. \"selected\" 1 for the best performing model and 0 for the rest.","title":"Syntax"},{"location":"mongo/stats/#example_1","text":"Let's see the output for the home_rentals_model model. db . home_rentals_model . stats ( { scale : \"model\" } ); On execution, we get: { \"data\" : [ { \"name\" : \"Neural\" , \"performance\" : 0.999 , \"training_time\" : 48.37 , \"selected\" : 0 }, { \"name\" : \"LightGBM\" , \"performance\" : 1 , \"training_time\" : 33 , \"selected\" : 1 }, { \"name\" : \"Regression\" , \"performance\" : 0.999 , \"training_time\" : 0.05 , \"selected\" : 0 } ], \"ns\" : \"mindsdb.home_rentals_model\" }","title":"Example"},{"location":"mongo/stats/#the-stats-method-with-the-scale-ensemble-parameter","text":"","title":"The stats() Method with the {scale: \"ensemble\"} Parameter"},{"location":"mongo/stats/#description_2","text":"The db.predictor_name.stats({scale: \"ensemble\"}) method is used to display the parameters used to select the best candidate model.","title":"Description"},{"location":"mongo/stats/#syntax_2","text":"Here is the syntax: db . predictor_name . stats ( { scale : \"ensemble\" } ); On execution, we get: + -----------------+ | ensemble | + -----------------+ | { JSON } | + -----------------+ Where: Name Description ensemble Object of the JSON type describing the parameters used to select the best candidate model.","title":"Syntax"},{"location":"mongo/stats/#example_2","text":"Example WIP This example is a work in progress. Need More Info? If you need more information on how to describe your model or understand the results, feel free to ask us on the community Slack workspace .","title":"Example"},{"location":"rest/sql/","text":"/ api / sql / query Endpoint \u00b6 Description \u00b6 This API provides a REST endpoint for executing the SQL queries. Note: This endpoint is a HTTP POST method. This endpoint accept data via application/json request body. The only required key is the query which has the SQL statement value. Syntax \u00b6 POST h tt p : //{{url}}}/api/sql/query { \"query\" : \"The SQL Query you want to execute\" } On execution, we get Status 200 OK : { \"context\" : {}, \"type\" : \"ok\" } Example \u00b6 MindsDB Cloud Local MindsDB POST h tt ps : //cloud.mindsdb.com/api/sql/query { \"query\" : \"SELECT sqft, rental_price FROM example_db.demo_data.home_rentals LIMIT 10;\" } On execution, we get: { \"column_names\" : [ \"sqft\" , \"rental_price\" ], \"context\" : { \"db\" : \"mindsdb\" }, \"data\" : [ [ 917 , 3901 ], [ 194 , 2042 ] ], \"type\" : \"table\" } POST h tt p : //127.0.0.1:47334/api/sql/query { \"query\" : \"SELECT sqft, rental_price FROM example_db.demo_data.home_rentals LIMIT 10;\" } On execution, we get: { \"column_names\" : [ \"sqft\" , \"rental_price\" ], \"context\" : { \"db\" : \"mindsdb\" }, \"data\" : [ [ 917 , 3901 ], [ 194 , 2042 ] ], \"type\" : \"table\" }","title":"SQL Queries"},{"location":"rest/sql/#apisqlquery-endpoint","text":"","title":"/api/sql/query Endpoint"},{"location":"rest/sql/#description","text":"This API provides a REST endpoint for executing the SQL queries. Note: This endpoint is a HTTP POST method. This endpoint accept data via application/json request body. The only required key is the query which has the SQL statement value.","title":"Description"},{"location":"rest/sql/#syntax","text":"POST h tt p : //{{url}}}/api/sql/query { \"query\" : \"The SQL Query you want to execute\" } On execution, we get Status 200 OK : { \"context\" : {}, \"type\" : \"ok\" }","title":"Syntax"},{"location":"rest/sql/#example","text":"MindsDB Cloud Local MindsDB POST h tt ps : //cloud.mindsdb.com/api/sql/query { \"query\" : \"SELECT sqft, rental_price FROM example_db.demo_data.home_rentals LIMIT 10;\" } On execution, we get: { \"column_names\" : [ \"sqft\" , \"rental_price\" ], \"context\" : { \"db\" : \"mindsdb\" }, \"data\" : [ [ 917 , 3901 ], [ 194 , 2042 ] ], \"type\" : \"table\" } POST h tt p : //127.0.0.1:47334/api/sql/query { \"query\" : \"SELECT sqft, rental_price FROM example_db.demo_data.home_rentals LIMIT 10;\" } On execution, we get: { \"column_names\" : [ \"sqft\" , \"rental_price\" ], \"context\" : { \"db\" : \"mindsdb\" }, \"data\" : [ [ 917 , 3901 ], [ 194 , 2042 ] ], \"type\" : \"table\" }","title":"Example"},{"location":"setup/civo/","text":"Documentation WIP Note: This documentation is work in progress, for now you can follow up to Deploying MindsDB on a Civo Compute tutorial.","title":"Civo Cloud"},{"location":"setup/cloud/","text":"MindsDB Cloud \u00b6 MindsDB Cloud is a service hosted by MindsDB. It contains all of the latest updates and provides a handy SQL editor so you can run your queries right away. You can sign up for a free account at MindsDB Cloud here . Follow the steps below that guide you through the sign-up process. Create a Free Account at MindsDB Cloud \u00b6 Follow this link and fill out the sign-up form, as below. MindsDB Cloud Terms and Conditions You can view it here . Log In to the MindsDB Cloud \u00b6 Follow this link and input your credentials, as below. Email Validation \u00b6 After you sign up for a free MindsDB Cloud account, we'll send you a confirmation email. Click on the Verify Email button to validate your account. Use MindsDB Cloud \u00b6 Now, you are ready to use MindsDB Cloud. What's next? We recommend you follow one of our tutorials or learn more about the MindsDB Database .","title":"MindsDB Cloud"},{"location":"setup/cloud/#mindsdb-cloud","text":"MindsDB Cloud is a service hosted by MindsDB. It contains all of the latest updates and provides a handy SQL editor so you can run your queries right away. You can sign up for a free account at MindsDB Cloud here . Follow the steps below that guide you through the sign-up process.","title":"MindsDB Cloud"},{"location":"setup/cloud/#create-a-free-account-at-mindsdb-cloud","text":"Follow this link and fill out the sign-up form, as below. MindsDB Cloud Terms and Conditions You can view it here .","title":"Create a Free Account at MindsDB Cloud"},{"location":"setup/cloud/#log-in-to-the-mindsdb-cloud","text":"Follow this link and input your credentials, as below.","title":"Log In to the MindsDB Cloud"},{"location":"setup/cloud/#email-validation","text":"After you sign up for a free MindsDB Cloud account, we'll send you a confirmation email. Click on the Verify Email button to validate your account.","title":"Email Validation"},{"location":"setup/cloud/#use-mindsdb-cloud","text":"Now, you are ready to use MindsDB Cloud. What's next? We recommend you follow one of our tutorials or learn more about the MindsDB Database .","title":"Use MindsDB Cloud"},{"location":"setup/custom-config/","text":"Extend the Default MindsDB Configuration \u00b6 To follow this guide, please make sure you have a local installation of MindsDB. Here , you can find out how to install MindsDB locally. Starting MindsDB with Default Configuration \u00b6 It is very straightforward to start MindsDB locally with the default config file - just run the commands below. First, activate the virtual environment with this command: source mindsdb/bin/activate And then, start MindsDB using this command: python -m mindsdb Now you can access your MindsDB locally at 127.0.0.1:47334 . Starting MindsDB with Extended Configuration \u00b6 First, you should prepare a config.json file based on the config file from the MindsDB GitHub repository . Or you can use the template below and substitute your custom configuration values. { \"permanent_storage\" : { \"location\" : \"local\" } , \"paths\" : {} , \"log\" : { \"level\" : { \"console\" : \"INFO\" , \"file\" : \"DEBUG\" , \"db\" : \"WARNING\" } } , \"debug\" : false, \"integrations\" : {} , \"api\" : { \"http\" : { \"host\" : \"127.0.0.1\" , \"port\" : \"47334\" } , \"mysql\" : { \"host\" : \"127.0.0.1\" , \"password\" : \"\" , \"port\" : \"47335\" , \"user\" : \"mindsdb\" , \"database\" : \"mindsdb\" , \"ssl\" : true } , \"mongodb\" : { \"host\" : \"127.0.0.1\" , \"port\" : \"47336\" , \"database\" : \"mindsdb\" } } , \"cache\" : { \"type\" : \"local\" } } Now that your config.json file is ready, run the command below to start MindsDB locally with your custom configuration. python -m mindsdb --config = /path-to-the-extended-config-file/config-file.json You can access your MindsDB locally at 127.0.0.1:47334 , or any other IP address and port combination if you altered them. What's next? We recommend you follow one of our tutorials or learn more about the MindsDB Database .","title":"Extend the Default MindsDB Configuration"},{"location":"setup/custom-config/#extend-the-default-mindsdb-configuration","text":"To follow this guide, please make sure you have a local installation of MindsDB. Here , you can find out how to install MindsDB locally.","title":"Extend the Default MindsDB Configuration"},{"location":"setup/custom-config/#starting-mindsdb-with-default-configuration","text":"It is very straightforward to start MindsDB locally with the default config file - just run the commands below. First, activate the virtual environment with this command: source mindsdb/bin/activate And then, start MindsDB using this command: python -m mindsdb Now you can access your MindsDB locally at 127.0.0.1:47334 .","title":"Starting MindsDB with Default Configuration"},{"location":"setup/custom-config/#starting-mindsdb-with-extended-configuration","text":"First, you should prepare a config.json file based on the config file from the MindsDB GitHub repository . Or you can use the template below and substitute your custom configuration values. { \"permanent_storage\" : { \"location\" : \"local\" } , \"paths\" : {} , \"log\" : { \"level\" : { \"console\" : \"INFO\" , \"file\" : \"DEBUG\" , \"db\" : \"WARNING\" } } , \"debug\" : false, \"integrations\" : {} , \"api\" : { \"http\" : { \"host\" : \"127.0.0.1\" , \"port\" : \"47334\" } , \"mysql\" : { \"host\" : \"127.0.0.1\" , \"password\" : \"\" , \"port\" : \"47335\" , \"user\" : \"mindsdb\" , \"database\" : \"mindsdb\" , \"ssl\" : true } , \"mongodb\" : { \"host\" : \"127.0.0.1\" , \"port\" : \"47336\" , \"database\" : \"mindsdb\" } } , \"cache\" : { \"type\" : \"local\" } } Now that your config.json file is ready, run the command below to start MindsDB locally with your custom configuration. python -m mindsdb --config = /path-to-the-extended-config-file/config-file.json You can access your MindsDB locally at 127.0.0.1:47334 , or any other IP address and port combination if you altered them. What's next? We recommend you follow one of our tutorials or learn more about the MindsDB Database .","title":"Starting MindsDB with Extended Configuration"},{"location":"setup/digital-ocean/","text":"Documentation WIP Note: This documentation is work in progress, for now you can follow up to Deploying MindsDB on a Digital Ocean Droplet tutorial.","title":"Digital Ocean"},{"location":"setup/gcp/","text":"Documentation WIP Note: This documentation is work in progress, for now you can follow up to Deploying MindsDB on Google Cloud Platform tutorial.","title":"Google Cloud Platform"},{"location":"setup/vultr/","text":"Documentation WIP Note: This documentation is work in progress, for now you can follow up to Deploying MindsDB on Vultr Cloud Instance tutorial.","title":"Vultr Cloud"},{"location":"setup/self-hosted/docker/","text":"Setup for Docker \u00b6 Install Docker \u00b6 If you haven't done that already, install Docker on your machine following the instructions . To make sure Docker is successfully installed on your machine, run a test container as follows: docker run hello-world You should see the Hello from Docker! message. Otherwise, check the Docker's Get Started documentation. Docker for Mac users - RAM allocation issues By default, Docker for Mac allocates 2 GB of RAM, which is insufficient for deploying MindsDB with Docker. We recommend increasing the default RAM limit to 4 GB. Please refer to the Docker Desktop for Mac users manual for more information on how to increase the allocated memory. Start MindsDB \u00b6 Run the command below to start MindsDB in Docker. docker run -p 47334 :47334 -p 47335 :47335 mindsdb/mindsdb Let's analyze this command part by part: docker run is a native Docker command used to start a container -p 47334 :47334 publishes port 47334 to access MindsDB GUI and HTTP API -p 47335 :47335 publishes port 47335 to access MindsDB MySQL API mindsdb/mindsdb is the container we want to start Default Configuration \u00b6 The default configuration for MindsDB's Docker image is stored as a JSON code, as below. { \"config_version\" : \"1.4\" , \"storage_dir\" : \"/root/mdb_storage\" , \"log\" : { \"level\" : { \"console\" : \"ERROR\" , \"file\" : \"WARNING\" , \"db\" : \"WARNING\" } }, \"debug\" : false , \"integrations\" : {}, \"api\" : { \"http\" : { \"host\" : \"0.0.0.0\" , \"port\" : \"47334\" }, \"mysql\" : { \"host\" : \"0.0.0.0\" , \"password\" : \"\" , \"port\" : \"47335\" , \"user\" : \"mindsdb\" , \"database\" : \"mindsdb\" , \"ssl\" : true }, \"mongodb\" : { \"host\" : \"0.0.0.0\" , \"port\" : \"47336\" , \"database\" : \"mindsdb\" } } } Custom Configuration \u00b6 To override the default configuration, you can provide values via the MDB_CONFIG_CONTENT environment variable, as below. docker run -e MDB_CONFIG_CONTENT = '{\"api\":{\"http\": {\"host\": \"0.0.0.0\",\"port\": \"8080\"}}}' mindsdb/mindsdb Known Issues \u00b6 #1 \u00b6 If you experience any issues related to MKL or your training process does not complete, please add the MKL_SERVICE_FORCE_INTEL environment variable, as below. docker run -e MKL_SERVICE_FORCE_INTEL = 1 -p 47334 :47334 -p 47335 :47335 mindsdb/mindsdb What's next? We recommend you follow one of our tutorials or learn more about the MindsDB Database .","title":"Docker"},{"location":"setup/self-hosted/docker/#setup-for-docker","text":"","title":"Setup for Docker"},{"location":"setup/self-hosted/docker/#install-docker","text":"If you haven't done that already, install Docker on your machine following the instructions . To make sure Docker is successfully installed on your machine, run a test container as follows: docker run hello-world You should see the Hello from Docker! message. Otherwise, check the Docker's Get Started documentation. Docker for Mac users - RAM allocation issues By default, Docker for Mac allocates 2 GB of RAM, which is insufficient for deploying MindsDB with Docker. We recommend increasing the default RAM limit to 4 GB. Please refer to the Docker Desktop for Mac users manual for more information on how to increase the allocated memory.","title":"Install Docker"},{"location":"setup/self-hosted/docker/#start-mindsdb","text":"Run the command below to start MindsDB in Docker. docker run -p 47334 :47334 -p 47335 :47335 mindsdb/mindsdb Let's analyze this command part by part: docker run is a native Docker command used to start a container -p 47334 :47334 publishes port 47334 to access MindsDB GUI and HTTP API -p 47335 :47335 publishes port 47335 to access MindsDB MySQL API mindsdb/mindsdb is the container we want to start","title":"Start MindsDB"},{"location":"setup/self-hosted/docker/#default-configuration","text":"The default configuration for MindsDB's Docker image is stored as a JSON code, as below. { \"config_version\" : \"1.4\" , \"storage_dir\" : \"/root/mdb_storage\" , \"log\" : { \"level\" : { \"console\" : \"ERROR\" , \"file\" : \"WARNING\" , \"db\" : \"WARNING\" } }, \"debug\" : false , \"integrations\" : {}, \"api\" : { \"http\" : { \"host\" : \"0.0.0.0\" , \"port\" : \"47334\" }, \"mysql\" : { \"host\" : \"0.0.0.0\" , \"password\" : \"\" , \"port\" : \"47335\" , \"user\" : \"mindsdb\" , \"database\" : \"mindsdb\" , \"ssl\" : true }, \"mongodb\" : { \"host\" : \"0.0.0.0\" , \"port\" : \"47336\" , \"database\" : \"mindsdb\" } } }","title":"Default Configuration"},{"location":"setup/self-hosted/docker/#custom-configuration","text":"To override the default configuration, you can provide values via the MDB_CONFIG_CONTENT environment variable, as below. docker run -e MDB_CONFIG_CONTENT = '{\"api\":{\"http\": {\"host\": \"0.0.0.0\",\"port\": \"8080\"}}}' mindsdb/mindsdb","title":"Custom Configuration"},{"location":"setup/self-hosted/docker/#known-issues","text":"","title":"Known Issues"},{"location":"setup/self-hosted/docker/#1","text":"If you experience any issues related to MKL or your training process does not complete, please add the MKL_SERVICE_FORCE_INTEL environment variable, as below. docker run -e MKL_SERVICE_FORCE_INTEL = 1 -p 47334 :47334 -p 47335 :47335 mindsdb/mindsdb What's next? We recommend you follow one of our tutorials or learn more about the MindsDB Database .","title":"#1"},{"location":"setup/self-hosted/pip/linux/","text":"Setup for Linux via pip \u00b6 Before You Start \u00b6 There are some points that you should consider before jumping into the installation. Please have a look at them below. Pip and Python Versions \u00b6 Due to some of our dependencies having issues with the latest versions of Python 3.9.x, we suggest using Python 3.7.x or 3.8.x versions for now. We are working on Python 3.9.x to be supported soon. To successfully install MindsDB, use Python 64-bit version . Also, make sure that Python >= 3.7 and pip >= 19.3 . You can check the pip and python versions by running the pip --version and python --version commands. Please note that depending on your environment and installed pip and python packages, you might have to use pip3 instead of pip or python3.x instead of py . For example, pip3 install mindsdb instead of pip install mindsdb . How to Avoid Dependency Issues \u00b6 Install MindsDB in a virtual environment using pip to avoid dependency issues. Or you could try to install MindsDB with Anaconda and run the installation from the Anaconda prompt . How to Avoid Common Errors \u00b6 MindsDB requires around 3 GB of free disk space to install all of its dependencies. Make sure to allocate min. 3 GB of disk space to avoid the IOError: [Errno 28] No space left on device while installing MindsDB error. Before anything, activate your virtual environment where your MindsDB is installed. It is to avoid the No module named mindsdb error. Using the Python venv Module \u00b6 Create a new virtual environment called mindsdb : python -m venv mindsdb Now, activate it: source mindsdb/bin/activate Once inside the virtual environment, run the command below to mitigate the dependency issues: pip install --upgrade pip setuptools wheel Install MindsDB: pip install mindsdb Verify MindsDB installation: pip freeze You should see a list of installed packages including but not limited to the following: ... alembic == 1 .7.7 aniso8601 == 9 .0.1 appdirs == 1 .4.4 lightgbm == 3 .3.0 lightwood == 22 .4.1.0 MindsDB == 22 .4.5.0 mindsdb-datasources == 1 .8.2 mindsdb-sql == 0 .3.3 mindsdb-streams == 0 .0.5 ... Using Anaconda \u00b6 Here, you need either Anaconda or Conda installed on your machine. Open Anaconda prompt and create a new virtual environment: conda create -n mindsdb Now, activate it: conda activate mindsdb Once inside the virtual environment, run the command below to mitigate the dependency issues: pip install --upgrade pip setuptools wheel Install MindsDB: pip install mindsdb Verify MindsDB installation: conda list You should see a list of installed packages including but not limited to the following: ... alembic == 1 .7.7 aniso8601 == 9 .0.1 appdirs == 1 .4.4 lightgbm == 3 .3.0 lightwood == 22 .4.1.0 MindsDB == 22 .4.5.0 mindsdb-datasources == 1 .8.2 mindsdb-sql == 0 .3.3 mindsdb-streams == 0 .0.5 ... Further Issues? \u00b6 You can try to replicate your issue using the Docker setup . Also, please create an issue with detailed description in the MindsDB GitHub repository so we can help you. Usually, we review issues and respond within a few hours. What's next? We recommend you follow one of our tutorials or learn more about the MindsDB Database .","title":"Linux via pip"},{"location":"setup/self-hosted/pip/linux/#setup-for-linux-via-pip","text":"","title":"Setup for Linux via pip"},{"location":"setup/self-hosted/pip/linux/#before-you-start","text":"There are some points that you should consider before jumping into the installation. Please have a look at them below.","title":"Before You Start"},{"location":"setup/self-hosted/pip/linux/#pip-and-python-versions","text":"Due to some of our dependencies having issues with the latest versions of Python 3.9.x, we suggest using Python 3.7.x or 3.8.x versions for now. We are working on Python 3.9.x to be supported soon. To successfully install MindsDB, use Python 64-bit version . Also, make sure that Python >= 3.7 and pip >= 19.3 . You can check the pip and python versions by running the pip --version and python --version commands. Please note that depending on your environment and installed pip and python packages, you might have to use pip3 instead of pip or python3.x instead of py . For example, pip3 install mindsdb instead of pip install mindsdb .","title":"Pip and Python Versions"},{"location":"setup/self-hosted/pip/linux/#how-to-avoid-dependency-issues","text":"Install MindsDB in a virtual environment using pip to avoid dependency issues. Or you could try to install MindsDB with Anaconda and run the installation from the Anaconda prompt .","title":"How to Avoid Dependency Issues"},{"location":"setup/self-hosted/pip/linux/#how-to-avoid-common-errors","text":"MindsDB requires around 3 GB of free disk space to install all of its dependencies. Make sure to allocate min. 3 GB of disk space to avoid the IOError: [Errno 28] No space left on device while installing MindsDB error. Before anything, activate your virtual environment where your MindsDB is installed. It is to avoid the No module named mindsdb error.","title":"How to Avoid Common Errors"},{"location":"setup/self-hosted/pip/linux/#using-the-python-venv-module","text":"Create a new virtual environment called mindsdb : python -m venv mindsdb Now, activate it: source mindsdb/bin/activate Once inside the virtual environment, run the command below to mitigate the dependency issues: pip install --upgrade pip setuptools wheel Install MindsDB: pip install mindsdb Verify MindsDB installation: pip freeze You should see a list of installed packages including but not limited to the following: ... alembic == 1 .7.7 aniso8601 == 9 .0.1 appdirs == 1 .4.4 lightgbm == 3 .3.0 lightwood == 22 .4.1.0 MindsDB == 22 .4.5.0 mindsdb-datasources == 1 .8.2 mindsdb-sql == 0 .3.3 mindsdb-streams == 0 .0.5 ...","title":"Using the Python venv Module"},{"location":"setup/self-hosted/pip/linux/#using-anaconda","text":"Here, you need either Anaconda or Conda installed on your machine. Open Anaconda prompt and create a new virtual environment: conda create -n mindsdb Now, activate it: conda activate mindsdb Once inside the virtual environment, run the command below to mitigate the dependency issues: pip install --upgrade pip setuptools wheel Install MindsDB: pip install mindsdb Verify MindsDB installation: conda list You should see a list of installed packages including but not limited to the following: ... alembic == 1 .7.7 aniso8601 == 9 .0.1 appdirs == 1 .4.4 lightgbm == 3 .3.0 lightwood == 22 .4.1.0 MindsDB == 22 .4.5.0 mindsdb-datasources == 1 .8.2 mindsdb-sql == 0 .3.3 mindsdb-streams == 0 .0.5 ...","title":"Using Anaconda"},{"location":"setup/self-hosted/pip/linux/#further-issues","text":"You can try to replicate your issue using the Docker setup . Also, please create an issue with detailed description in the MindsDB GitHub repository so we can help you. Usually, we review issues and respond within a few hours. What's next? We recommend you follow one of our tutorials or learn more about the MindsDB Database .","title":"Further Issues?"},{"location":"setup/self-hosted/pip/macos/","text":"Setup for MacOS via pip \u00b6 Before You Start \u00b6 There are some points that you should consider before jumping into the installation. Please have a look at them below. Pip and Python Versions \u00b6 Due to some of our dependencies having issues with the latest versions of Python 3.9.x, we suggest using Python 3.7.x or 3.8.x versions for now. We are working on Python 3.9.x to be supported soon. To successfully install MindsDB, use Python 64-bit version . Also, make sure that Python >= 3.7 and pip >= 19.3 . You can check the pip and python versions by running the pip --version and python --version commands. Please note that depending on your environment and installed pip and python packages, you might have to use pip3 instead of pip or python3.x instead of py . For example, pip3 install mindsdb instead of pip install mindsdb . How to Avoid Dependency Issues \u00b6 Install MindsDB in a virtual environment using pip to avoid dependency issues. Or you could try to install MindsDB with Anaconda and run the installation from the Anaconda prompt . How to Avoid Common Errors \u00b6 MindsDB requires around 3 GB of free disk space to install all of its dependencies. Make sure to allocate min. 3 GB of disk space to avoid the IOError: [Errno 28] No space left on device while installing MindsDB error. Before anything, activate your virtual environment where your MindsDB is installed. It is to avoid the No module named mindsdb error. The numpy.distutils.system_info.NotFoundError: No lapack/blas resources found. Note: Accelerate is no longer supported. error might be caused by some dependencies not working with Python 3.9 version. For now, please use Python 3.7.x or Python 3.8.x, as mentioned before. Using the Python venv Module \u00b6 Create a new virtual environment called mindsdb : python -m venv mindsdb Now, activate it: source mindsdb/bin/activate Once inside the virtual environment, run the command below to mitigate the dependency issues: pip install --upgrade pip setuptools wheel Install MindsDB: pip install mindsdb Verify MindsDB installation: pip freeze You should see a list of installed packages including but not limited to the following: ... alembic == 1 .7.7 aniso8601 == 9 .0.1 appdirs == 1 .4.4 lightgbm == 3 .3.0 lightwood == 22 .4.1.0 MindsDB == 22 .4.5.0 mindsdb-datasources == 1 .8.2 mindsdb-sql == 0 .3.3 mindsdb-streams == 0 .0.5 ... Using Anaconda \u00b6 Here, you need either Anaconda or Conda installed on your machine. Open Anaconda prompt and create a new virtual environment: conda create -n mindsdb Now, activate it: conda activate mindsdb Once inside the virtual environment, run the command below to mitigate the dependency issues: pip install --upgrade pip setuptools wheel Install MindsDB: pip install mindsdb Verify MindsDB installation: conda list You should see a list of installed packages including but not limited to the following: ... alembic == 1 .7.7 aniso8601 == 9 .0.1 appdirs == 1 .4.4 lightgbm == 3 .3.0 lightwood == 22 .4.1.0 MindsDB == 22 .4.5.0 mindsdb-datasources == 1 .8.2 mindsdb-sql == 0 .3.3 mindsdb-streams == 0 .0.5 ... Further Issues? \u00b6 You can try to replicate your issue using the Docker setup . Also, please create an issue with detailed description in the MindsDB GitHub repository so we can help you. Usually, we review issues and respond within a few hours. What's next? We recommend you follow one of our tutorials or learn more about the MindsDB Database .","title":"MacOS via pip"},{"location":"setup/self-hosted/pip/macos/#setup-for-macos-via-pip","text":"","title":"Setup for MacOS via pip"},{"location":"setup/self-hosted/pip/macos/#before-you-start","text":"There are some points that you should consider before jumping into the installation. Please have a look at them below.","title":"Before You Start"},{"location":"setup/self-hosted/pip/macos/#pip-and-python-versions","text":"Due to some of our dependencies having issues with the latest versions of Python 3.9.x, we suggest using Python 3.7.x or 3.8.x versions for now. We are working on Python 3.9.x to be supported soon. To successfully install MindsDB, use Python 64-bit version . Also, make sure that Python >= 3.7 and pip >= 19.3 . You can check the pip and python versions by running the pip --version and python --version commands. Please note that depending on your environment and installed pip and python packages, you might have to use pip3 instead of pip or python3.x instead of py . For example, pip3 install mindsdb instead of pip install mindsdb .","title":"Pip and Python Versions"},{"location":"setup/self-hosted/pip/macos/#how-to-avoid-dependency-issues","text":"Install MindsDB in a virtual environment using pip to avoid dependency issues. Or you could try to install MindsDB with Anaconda and run the installation from the Anaconda prompt .","title":"How to Avoid Dependency Issues"},{"location":"setup/self-hosted/pip/macos/#how-to-avoid-common-errors","text":"MindsDB requires around 3 GB of free disk space to install all of its dependencies. Make sure to allocate min. 3 GB of disk space to avoid the IOError: [Errno 28] No space left on device while installing MindsDB error. Before anything, activate your virtual environment where your MindsDB is installed. It is to avoid the No module named mindsdb error. The numpy.distutils.system_info.NotFoundError: No lapack/blas resources found. Note: Accelerate is no longer supported. error might be caused by some dependencies not working with Python 3.9 version. For now, please use Python 3.7.x or Python 3.8.x, as mentioned before.","title":"How to Avoid Common Errors"},{"location":"setup/self-hosted/pip/macos/#using-the-python-venv-module","text":"Create a new virtual environment called mindsdb : python -m venv mindsdb Now, activate it: source mindsdb/bin/activate Once inside the virtual environment, run the command below to mitigate the dependency issues: pip install --upgrade pip setuptools wheel Install MindsDB: pip install mindsdb Verify MindsDB installation: pip freeze You should see a list of installed packages including but not limited to the following: ... alembic == 1 .7.7 aniso8601 == 9 .0.1 appdirs == 1 .4.4 lightgbm == 3 .3.0 lightwood == 22 .4.1.0 MindsDB == 22 .4.5.0 mindsdb-datasources == 1 .8.2 mindsdb-sql == 0 .3.3 mindsdb-streams == 0 .0.5 ...","title":"Using the Python venv Module"},{"location":"setup/self-hosted/pip/macos/#using-anaconda","text":"Here, you need either Anaconda or Conda installed on your machine. Open Anaconda prompt and create a new virtual environment: conda create -n mindsdb Now, activate it: conda activate mindsdb Once inside the virtual environment, run the command below to mitigate the dependency issues: pip install --upgrade pip setuptools wheel Install MindsDB: pip install mindsdb Verify MindsDB installation: conda list You should see a list of installed packages including but not limited to the following: ... alembic == 1 .7.7 aniso8601 == 9 .0.1 appdirs == 1 .4.4 lightgbm == 3 .3.0 lightwood == 22 .4.1.0 MindsDB == 22 .4.5.0 mindsdb-datasources == 1 .8.2 mindsdb-sql == 0 .3.3 mindsdb-streams == 0 .0.5 ...","title":"Using Anaconda"},{"location":"setup/self-hosted/pip/macos/#further-issues","text":"You can try to replicate your issue using the Docker setup . Also, please create an issue with detailed description in the MindsDB GitHub repository so we can help you. Usually, we review issues and respond within a few hours. What's next? We recommend you follow one of our tutorials or learn more about the MindsDB Database .","title":"Further Issues?"},{"location":"setup/self-hosted/pip/source/","text":"Setup for Sourcecode via pip \u00b6 This section describes how to deploy MindsDB from the source code. It is the preferred way to use MindsDB if you want to contribute to our code or debug MindsDB. Before You Start \u00b6 There are some points that you should consider before jumping into the installation. Please have a look at them below. Pip and Python Versions \u00b6 Due to some of our dependencies having issues with the latest versions of Python 3.9.x, we suggest using Python 3.7.x or 3.8.x versions for now. We are working on Python 3.9.x to be supported soon. To successfully install MindsDB, use Python 64-bit version . Also, make sure that Python >= 3.7 and pip >= 19.3 . You can check the pip and python versions by running the pip --version and python --version commands. Please note that depending on your environment and installed pip and python packages, you might have to use pip3 instead of pip or python3.x instead of py . For example, pip3 install mindsdb instead of pip install mindsdb . How to Avoid Dependency Issues \u00b6 Install MindsDB in a virtual environment using pip to avoid dependency issues. How to Avoid Common Errors \u00b6 MindsDB requires around 3 GB of free disk space to install all of its dependencies. Make sure to allocate min. 3 GB of disk space to avoid the IOError: [Errno 28] No space left on device while installing MindsDB error. Before anything, activate your virtual environment where your MindsDB is installed. It is to avoid the No module named mindsdb error. The ImportError: No module named {dependency name} error may occur if you skip step 3 of the Installation section. Make sure you install all of the prerequisites. If you encounter the This site can\u2019t be reached. 127.0.0.1 refused to connect. error, please check the MindsDB server console to see if the server is still in the starting phase. But if the server has started and you still get this error, please report it on our GitHub repository . Installation \u00b6 Clone the MindsDB repository: git clone git@github.com:mindsdb/mindsdb.git Create a new virtual environment called mindsdb-venv : python -m venv mindsdb-venv Now, activate it: source mindsdb-venv/bin/activate Install MindsDB prerequisites: cd mindsdb && pip install -r requirements.txt Install MindsDB: python setup.py develop Verify MindsDB installation by starting the MindsDB server: python -m mindsdb Now, you can access the following: MindsDB APIs MindsDB Studio MindsDB Studio using MySQL http://127.0.0.1:47334/api http://127.0.0.1:47334/ mysql -h 127 .0.0.1 --port 3306 -u mindsdb -p Further Issues? \u00b6 You can try to replicate your issue using the Docker setup . Also, please create an issue with detailed description in the MindsDB GitHub repository so we can help you. Usually, we review issues and respond within a few hours. What's next? We recommend you follow one of our tutorials or learn more about the MindsDB Database .","title":"Sourcecode via pip"},{"location":"setup/self-hosted/pip/source/#setup-for-sourcecode-via-pip","text":"This section describes how to deploy MindsDB from the source code. It is the preferred way to use MindsDB if you want to contribute to our code or debug MindsDB.","title":"Setup for Sourcecode via pip"},{"location":"setup/self-hosted/pip/source/#before-you-start","text":"There are some points that you should consider before jumping into the installation. Please have a look at them below.","title":"Before You Start"},{"location":"setup/self-hosted/pip/source/#pip-and-python-versions","text":"Due to some of our dependencies having issues with the latest versions of Python 3.9.x, we suggest using Python 3.7.x or 3.8.x versions for now. We are working on Python 3.9.x to be supported soon. To successfully install MindsDB, use Python 64-bit version . Also, make sure that Python >= 3.7 and pip >= 19.3 . You can check the pip and python versions by running the pip --version and python --version commands. Please note that depending on your environment and installed pip and python packages, you might have to use pip3 instead of pip or python3.x instead of py . For example, pip3 install mindsdb instead of pip install mindsdb .","title":"Pip and Python Versions"},{"location":"setup/self-hosted/pip/source/#how-to-avoid-dependency-issues","text":"Install MindsDB in a virtual environment using pip to avoid dependency issues.","title":"How to Avoid Dependency Issues"},{"location":"setup/self-hosted/pip/source/#how-to-avoid-common-errors","text":"MindsDB requires around 3 GB of free disk space to install all of its dependencies. Make sure to allocate min. 3 GB of disk space to avoid the IOError: [Errno 28] No space left on device while installing MindsDB error. Before anything, activate your virtual environment where your MindsDB is installed. It is to avoid the No module named mindsdb error. The ImportError: No module named {dependency name} error may occur if you skip step 3 of the Installation section. Make sure you install all of the prerequisites. If you encounter the This site can\u2019t be reached. 127.0.0.1 refused to connect. error, please check the MindsDB server console to see if the server is still in the starting phase. But if the server has started and you still get this error, please report it on our GitHub repository .","title":"How to Avoid Common Errors"},{"location":"setup/self-hosted/pip/source/#installation","text":"Clone the MindsDB repository: git clone git@github.com:mindsdb/mindsdb.git Create a new virtual environment called mindsdb-venv : python -m venv mindsdb-venv Now, activate it: source mindsdb-venv/bin/activate Install MindsDB prerequisites: cd mindsdb && pip install -r requirements.txt Install MindsDB: python setup.py develop Verify MindsDB installation by starting the MindsDB server: python -m mindsdb Now, you can access the following: MindsDB APIs MindsDB Studio MindsDB Studio using MySQL http://127.0.0.1:47334/api http://127.0.0.1:47334/ mysql -h 127 .0.0.1 --port 3306 -u mindsdb -p","title":"Installation"},{"location":"setup/self-hosted/pip/source/#further-issues","text":"You can try to replicate your issue using the Docker setup . Also, please create an issue with detailed description in the MindsDB GitHub repository so we can help you. Usually, we review issues and respond within a few hours. What's next? We recommend you follow one of our tutorials or learn more about the MindsDB Database .","title":"Further Issues?"},{"location":"setup/self-hosted/pip/windows/","text":"Setup for Windows via pip \u00b6 Before You Start \u00b6 There are some points that you should consider before jumping into the installation. Please have a look at them below. Pip and Python Versions \u00b6 Due to some of our dependencies having issues with the latest versions of Python 3.9.x, we suggest using Python 3.7.x or 3.8.x versions for now. We are working on Python 3.9.x to be supported soon. To successfully install MindsDB, use Python 64-bit version . Also, make sure that Python >= 3.7 and pip >= 19.3 . You can check the pip and python versions by running the pip --version and python --version commands. Please note that depending on your environment and installed pip and python packages, you might have to use pip3 instead of pip or python3.x instead of py . For example, pip3 install mindsdb instead of pip install mindsdb . How to Avoid Dependency Issues \u00b6 Install MindsDB in a virtual environment using pip to avoid dependency issues. Or you could try to install MindsDB with Anaconda and run the installation from the Anaconda prompt . Installing torch or torchvision \u00b6 If the installation fails when installing torch or torchvision , try to install them manually by following the instructions on their official website . Using the Python venv Module \u00b6 Create a new virtual environment called mindsdb : py -m venv mindsdb Now, activate it: .\\mindsdb\\Scripts\\activate.bat Once inside the virtual environment, run the command below to mitigate the dependency issues: pip install --upgrade pip setuptools wheel Install MindsDB: pip install mindsdb Verify MindsDB installation: pip freeze You should see a list of installed packages including but not limited to the following: ... alembic == 1 .7.7 aniso8601 == 9 .0.1 appdirs == 1 .4.4 lightgbm == 3 .3.0 lightwood == 22 .4.1.0 MindsDB == 22 .4.5.0 mindsdb-datasources == 1 .8.2 mindsdb-sql == 0 .3.3 mindsdb-streams == 0 .0.5 ... Using Anaconda \u00b6 Here, you need either Anaconda or Conda installed on your machine. Open Anaconda prompt and create a new virtual environment: conda create -n mindsdb Now, activate it: conda activate mindsdb Once inside the virtual environment, run the command below to mitigate the dependency issues: pip install --upgrade pip setuptools wheel Install MindsDB: pip install mindsdb Verify MindsDB installation: conda list You should see a list of installed packages including but not limited to the following: ... alembic == 1 .7.7 aniso8601 == 9 .0.1 appdirs == 1 .4.4 lightgbm == 3 .3.0 lightwood == 22 .4.1.0 MindsDB == 22 .4.5.0 mindsdb-datasources == 1 .8.2 mindsdb-sql == 0 .3.3 mindsdb-streams == 0 .0.5 ... Further Issues? \u00b6 You can try to replicate your issue using the Docker setup . Also, please create an issue with detailed description in the MindsDB GitHub repository so we can help you. Usually, we review issues and respond within a few hours. What's next? We recommend you follow one of our tutorials or learn more about the MindsDB Database .","title":"Windows via pip"},{"location":"setup/self-hosted/pip/windows/#setup-for-windows-via-pip","text":"","title":"Setup for Windows via pip"},{"location":"setup/self-hosted/pip/windows/#before-you-start","text":"There are some points that you should consider before jumping into the installation. Please have a look at them below.","title":"Before You Start"},{"location":"setup/self-hosted/pip/windows/#pip-and-python-versions","text":"Due to some of our dependencies having issues with the latest versions of Python 3.9.x, we suggest using Python 3.7.x or 3.8.x versions for now. We are working on Python 3.9.x to be supported soon. To successfully install MindsDB, use Python 64-bit version . Also, make sure that Python >= 3.7 and pip >= 19.3 . You can check the pip and python versions by running the pip --version and python --version commands. Please note that depending on your environment and installed pip and python packages, you might have to use pip3 instead of pip or python3.x instead of py . For example, pip3 install mindsdb instead of pip install mindsdb .","title":"Pip and Python Versions"},{"location":"setup/self-hosted/pip/windows/#how-to-avoid-dependency-issues","text":"Install MindsDB in a virtual environment using pip to avoid dependency issues. Or you could try to install MindsDB with Anaconda and run the installation from the Anaconda prompt .","title":"How to Avoid Dependency Issues"},{"location":"setup/self-hosted/pip/windows/#installing-torch-or-torchvision","text":"If the installation fails when installing torch or torchvision , try to install them manually by following the instructions on their official website .","title":"Installing torch or torchvision"},{"location":"setup/self-hosted/pip/windows/#using-the-python-venv-module","text":"Create a new virtual environment called mindsdb : py -m venv mindsdb Now, activate it: .\\mindsdb\\Scripts\\activate.bat Once inside the virtual environment, run the command below to mitigate the dependency issues: pip install --upgrade pip setuptools wheel Install MindsDB: pip install mindsdb Verify MindsDB installation: pip freeze You should see a list of installed packages including but not limited to the following: ... alembic == 1 .7.7 aniso8601 == 9 .0.1 appdirs == 1 .4.4 lightgbm == 3 .3.0 lightwood == 22 .4.1.0 MindsDB == 22 .4.5.0 mindsdb-datasources == 1 .8.2 mindsdb-sql == 0 .3.3 mindsdb-streams == 0 .0.5 ...","title":"Using the Python venv Module"},{"location":"setup/self-hosted/pip/windows/#using-anaconda","text":"Here, you need either Anaconda or Conda installed on your machine. Open Anaconda prompt and create a new virtual environment: conda create -n mindsdb Now, activate it: conda activate mindsdb Once inside the virtual environment, run the command below to mitigate the dependency issues: pip install --upgrade pip setuptools wheel Install MindsDB: pip install mindsdb Verify MindsDB installation: conda list You should see a list of installed packages including but not limited to the following: ... alembic == 1 .7.7 aniso8601 == 9 .0.1 appdirs == 1 .4.4 lightgbm == 3 .3.0 lightwood == 22 .4.1.0 MindsDB == 22 .4.5.0 mindsdb-datasources == 1 .8.2 mindsdb-sql == 0 .3.3 mindsdb-streams == 0 .0.5 ...","title":"Using Anaconda"},{"location":"setup/self-hosted/pip/windows/#further-issues","text":"You can try to replicate your issue using the Docker setup . Also, please create an issue with detailed description in the MindsDB GitHub repository so we can help you. Usually, we review issues and respond within a few hours. What's next? We recommend you follow one of our tutorials or learn more about the MindsDB Database .","title":"Further Issues?"},{"location":"sql/data-insights/","text":"MindsDB Data Insights \u00b6 Data Insights is a data visualization feature of the MindsDB Cloud editor. It lets you explore the queried data by initially displaying and analyzing a subset of the first ten rows. You can choose to analyze a full dataset by clicking the Full Data Analysis button. The analysis presents the distribution of your data aggregated by column. The data used here comes from one of our tutorials. For details, click here . Before you see the Data Insights pane, you must run a SELECT query on your dataset. Let's have a look at the available features. Features \u00b6 Distribution of Data per Column \u00b6 When opening the Data Insights pane, you see the distribution of data of each output dataset column. Initially, the visualization and analysis of the first ten rows is shown, as below. There is one histogram per column that depicts the column name, data types of the distribution, and the distribution itself. Potential Bias Flag \u00b6 To see the Potential Bias flag, enter a full-screen mode of the Data Insights pane. Here, the location column exhibits potential bias, as there are more great column values than good or poor column values. Such cases are typically flagged. However, it does not necessarily mean that there is a problem with the dataset. The Potential Bias flag is used when data does not distribute normally or uniformly, likely over-representing or under-representing some values. This may be normal, hence, bias is only potential. Missing Values Flag \u00b6 To see the Missing Values flag, enter a full-screen mode of the Data Insights pane. This flag indicates the proportion of missing values in a column. Columns with a high percentage of missing values are not useful for modeling purposes. Hence, it is recommended to pay attention to the Missing Values flag and try to mitigate it whenever possible, as it indicates the degrading quality of your data. Hovering Over the Histogram \u00b6 When hovering over the histogram, you get the information on a particular column value and how many of such values are present in a column. The format is (column_value, count) . It is helpful to determine the exact data value counts from the histograms. Full Data Analysis \u00b6 Let's do a full data analysis step by step. First, we need to query data for analysis in the MindsDB Cloud editor. Please note that you need to query your dataset without using a LIMIT keyword to be able to perform a complete data analysis. SELECT * FROM example_db . demo_data . home_rentals ; On execution, we get: + ---------------+-------------------+----+--------+--------------+--------------+------------+ | number_of_rooms | number_of_bathrooms | sqft | location | days_on_market | neighborhood | rental_price | + ---------------+-------------------+----+--------+--------------+--------------+------------+ | 2 | 1 | 917 | great | 13 | berkeley_hills | 3901 | | 0 | 1 | 194 | great | 10 | berkeley_hills | 2042 | | 1 | 1 | 543 | poor | 18 | westbrae | 1871 | | 2 | 1 | 503 | good | 10 | downtown | 3026 | | 3 | 2 | 1066 | good | 13 | thowsand_oaks | 4774 | + ---------------+-------------------+----+--------+--------------+--------------+------------+ Now, open the Data Insights pane by clicking the Data Insights button to the right of the output table. Initially, it shows the analysis of the first ten rows of the output table. To perform a complete analysis of your data, you can either go to a full-screen mode or stay in a pane mode and click on the Full Data Analysis button. Below is the complete data analysis. Also, whenever your dataset changes, you can click on the Refresh Data Analysis button to update the data visualization and analysis. What's Next? \u00b6 Want to do more exploratory data analysis in MindsDB? We\u2019re collecting feedback to develop even more data visualization features. Let us know what you'd like to see as part of Data Insights. Link to the feedback form The link to our feedback form will be shared soon.","title":"MindsDB Data Insights"},{"location":"sql/data-insights/#mindsdb-data-insights","text":"Data Insights is a data visualization feature of the MindsDB Cloud editor. It lets you explore the queried data by initially displaying and analyzing a subset of the first ten rows. You can choose to analyze a full dataset by clicking the Full Data Analysis button. The analysis presents the distribution of your data aggregated by column. The data used here comes from one of our tutorials. For details, click here . Before you see the Data Insights pane, you must run a SELECT query on your dataset. Let's have a look at the available features.","title":"MindsDB Data Insights"},{"location":"sql/data-insights/#features","text":"","title":"Features"},{"location":"sql/data-insights/#distribution-of-data-per-column","text":"When opening the Data Insights pane, you see the distribution of data of each output dataset column. Initially, the visualization and analysis of the first ten rows is shown, as below. There is one histogram per column that depicts the column name, data types of the distribution, and the distribution itself.","title":"Distribution of Data per Column"},{"location":"sql/data-insights/#potential-bias-flag","text":"To see the Potential Bias flag, enter a full-screen mode of the Data Insights pane. Here, the location column exhibits potential bias, as there are more great column values than good or poor column values. Such cases are typically flagged. However, it does not necessarily mean that there is a problem with the dataset. The Potential Bias flag is used when data does not distribute normally or uniformly, likely over-representing or under-representing some values. This may be normal, hence, bias is only potential.","title":"Potential Bias Flag"},{"location":"sql/data-insights/#missing-values-flag","text":"To see the Missing Values flag, enter a full-screen mode of the Data Insights pane. This flag indicates the proportion of missing values in a column. Columns with a high percentage of missing values are not useful for modeling purposes. Hence, it is recommended to pay attention to the Missing Values flag and try to mitigate it whenever possible, as it indicates the degrading quality of your data.","title":"Missing Values Flag"},{"location":"sql/data-insights/#hovering-over-the-histogram","text":"When hovering over the histogram, you get the information on a particular column value and how many of such values are present in a column. The format is (column_value, count) . It is helpful to determine the exact data value counts from the histograms.","title":"Hovering Over the Histogram"},{"location":"sql/data-insights/#full-data-analysis","text":"Let's do a full data analysis step by step. First, we need to query data for analysis in the MindsDB Cloud editor. Please note that you need to query your dataset without using a LIMIT keyword to be able to perform a complete data analysis. SELECT * FROM example_db . demo_data . home_rentals ; On execution, we get: + ---------------+-------------------+----+--------+--------------+--------------+------------+ | number_of_rooms | number_of_bathrooms | sqft | location | days_on_market | neighborhood | rental_price | + ---------------+-------------------+----+--------+--------------+--------------+------------+ | 2 | 1 | 917 | great | 13 | berkeley_hills | 3901 | | 0 | 1 | 194 | great | 10 | berkeley_hills | 2042 | | 1 | 1 | 543 | poor | 18 | westbrae | 1871 | | 2 | 1 | 503 | good | 10 | downtown | 3026 | | 3 | 2 | 1066 | good | 13 | thowsand_oaks | 4774 | + ---------------+-------------------+----+--------+--------------+--------------+------------+ Now, open the Data Insights pane by clicking the Data Insights button to the right of the output table. Initially, it shows the analysis of the first ten rows of the output table. To perform a complete analysis of your data, you can either go to a full-screen mode or stay in a pane mode and click on the Full Data Analysis button. Below is the complete data analysis. Also, whenever your dataset changes, you can click on the Refresh Data Analysis button to update the data visualization and analysis.","title":"Full Data Analysis"},{"location":"sql/data-insights/#whats-next","text":"Want to do more exploratory data analysis in MindsDB? We\u2019re collecting feedback to develop even more data visualization features. Let us know what you'd like to see as part of Data Insights. Link to the feedback form The link to our feedback form will be shared soon.","title":"What's Next?"},{"location":"sql/feature-eng/","text":"Feature Engineering in MindsDB \u00b6 The more data you have, the more accurate predictions you get. We recommend you provide the predictor with as many historical data rows and data columns as possible to make your predictions even more accurate. The examples presented here prove this hypothesis. If you want to follow the examples, please sign up for the MindsDB Cloud account here . Prerequisites \u00b6 The base table is available in the example_db integration in the MindsDB Cloud Editor. In order to be able to use it, you must first create a database like this: CREATE DATABASE example_db WITH ENGINE = \"postgres\" , PARAMETERS = { \"user\" : \"demo_user\" , \"password\" : \"demo_password\" , \"host\" : \"3.220.66.106\" , \"port\" : \"5432\" , \"database\" : \"demo\" } ; On execution, we get: Query OK , 0 rows affected ( x . xxx sec ) Once that's done, you can run the following commands with us. Example: Adding More Data Columns \u00b6 Introduction \u00b6 Here, we'll create several predictors using the same table, increasing the number of data columns with each step. We start with the base table and create a predictor based on it. Then we add two columns to our base table and again create a predictor based on the enhanced table. At last, we add another two columns and create a predictor. By comparing the accuracies of the predictors, we'll find that more data results in more accurate predictions. Let's get started. Let's Run the Codes \u00b6 Here, we go through the codes for the base table and enhanced base tables simultaneously. Data Setup \u00b6 Let's prepare and verify the data. Here, we create the views and query them to ensure the input for the predictors is in order. Using the Base Table Using the Base Table + 2 More Columns Using the Base Table + 4 More Columns Let's start by querying the data from the example_db.demo_data.used_car_price table, which is our base table. SELECT * FROM example_db . demo_data . used_car_price LIMIT 5 ; On execution, we get: + -----+----+-----+------------+-------+--------+---+----+----------+ | model | year | price | transmission | mileage | fueltype | tax | mpg | enginesize | + -----+----+-----+------------+-------+--------+---+----+----------+ | A1 | 2017 | 12500 | Manual | 15735 | Petrol | 150 | 55 . 4 | 1 . 4 | | A6 | 2016 | 16500 | Automatic | 36203 | Diesel | 20 | 64 . 2 | 2 | | A1 | 2016 | 11000 | Manual | 29946 | Petrol | 30 | 55 . 4 | 1 . 4 | | A4 | 2017 | 16800 | Automatic | 25952 | Diesel | 145 | 67 . 3 | 2 | | A3 | 2019 | 17300 | Manual | 1998 | Petrol | 145 | 49 . 6 | 1 | + -----+----+-----+------------+-------+--------+---+----+----------+ Where: Name Description model Model of the car. year Year of production. price Price of the car. transmission Transmission ( Manual , or Automatic , or Semi-Auto ). mileage Mileage of the car. fueltype Fuel type of the car. tax Tax. mpg Miles per gallon. enginesize Engine size of the car. Let's create a view based on the example_db.demo_data.used_car_price table, and add two more columns. Please note that we replace the mpg column with the kml column. The added columns are: - the kml column, calculated from the mpg column using the formula like in the query below, stands for kilometers per liter , - the years_old column, calculated by subtracting car's year from the current date, stands for car's age. CREATE VIEW used_car_price_plus_2_columns ( SELECT * FROM example_db ( SELECT model , year , price , transmission , mileage , fueltype , tax , enginesize , ROUND ( CAST (( mpg / 2 . 3521458 ) AS numeric ), 1 ) AS kml , -- mpg (miles per galon) is replaced with kml (kilometers per liter) ( date_part ( 'year' , CURRENT_DATE ) - year ) AS years_old -- age of a car FROM demo_data . used_car_price ) ); On execution, we get: Query OK , 0 rows affected ( x . xxx sec ) Let's query the newly created view. SELECT * FROM mindsdb . used_car_price_plus_2_columns LIMIT 5 ; On execution, we get: + -----+----+-----+------------+-------+--------+---+----+----------+----+---------+ | model | year | price | transmission | mileage | fueltype | tax | mpg | enginesize | kml | years_old | + -----+----+-----+------------+-------+--------+---+----+----------+----+---------+ | A1 | 2017 | 12500 | Manual | 15735 | Petrol | 150 | 55 . 4 | 1 . 4 | 23 . 6 | 5 | | A6 | 2016 | 16500 | Automatic | 36203 | Diesel | 20 | 64 . 2 | 2 | 27 . 3 | 6 | | A1 | 2016 | 11000 | Manual | 29946 | Petrol | 30 | 55 . 4 | 1 . 4 | 23 . 6 | 6 | | A4 | 2017 | 16800 | Automatic | 25952 | Diesel | 145 | 67 . 3 | 2 | 28 . 6 | 5 | | A3 | 2019 | 17300 | Manual | 1998 | Petrol | 145 | 49 . 6 | 1 | 21 . 1 | 3 | + -----+----+-----+------------+-------+--------+---+----+----------+----+---------+ Let's create a view based on the example_db.demo_data.used_car_price table, and add four more columns. Please note that we replace the mpg column with the kml column. The added columns are: - the kml column, calculated from the mpg column using the formula like in the query below, stands for kilometers per liter , - the years_old column, calculated by subtracting car's year from the current date, stands for car's age, - the units_to_sell column, calculated using the OVER and PARTITION BY clauses, indicates how many units of a certain car model are sold in a year, - the tax_div_price column, calculated by dividing the tax column by the price column. CREATE VIEW used_car_price_plus_another_2_columns ( SELECT * FROM example_db ( SELECT model , year , price , transmission , mileage , fueltype , tax , enginesize , ROUND ( CAST (( mpg / 2 . 3521458 ) AS numeric ), 1 ) AS kml , -- mpg (miles per galon) is replaced with kml (kilometers per liter) ( date_part ( 'year' , CURRENT_DATE ) - year ) AS years_old , -- age of a car COUNT ( * ) OVER ( PARTITION BY model , year ) AS units_to_sell , -- how many units of a certain model are sold in a year ROUND (( CAST ( tax AS decimal ) / price ), 3 ) AS tax_div_price -- value of tax divided by price of a car FROM demo_data . used_car_price ) ); On execution, we get: Query OK , 0 rows affected ( x . xxx sec ) Let's query the newly created view. SELECT * FROM mindsdb . used_car_price_plus_another_2_columns LIMIT 5 ; On execution, we get: + -----+----+-----+------------+-------+--------+---+----+----------+----+---------+-------------+-------------+ | model | year | price | transmission | mileage | fueltype | tax | mpg | enginesize | kml | years_old | units_to_sell | tax_div_price | + -----+----+-----+------------+-------+--------+---+----+----------+----+---------+-------------+-------------+ | A1 | 2010 | 9990 | Automatic | 38000 | Petrol | 125 | 53 . 3 | 1 . 4 | 22 . 7 | 12 | 1 | 0 . 013 | | A1 | 2011 | 6995 | Manual | 65000 | Petrol | 125 | 53 . 3 | 1 . 4 | 22 . 7 | 11 | 5 | 0 . 018 | | A1 | 2011 | 6295 | Manual | 107000 | Petrol | 125 | 53 . 3 | 1 . 4 | 22 . 7 | 11 | 5 | 0 . 020 | | A1 | 2011 | 4250 | Manual | 116000 | Diesel | 20 | 70 . 6 | 1 . 6 | 30 . 0 | 11 | 5 | 0 . 005 | | A1 | 2011 | 6475 | Manual | 45000 | Diesel | 0 | 70 . 6 | 1 . 6 | 30 . 0 | 11 | 5 | 0 . 000 | + -----+----+-----+------------+-------+--------+---+----+----------+----+---------+-------------+-------------+ Dropping a View If you want to drop a view, run the command DROP VIEW view_name; . Creating Predictors \u00b6 Now, we create predictors based on the example_db.demo_data.used_car_price table and its extensions. Using the Base Table Using the Base Table + 2 More Columns Using the Base Table + 4 More Columns CREATE MODEL mindsdb . price_predictor FROM example_db ( SELECT * FROM demo_data . used_car_price ) PREDICT price ; On execution, we get: Query OK , 0 rows affected ( x . xxx sec ) CREATE MODEL mindsdb . price_predictor_plus_2_columns FROM mindsdb ( SELECT * FROM used_car_price_plus_2_columns ) PREDICT price ; On execution, we get: Query OK , 0 rows affected ( x . xxx sec ) CREATE MODEL mindsdb . price_predictor_plus_another_2_columns FROM mindsdb ( SELECT * FROM used_car_price_plus_another_2_columns ) PREDICT price ; On execution, we get: Query OK , 0 rows affected ( x . xxx sec ) Dropping a Predictor If you want to drop a predictor, run the command DROP MODEL predictor_name; . Predictor Status \u00b6 Finally, let's check the predictor status whose value is generating at first, then training , and at last, complete . Using the Base Table Using the Base Table + 2 More Columns Using the Base Table + 4 More Columns SELECT * FROM mindsdb . models WHERE name = 'price_predictor' ; On execution, we get: + ---------------+--------+--------+---------+-------------+---------------+------+--------------------------------------+----------------+ | name | status | accuracy | predict | update_status | mindsdb_version | error | select_data_query | training_options | + ---------------+--------+--------+---------+-------------+---------------+------+--------------------------------------+----------------+ | price_predictor | complete | 0 . 963 | price | up_to_date | 22 . 10 . 2 . 1 | [ NULL ] | SELECT * FROM demo_data . used_car_price | | + ---------------+--------+--------+---------+-------------+---------------+------+--------------------------------------+----------------+ SELECT * FROM mindsdb . models WHERE name = 'price_predictor_plus_2_columns' ; On execution, we get: + ------------------------------+--------+--------+---------+-------------+---------------+------+-------------------------------------------+----------------+ | name | status | accuracy | predict | update_status | mindsdb_version | error | select_data_query | training_options | + ------------------------------+--------+--------+---------+-------------+---------------+------+-------------------------------------------+----------------+ | price_predictor_plus_2_columns | complete | 0 . 965 | price | up_to_date | 22 . 10 . 2 . 1 | [ NULL ] | SELECT * FROM used_car_price_plus_2_columns | | + ------------------------------+--------+--------+---------+-------------+---------------+------+-------------------------------------------+----------------+ SELECT * FROM mindsdb . models WHERE name = 'price_predictor_plus_another_2_columns' ; On execution, we get: + --------------------------------------+--------+--------+---------+-------------+---------------+------+---------------------------------------------------+----------------+ | name | status | accuracy | predict | update_status | mindsdb_version | error | select_data_query | training_options | + --------------------------------------+--------+--------+---------+-------------+---------------+------+---------------------------------------------------+----------------+ | price_predictor_plus_another_2_columns | complete | 0 . 982 | price | up_to_date | 22 . 10 . 2 . 1 | [ NULL ] | SELECT * FROM used_car_price_plus_another_2_columns | | + --------------------------------------+--------+--------+---------+-------------+---------------+------+---------------------------------------------------+----------------+ Accuracy Comparison \u00b6 Once the training process of all three predictors completes, we see the accuracy values. For the base table, we get an accuracy value of 0.963 . For the base table with two more data columns, we get an accuracy value of 0.965 . The accuracy value increased, as expected. For the base table with four more data columns, we get an accuracy value of 0.982 . The accuracy value increased again, as expected. True vs Predicted Price Comparison \u00b6 Let's compare how close the predicted price values are to the true price. + -------+-------+---------------+-----------+-----------+--------------+----------------+----------------+---------------+ | model | year | transmission | fueltype | mileage | true_price | pred_price_1 | pred_price_2 | pred_price_3 | + -------+-------+---------------+-----------+-----------+--------------+----------------+----------------+---------------+ | A1 | 2017 | Manual | Petrol | 7620 | 14440 | 17268 | 17020 | 14278 | | A6 | 2016 | Automatic | Diesel | 20335 | 18982 | 17226 | 17935 | 19016 | | A3 | 2018 | Semi - Auto | Diesel | 9058 | 19900 | 25641 | 23008 | 21286 | + -------+-------+---------------+-----------+-----------+--------------+----------------+----------------+---------------+ The prices predicted by the third predictor, having the highest accuracy value, are the closest to the true price, as expected. Example: Joining Data Tables \u00b6 Introduction \u00b6 We start by creating a predictor from the car_sales table. Then, we add more data by joining the car_sales and car_info tables. We create a predictor based on the car_sales_info view. Let's get started. Let's Run the Codes \u00b6 Here, we go through the codes using partial tables and the full table after joining the data. Data Setup \u00b6 Here is the car_sales table: SELECT * FROM example_db . demo_data . car_sales LIMIT 5 ; On execution, we get: + -----+----+-----+------------+-------+--------+---+ | model | year | price | transmission | mileage | fueltype | tax | + -----+----+-----+------------+-------+--------+---+ | A1 | 2017 | 12500 | Manual | 15735 | Petrol | 150 | | A6 | 2016 | 16500 | Automatic | 36203 | Diesel | 20 | | A1 | 2016 | 11000 | Manual | 29946 | Petrol | 30 | | A4 | 2017 | 16800 | Automatic | 25952 | Diesel | 145 | | A3 | 2019 | 17300 | Manual | 1998 | Petrol | 145 | + -----+----+-----+------------+-------+--------+---+ Where: Name Description model Model of the car. year Year of production. price Price of the car. transmission Transmission ( Manual , or Automatic , or Semi-Auto ). mileage Mileage of the car. fueltype Fuel type of the car. tax Tax. And here is the car_info table: SELECT * FROM example_db . demo_data . car_info LIMIT 5 ; On execution, we get: + -----+----+------------+---------+-----+----------+ | model | year | transmission | fueltype | mpg | enginesize | + -----+----+------------+---------+-----+----------+ | A1 | 2010 | Automatic | Petrol | 53 . 3 | 1 . 4 | | A1 | 2011 | Manual | Diesel | 70 . 6 | 1 . 6 | | A1 | 2011 | Manual | Petrol | 53 . 3 | 1 . 4 | | A1 | 2012 | Automatic | Petrol | 50 . 6 | 1 . 4 | | A1 | 2012 | Manual | Diesel | 72 . 95 | 1 . 7 | + -----+----+------------+---------+-----+----------+ Where: Name Description model Model of the car. year Year of production. transmission Transmission ( Manual , or Automatic , or Semi-Auto ). fueltype Fuel type of the car. mpg Miles per gallon. enginesize Engine size of the car. Let's join the car_sales and car_info tables on the model , year , transmission , and fueltype columns. SELECT * FROM example_db ( SELECT s . * , i . mpg , i . enginesize FROM demo_data . car_sales s JOIN demo_data . car_info i ON s . model = i . model AND s . year = i . year AND s . transmission = i . transmission AND s . fueltype = i . fueltype ) LIMIT 5 ; Nested SELECT Statements Please note that we use the nested SELECT statement in order to trigger native query at the MindsDB Cloud Editor. Here, the example_db database is a PostgreSQL database, so we trigger PostgreSQL-native syntax. On execution, we get: + -----+----+-----+------------+-------+--------+---+----+----------+ | model | year | price | transmission | mileage | fueltype | tax | mpg | enginesize | + -----+----+-----+------------+-------+--------+---+----+----------+ | A1 | 2010 | 9990 | Automatic | 38000 | Petrol | 125 | 53 . 3 | 1 . 4 | | A1 | 2011 | 4250 | Manual | 116000 | Diesel | 20 | 70 . 6 | 1 . 6 | | A1 | 2011 | 6475 | Manual | 45000 | Diesel | 0 | 70 . 6 | 1 . 6 | | A1 | 2011 | 6295 | Manual | 107000 | Petrol | 125 | 53 . 3 | 1 . 4 | | A1 | 2011 | 7495 | Manual | 60700 | Petrol | 125 | 53 . 3 | 1 . 4 | + -----+----+-----+------------+-------+--------+---+----+----------+ Now, we create a view based on the JOIN query: CREATE VIEW car_sales_info ( SELECT * FROM example_db ( SELECT s . * , i . mpg , i . enginesize FROM demo_data . car_sales s JOIN demo_data . car_info i ON s . model = i . model AND s . year = i . year AND s . transmission = i . transmission AND s . fueltype = i . fueltype ) ); On execution, we get: Query OK , 0 rows affected ( x . xxx sec ) Let's verify the view by selecting from it. SELECT * FROM mindsdb . car_sales_info LIMIT 5 ; On execution, we get: + -----+----+-----+------------+-------+--------+---+----+----------+ | model | year | price | transmission | mileage | fueltype | tax | mpg | enginesize | + -----+----+-----+------------+-------+--------+---+----+----------+ | A1 | 2010 | 9990 | Automatic | 38000 | Petrol | 125 | 53 . 3 | 1 . 4 | | A1 | 2011 | 4250 | Manual | 116000 | Diesel | 20 | 70 . 6 | 1 . 6 | | A1 | 2011 | 6475 | Manual | 45000 | Diesel | 0 | 70 . 6 | 1 . 6 | | A1 | 2011 | 6295 | Manual | 107000 | Petrol | 125 | 53 . 3 | 1 . 4 | | A1 | 2011 | 7495 | Manual | 60700 | Petrol | 125 | 53 . 3 | 1 . 4 | + -----+----+-----+------------+-------+--------+---+----+----------+ Creating Predictors \u00b6 Let's create a predictor with the car_sales table as input data. CREATE MODEL mindsdb . price_predictor_car_sales FROM example_db ( SELECT * FROM demo_data . car_sales ) PREDICT price ; On execution, we get: Query OK , 0 rows affected ( x . xxx sec ) Now, let's create a predictor for the table that is a JOIN between the car_sales and car_info tables. CREATE MODEL mindsdb . price_predictor_car_sales_info FROM mindsdb ( SELECT * FROM car_sales_info ) PREDICT price ; On execution, we get: Query OK , 0 rows affected ( x . xxx sec ) Predictor Status \u00b6 Next, we check the status of both predictors. We start with the predictor based on the partial table. SELECT * FROM mindsdb . models WHERE name = 'price_predictor_car_sales' ; On execution, we get: + -------------------------+--------+--------+---------+-------------+---------------+------+---------------------------------+----------------+ | name | status | accuracy | predict | update_status | mindsdb_version | error | select_data_query | training_options | + -------------------------+--------+--------+---------+-------------+---------------+------+---------------------------------+----------------+ | price_predictor_car_sales | complete | 0 . 912 | price | up_to_date | 22 . 10 . 2 . 1 | [ NULL ] | SELECT * FROM demo_data . car_sales | | + -------------------------+--------+--------+---------+-------------+---------------+------+---------------------------------+----------------+ And now, for the predictor based on the full table. SELECT * FROM mindsdb . models WHERE name = 'price_predictor_car_sales_info' ; On execution, we get: + ------------------------------+--------+--------+---------+-------------+---------------+------+----------------------------+----------------+ | name | status | accuracy | predict | update_status | mindsdb_version | error | select_data_query | training_options | + ------------------------------+--------+--------+---------+-------------+---------------+------+----------------------------+----------------+ | price_predictor_car_sales_info | complete | 0 . 912 | price | up_to_date | 22 . 10 . 2 . 1 | [ NULL ] | SELECT * FROM car_sales_info | | + ------------------------------+--------+--------+---------+-------------+---------------+------+----------------------------+----------------+ Accuracy Comparison \u00b6 The accuracy values are 0.912 for both the predictors. The predictor already learns how the combination of model+year+transmission+fueltype affects the price, so joining more data columns doesn't play a role in this particular example.","title":"Feature Engineering in MindsDB"},{"location":"sql/feature-eng/#feature-engineering-in-mindsdb","text":"The more data you have, the more accurate predictions you get. We recommend you provide the predictor with as many historical data rows and data columns as possible to make your predictions even more accurate. The examples presented here prove this hypothesis. If you want to follow the examples, please sign up for the MindsDB Cloud account here .","title":"Feature Engineering in MindsDB"},{"location":"sql/feature-eng/#prerequisites","text":"The base table is available in the example_db integration in the MindsDB Cloud Editor. In order to be able to use it, you must first create a database like this: CREATE DATABASE example_db WITH ENGINE = \"postgres\" , PARAMETERS = { \"user\" : \"demo_user\" , \"password\" : \"demo_password\" , \"host\" : \"3.220.66.106\" , \"port\" : \"5432\" , \"database\" : \"demo\" } ; On execution, we get: Query OK , 0 rows affected ( x . xxx sec ) Once that's done, you can run the following commands with us.","title":"Prerequisites"},{"location":"sql/feature-eng/#example-adding-more-data-columns","text":"","title":"Example: Adding More Data Columns"},{"location":"sql/feature-eng/#introduction","text":"Here, we'll create several predictors using the same table, increasing the number of data columns with each step. We start with the base table and create a predictor based on it. Then we add two columns to our base table and again create a predictor based on the enhanced table. At last, we add another two columns and create a predictor. By comparing the accuracies of the predictors, we'll find that more data results in more accurate predictions. Let's get started.","title":"Introduction"},{"location":"sql/feature-eng/#lets-run-the-codes","text":"Here, we go through the codes for the base table and enhanced base tables simultaneously.","title":"Let's Run the Codes"},{"location":"sql/feature-eng/#data-setup","text":"Let's prepare and verify the data. Here, we create the views and query them to ensure the input for the predictors is in order. Using the Base Table Using the Base Table + 2 More Columns Using the Base Table + 4 More Columns Let's start by querying the data from the example_db.demo_data.used_car_price table, which is our base table. SELECT * FROM example_db . demo_data . used_car_price LIMIT 5 ; On execution, we get: + -----+----+-----+------------+-------+--------+---+----+----------+ | model | year | price | transmission | mileage | fueltype | tax | mpg | enginesize | + -----+----+-----+------------+-------+--------+---+----+----------+ | A1 | 2017 | 12500 | Manual | 15735 | Petrol | 150 | 55 . 4 | 1 . 4 | | A6 | 2016 | 16500 | Automatic | 36203 | Diesel | 20 | 64 . 2 | 2 | | A1 | 2016 | 11000 | Manual | 29946 | Petrol | 30 | 55 . 4 | 1 . 4 | | A4 | 2017 | 16800 | Automatic | 25952 | Diesel | 145 | 67 . 3 | 2 | | A3 | 2019 | 17300 | Manual | 1998 | Petrol | 145 | 49 . 6 | 1 | + -----+----+-----+------------+-------+--------+---+----+----------+ Where: Name Description model Model of the car. year Year of production. price Price of the car. transmission Transmission ( Manual , or Automatic , or Semi-Auto ). mileage Mileage of the car. fueltype Fuel type of the car. tax Tax. mpg Miles per gallon. enginesize Engine size of the car. Let's create a view based on the example_db.demo_data.used_car_price table, and add two more columns. Please note that we replace the mpg column with the kml column. The added columns are: - the kml column, calculated from the mpg column using the formula like in the query below, stands for kilometers per liter , - the years_old column, calculated by subtracting car's year from the current date, stands for car's age. CREATE VIEW used_car_price_plus_2_columns ( SELECT * FROM example_db ( SELECT model , year , price , transmission , mileage , fueltype , tax , enginesize , ROUND ( CAST (( mpg / 2 . 3521458 ) AS numeric ), 1 ) AS kml , -- mpg (miles per galon) is replaced with kml (kilometers per liter) ( date_part ( 'year' , CURRENT_DATE ) - year ) AS years_old -- age of a car FROM demo_data . used_car_price ) ); On execution, we get: Query OK , 0 rows affected ( x . xxx sec ) Let's query the newly created view. SELECT * FROM mindsdb . used_car_price_plus_2_columns LIMIT 5 ; On execution, we get: + -----+----+-----+------------+-------+--------+---+----+----------+----+---------+ | model | year | price | transmission | mileage | fueltype | tax | mpg | enginesize | kml | years_old | + -----+----+-----+------------+-------+--------+---+----+----------+----+---------+ | A1 | 2017 | 12500 | Manual | 15735 | Petrol | 150 | 55 . 4 | 1 . 4 | 23 . 6 | 5 | | A6 | 2016 | 16500 | Automatic | 36203 | Diesel | 20 | 64 . 2 | 2 | 27 . 3 | 6 | | A1 | 2016 | 11000 | Manual | 29946 | Petrol | 30 | 55 . 4 | 1 . 4 | 23 . 6 | 6 | | A4 | 2017 | 16800 | Automatic | 25952 | Diesel | 145 | 67 . 3 | 2 | 28 . 6 | 5 | | A3 | 2019 | 17300 | Manual | 1998 | Petrol | 145 | 49 . 6 | 1 | 21 . 1 | 3 | + -----+----+-----+------------+-------+--------+---+----+----------+----+---------+ Let's create a view based on the example_db.demo_data.used_car_price table, and add four more columns. Please note that we replace the mpg column with the kml column. The added columns are: - the kml column, calculated from the mpg column using the formula like in the query below, stands for kilometers per liter , - the years_old column, calculated by subtracting car's year from the current date, stands for car's age, - the units_to_sell column, calculated using the OVER and PARTITION BY clauses, indicates how many units of a certain car model are sold in a year, - the tax_div_price column, calculated by dividing the tax column by the price column. CREATE VIEW used_car_price_plus_another_2_columns ( SELECT * FROM example_db ( SELECT model , year , price , transmission , mileage , fueltype , tax , enginesize , ROUND ( CAST (( mpg / 2 . 3521458 ) AS numeric ), 1 ) AS kml , -- mpg (miles per galon) is replaced with kml (kilometers per liter) ( date_part ( 'year' , CURRENT_DATE ) - year ) AS years_old , -- age of a car COUNT ( * ) OVER ( PARTITION BY model , year ) AS units_to_sell , -- how many units of a certain model are sold in a year ROUND (( CAST ( tax AS decimal ) / price ), 3 ) AS tax_div_price -- value of tax divided by price of a car FROM demo_data . used_car_price ) ); On execution, we get: Query OK , 0 rows affected ( x . xxx sec ) Let's query the newly created view. SELECT * FROM mindsdb . used_car_price_plus_another_2_columns LIMIT 5 ; On execution, we get: + -----+----+-----+------------+-------+--------+---+----+----------+----+---------+-------------+-------------+ | model | year | price | transmission | mileage | fueltype | tax | mpg | enginesize | kml | years_old | units_to_sell | tax_div_price | + -----+----+-----+------------+-------+--------+---+----+----------+----+---------+-------------+-------------+ | A1 | 2010 | 9990 | Automatic | 38000 | Petrol | 125 | 53 . 3 | 1 . 4 | 22 . 7 | 12 | 1 | 0 . 013 | | A1 | 2011 | 6995 | Manual | 65000 | Petrol | 125 | 53 . 3 | 1 . 4 | 22 . 7 | 11 | 5 | 0 . 018 | | A1 | 2011 | 6295 | Manual | 107000 | Petrol | 125 | 53 . 3 | 1 . 4 | 22 . 7 | 11 | 5 | 0 . 020 | | A1 | 2011 | 4250 | Manual | 116000 | Diesel | 20 | 70 . 6 | 1 . 6 | 30 . 0 | 11 | 5 | 0 . 005 | | A1 | 2011 | 6475 | Manual | 45000 | Diesel | 0 | 70 . 6 | 1 . 6 | 30 . 0 | 11 | 5 | 0 . 000 | + -----+----+-----+------------+-------+--------+---+----+----------+----+---------+-------------+-------------+ Dropping a View If you want to drop a view, run the command DROP VIEW view_name; .","title":"Data Setup"},{"location":"sql/feature-eng/#creating-predictors","text":"Now, we create predictors based on the example_db.demo_data.used_car_price table and its extensions. Using the Base Table Using the Base Table + 2 More Columns Using the Base Table + 4 More Columns CREATE MODEL mindsdb . price_predictor FROM example_db ( SELECT * FROM demo_data . used_car_price ) PREDICT price ; On execution, we get: Query OK , 0 rows affected ( x . xxx sec ) CREATE MODEL mindsdb . price_predictor_plus_2_columns FROM mindsdb ( SELECT * FROM used_car_price_plus_2_columns ) PREDICT price ; On execution, we get: Query OK , 0 rows affected ( x . xxx sec ) CREATE MODEL mindsdb . price_predictor_plus_another_2_columns FROM mindsdb ( SELECT * FROM used_car_price_plus_another_2_columns ) PREDICT price ; On execution, we get: Query OK , 0 rows affected ( x . xxx sec ) Dropping a Predictor If you want to drop a predictor, run the command DROP MODEL predictor_name; .","title":"Creating Predictors"},{"location":"sql/feature-eng/#predictor-status","text":"Finally, let's check the predictor status whose value is generating at first, then training , and at last, complete . Using the Base Table Using the Base Table + 2 More Columns Using the Base Table + 4 More Columns SELECT * FROM mindsdb . models WHERE name = 'price_predictor' ; On execution, we get: + ---------------+--------+--------+---------+-------------+---------------+------+--------------------------------------+----------------+ | name | status | accuracy | predict | update_status | mindsdb_version | error | select_data_query | training_options | + ---------------+--------+--------+---------+-------------+---------------+------+--------------------------------------+----------------+ | price_predictor | complete | 0 . 963 | price | up_to_date | 22 . 10 . 2 . 1 | [ NULL ] | SELECT * FROM demo_data . used_car_price | | + ---------------+--------+--------+---------+-------------+---------------+------+--------------------------------------+----------------+ SELECT * FROM mindsdb . models WHERE name = 'price_predictor_plus_2_columns' ; On execution, we get: + ------------------------------+--------+--------+---------+-------------+---------------+------+-------------------------------------------+----------------+ | name | status | accuracy | predict | update_status | mindsdb_version | error | select_data_query | training_options | + ------------------------------+--------+--------+---------+-------------+---------------+------+-------------------------------------------+----------------+ | price_predictor_plus_2_columns | complete | 0 . 965 | price | up_to_date | 22 . 10 . 2 . 1 | [ NULL ] | SELECT * FROM used_car_price_plus_2_columns | | + ------------------------------+--------+--------+---------+-------------+---------------+------+-------------------------------------------+----------------+ SELECT * FROM mindsdb . models WHERE name = 'price_predictor_plus_another_2_columns' ; On execution, we get: + --------------------------------------+--------+--------+---------+-------------+---------------+------+---------------------------------------------------+----------------+ | name | status | accuracy | predict | update_status | mindsdb_version | error | select_data_query | training_options | + --------------------------------------+--------+--------+---------+-------------+---------------+------+---------------------------------------------------+----------------+ | price_predictor_plus_another_2_columns | complete | 0 . 982 | price | up_to_date | 22 . 10 . 2 . 1 | [ NULL ] | SELECT * FROM used_car_price_plus_another_2_columns | | + --------------------------------------+--------+--------+---------+-------------+---------------+------+---------------------------------------------------+----------------+","title":"Predictor Status"},{"location":"sql/feature-eng/#accuracy-comparison","text":"Once the training process of all three predictors completes, we see the accuracy values. For the base table, we get an accuracy value of 0.963 . For the base table with two more data columns, we get an accuracy value of 0.965 . The accuracy value increased, as expected. For the base table with four more data columns, we get an accuracy value of 0.982 . The accuracy value increased again, as expected.","title":"Accuracy Comparison"},{"location":"sql/feature-eng/#true-vs-predicted-price-comparison","text":"Let's compare how close the predicted price values are to the true price. + -------+-------+---------------+-----------+-----------+--------------+----------------+----------------+---------------+ | model | year | transmission | fueltype | mileage | true_price | pred_price_1 | pred_price_2 | pred_price_3 | + -------+-------+---------------+-----------+-----------+--------------+----------------+----------------+---------------+ | A1 | 2017 | Manual | Petrol | 7620 | 14440 | 17268 | 17020 | 14278 | | A6 | 2016 | Automatic | Diesel | 20335 | 18982 | 17226 | 17935 | 19016 | | A3 | 2018 | Semi - Auto | Diesel | 9058 | 19900 | 25641 | 23008 | 21286 | + -------+-------+---------------+-----------+-----------+--------------+----------------+----------------+---------------+ The prices predicted by the third predictor, having the highest accuracy value, are the closest to the true price, as expected.","title":"True vs Predicted Price Comparison"},{"location":"sql/feature-eng/#example-joining-data-tables","text":"","title":"Example: Joining Data Tables"},{"location":"sql/feature-eng/#introduction_1","text":"We start by creating a predictor from the car_sales table. Then, we add more data by joining the car_sales and car_info tables. We create a predictor based on the car_sales_info view. Let's get started.","title":"Introduction"},{"location":"sql/feature-eng/#lets-run-the-codes_1","text":"Here, we go through the codes using partial tables and the full table after joining the data.","title":"Let's Run the Codes"},{"location":"sql/feature-eng/#data-setup_1","text":"Here is the car_sales table: SELECT * FROM example_db . demo_data . car_sales LIMIT 5 ; On execution, we get: + -----+----+-----+------------+-------+--------+---+ | model | year | price | transmission | mileage | fueltype | tax | + -----+----+-----+------------+-------+--------+---+ | A1 | 2017 | 12500 | Manual | 15735 | Petrol | 150 | | A6 | 2016 | 16500 | Automatic | 36203 | Diesel | 20 | | A1 | 2016 | 11000 | Manual | 29946 | Petrol | 30 | | A4 | 2017 | 16800 | Automatic | 25952 | Diesel | 145 | | A3 | 2019 | 17300 | Manual | 1998 | Petrol | 145 | + -----+----+-----+------------+-------+--------+---+ Where: Name Description model Model of the car. year Year of production. price Price of the car. transmission Transmission ( Manual , or Automatic , or Semi-Auto ). mileage Mileage of the car. fueltype Fuel type of the car. tax Tax. And here is the car_info table: SELECT * FROM example_db . demo_data . car_info LIMIT 5 ; On execution, we get: + -----+----+------------+---------+-----+----------+ | model | year | transmission | fueltype | mpg | enginesize | + -----+----+------------+---------+-----+----------+ | A1 | 2010 | Automatic | Petrol | 53 . 3 | 1 . 4 | | A1 | 2011 | Manual | Diesel | 70 . 6 | 1 . 6 | | A1 | 2011 | Manual | Petrol | 53 . 3 | 1 . 4 | | A1 | 2012 | Automatic | Petrol | 50 . 6 | 1 . 4 | | A1 | 2012 | Manual | Diesel | 72 . 95 | 1 . 7 | + -----+----+------------+---------+-----+----------+ Where: Name Description model Model of the car. year Year of production. transmission Transmission ( Manual , or Automatic , or Semi-Auto ). fueltype Fuel type of the car. mpg Miles per gallon. enginesize Engine size of the car. Let's join the car_sales and car_info tables on the model , year , transmission , and fueltype columns. SELECT * FROM example_db ( SELECT s . * , i . mpg , i . enginesize FROM demo_data . car_sales s JOIN demo_data . car_info i ON s . model = i . model AND s . year = i . year AND s . transmission = i . transmission AND s . fueltype = i . fueltype ) LIMIT 5 ; Nested SELECT Statements Please note that we use the nested SELECT statement in order to trigger native query at the MindsDB Cloud Editor. Here, the example_db database is a PostgreSQL database, so we trigger PostgreSQL-native syntax. On execution, we get: + -----+----+-----+------------+-------+--------+---+----+----------+ | model | year | price | transmission | mileage | fueltype | tax | mpg | enginesize | + -----+----+-----+------------+-------+--------+---+----+----------+ | A1 | 2010 | 9990 | Automatic | 38000 | Petrol | 125 | 53 . 3 | 1 . 4 | | A1 | 2011 | 4250 | Manual | 116000 | Diesel | 20 | 70 . 6 | 1 . 6 | | A1 | 2011 | 6475 | Manual | 45000 | Diesel | 0 | 70 . 6 | 1 . 6 | | A1 | 2011 | 6295 | Manual | 107000 | Petrol | 125 | 53 . 3 | 1 . 4 | | A1 | 2011 | 7495 | Manual | 60700 | Petrol | 125 | 53 . 3 | 1 . 4 | + -----+----+-----+------------+-------+--------+---+----+----------+ Now, we create a view based on the JOIN query: CREATE VIEW car_sales_info ( SELECT * FROM example_db ( SELECT s . * , i . mpg , i . enginesize FROM demo_data . car_sales s JOIN demo_data . car_info i ON s . model = i . model AND s . year = i . year AND s . transmission = i . transmission AND s . fueltype = i . fueltype ) ); On execution, we get: Query OK , 0 rows affected ( x . xxx sec ) Let's verify the view by selecting from it. SELECT * FROM mindsdb . car_sales_info LIMIT 5 ; On execution, we get: + -----+----+-----+------------+-------+--------+---+----+----------+ | model | year | price | transmission | mileage | fueltype | tax | mpg | enginesize | + -----+----+-----+------------+-------+--------+---+----+----------+ | A1 | 2010 | 9990 | Automatic | 38000 | Petrol | 125 | 53 . 3 | 1 . 4 | | A1 | 2011 | 4250 | Manual | 116000 | Diesel | 20 | 70 . 6 | 1 . 6 | | A1 | 2011 | 6475 | Manual | 45000 | Diesel | 0 | 70 . 6 | 1 . 6 | | A1 | 2011 | 6295 | Manual | 107000 | Petrol | 125 | 53 . 3 | 1 . 4 | | A1 | 2011 | 7495 | Manual | 60700 | Petrol | 125 | 53 . 3 | 1 . 4 | + -----+----+-----+------------+-------+--------+---+----+----------+","title":"Data Setup"},{"location":"sql/feature-eng/#creating-predictors_1","text":"Let's create a predictor with the car_sales table as input data. CREATE MODEL mindsdb . price_predictor_car_sales FROM example_db ( SELECT * FROM demo_data . car_sales ) PREDICT price ; On execution, we get: Query OK , 0 rows affected ( x . xxx sec ) Now, let's create a predictor for the table that is a JOIN between the car_sales and car_info tables. CREATE MODEL mindsdb . price_predictor_car_sales_info FROM mindsdb ( SELECT * FROM car_sales_info ) PREDICT price ; On execution, we get: Query OK , 0 rows affected ( x . xxx sec )","title":"Creating Predictors"},{"location":"sql/feature-eng/#predictor-status_1","text":"Next, we check the status of both predictors. We start with the predictor based on the partial table. SELECT * FROM mindsdb . models WHERE name = 'price_predictor_car_sales' ; On execution, we get: + -------------------------+--------+--------+---------+-------------+---------------+------+---------------------------------+----------------+ | name | status | accuracy | predict | update_status | mindsdb_version | error | select_data_query | training_options | + -------------------------+--------+--------+---------+-------------+---------------+------+---------------------------------+----------------+ | price_predictor_car_sales | complete | 0 . 912 | price | up_to_date | 22 . 10 . 2 . 1 | [ NULL ] | SELECT * FROM demo_data . car_sales | | + -------------------------+--------+--------+---------+-------------+---------------+------+---------------------------------+----------------+ And now, for the predictor based on the full table. SELECT * FROM mindsdb . models WHERE name = 'price_predictor_car_sales_info' ; On execution, we get: + ------------------------------+--------+--------+---------+-------------+---------------+------+----------------------------+----------------+ | name | status | accuracy | predict | update_status | mindsdb_version | error | select_data_query | training_options | + ------------------------------+--------+--------+---------+-------------+---------------+------+----------------------------+----------------+ | price_predictor_car_sales_info | complete | 0 . 912 | price | up_to_date | 22 . 10 . 2 . 1 | [ NULL ] | SELECT * FROM car_sales_info | | + ------------------------------+--------+--------+---------+-------------+---------------+------+----------------------------+----------------+","title":"Predictor Status"},{"location":"sql/feature-eng/#accuracy-comparison_1","text":"The accuracy values are 0.912 for both the predictors. The predictor already learns how the combination of model+year+transmission+fueltype affects the price, so joining more data columns doesn't play a role in this particular example.","title":"Accuracy Comparison"},{"location":"sql/native-queries/","text":"How to Run Native Queries in MindsDB \u00b6 The underlying database engine of MindsDB is MySQL. However, you can run queries native to your database engine within MindsDB. Connect your Database to MindsDB \u00b6 To run queries native to your database, you must first connect your database to MindsDB using the CREATE DATABASE statement. CREATE DATABASE example_db WITH ENGINE = \"postgres\" , PARAMETERS = { \"user\" : \"demo_user\" , \"password\" : \"demo_password\" , \"host\" : \"3.220.66.106\" , \"port\" : \"5432\" , \"database\" : \"demo\" } ; Here we connect the example_db database, which is a PostgreSQL database. Run Queries Native to your Database \u00b6 Once we have our PostgreSQL database connected, we can run PostgreSQL-native queries. Querying \u00b6 To run PostgreSQL-native code, we must nest it within the SELECT statement like this: SELECT * FROM example_db ( SELECT model , year , price , transmission , mileage , fueltype , mpg , -- miles per galon ROUND ( CAST (( mpg / 2 . 3521458 ) AS numeric ), 1 ) AS kml , -- kilometers per liter ( date_part ( 'year' , CURRENT_DATE ) - year ) AS years_old , -- age of a car COUNT ( * ) OVER ( PARTITION BY model , year ) AS units_to_sell , -- how many units of a certain model are sold in a year ROUND (( CAST ( tax AS decimal ) / price ), 3 ) AS tax_div_price -- value of tax divided by price of a car FROM demo_data . used_car_price ); On execution, we get: + -----+----+-----+------------+-------+--------+----+----+---------+-------------+-------------+ | model | year | price | transmission | mileage | fueltype | mpg | kml | years_old | units_to_sell | tax_div_price | + -----+----+-----+------------+-------+--------+----+----+---------+-------------+-------------+ | A1 | 2010 | 9990 | Automatic | 38000 | Petrol | 53 . 3 | 22 . 7 | 12 | 1 | 0 . 013 | | A1 | 2011 | 6995 | Manual | 65000 | Petrol | 53 . 3 | 22 . 7 | 11 | 5 | 0 . 018 | | A1 | 2011 | 6295 | Manual | 107000 | Petrol | 53 . 3 | 22 . 7 | 11 | 5 | 0 . 02 | | A1 | 2011 | 4250 | Manual | 116000 | Diesel | 70 . 6 | 30 | 11 | 5 | 0 . 005 | | A1 | 2011 | 6475 | Manual | 45000 | Diesel | 70 . 6 | 30 | 11 | 5 | 0 | + -----+----+-----+------------+-------+--------+----+----+---------+-------------+-------------+ The first line ( SELECT * FROM example_db ) informs MindsDB that we select from a PostgreSQL database. After that, we nest a PostgreSQL code within brackets. Creating Views \u00b6 We can create a view based on a native query. CREATE VIEW cars ( SELECT * FROM example_db ( SELECT model , year , price , transmission , mileage , fueltype , mpg , -- miles per galon ROUND ( CAST (( mpg / 2 . 3521458 ) AS numeric ), 1 ) AS kml , -- kilometers per liter ( date_part ( 'year' , CURRENT_DATE ) - year ) AS years_old , -- age of a car COUNT ( * ) OVER ( PARTITION BY model , year ) AS units_to_sell , -- how many units of a certain model are sold in a year ROUND (( CAST ( tax AS decimal ) / price ), 3 ) AS tax_div_price -- value of tax divided by price of a car FROM demo_data . used_car_price ) ); On execution, we get: Query OK , 0 rows affected ( x . xxx sec ) Here we have two nesting levels: the native query is nested within the SELECT statement, and the SELECT statement is nested within the CREATE VIEW statement.","title":"Running Native Queries"},{"location":"sql/native-queries/#how-to-run-native-queries-in-mindsdb","text":"The underlying database engine of MindsDB is MySQL. However, you can run queries native to your database engine within MindsDB.","title":"How to Run Native Queries in MindsDB"},{"location":"sql/native-queries/#connect-your-database-to-mindsdb","text":"To run queries native to your database, you must first connect your database to MindsDB using the CREATE DATABASE statement. CREATE DATABASE example_db WITH ENGINE = \"postgres\" , PARAMETERS = { \"user\" : \"demo_user\" , \"password\" : \"demo_password\" , \"host\" : \"3.220.66.106\" , \"port\" : \"5432\" , \"database\" : \"demo\" } ; Here we connect the example_db database, which is a PostgreSQL database.","title":"Connect your Database to MindsDB"},{"location":"sql/native-queries/#run-queries-native-to-your-database","text":"Once we have our PostgreSQL database connected, we can run PostgreSQL-native queries.","title":"Run Queries Native to your Database"},{"location":"sql/native-queries/#querying","text":"To run PostgreSQL-native code, we must nest it within the SELECT statement like this: SELECT * FROM example_db ( SELECT model , year , price , transmission , mileage , fueltype , mpg , -- miles per galon ROUND ( CAST (( mpg / 2 . 3521458 ) AS numeric ), 1 ) AS kml , -- kilometers per liter ( date_part ( 'year' , CURRENT_DATE ) - year ) AS years_old , -- age of a car COUNT ( * ) OVER ( PARTITION BY model , year ) AS units_to_sell , -- how many units of a certain model are sold in a year ROUND (( CAST ( tax AS decimal ) / price ), 3 ) AS tax_div_price -- value of tax divided by price of a car FROM demo_data . used_car_price ); On execution, we get: + -----+----+-----+------------+-------+--------+----+----+---------+-------------+-------------+ | model | year | price | transmission | mileage | fueltype | mpg | kml | years_old | units_to_sell | tax_div_price | + -----+----+-----+------------+-------+--------+----+----+---------+-------------+-------------+ | A1 | 2010 | 9990 | Automatic | 38000 | Petrol | 53 . 3 | 22 . 7 | 12 | 1 | 0 . 013 | | A1 | 2011 | 6995 | Manual | 65000 | Petrol | 53 . 3 | 22 . 7 | 11 | 5 | 0 . 018 | | A1 | 2011 | 6295 | Manual | 107000 | Petrol | 53 . 3 | 22 . 7 | 11 | 5 | 0 . 02 | | A1 | 2011 | 4250 | Manual | 116000 | Diesel | 70 . 6 | 30 | 11 | 5 | 0 . 005 | | A1 | 2011 | 6475 | Manual | 45000 | Diesel | 70 . 6 | 30 | 11 | 5 | 0 | + -----+----+-----+------------+-------+--------+----+----+---------+-------------+-------------+ The first line ( SELECT * FROM example_db ) informs MindsDB that we select from a PostgreSQL database. After that, we nest a PostgreSQL code within brackets.","title":"Querying"},{"location":"sql/native-queries/#creating-views","text":"We can create a view based on a native query. CREATE VIEW cars ( SELECT * FROM example_db ( SELECT model , year , price , transmission , mileage , fueltype , mpg , -- miles per galon ROUND ( CAST (( mpg / 2 . 3521458 ) AS numeric ), 1 ) AS kml , -- kilometers per liter ( date_part ( 'year' , CURRENT_DATE ) - year ) AS years_old , -- age of a car COUNT ( * ) OVER ( PARTITION BY model , year ) AS units_to_sell , -- how many units of a certain model are sold in a year ROUND (( CAST ( tax AS decimal ) / price ), 3 ) AS tax_div_price -- value of tax divided by price of a car FROM demo_data . used_car_price ) ); On execution, we get: Query OK , 0 rows affected ( x . xxx sec ) Here we have two nesting levels: the native query is nested within the SELECT statement, and the SELECT statement is nested within the CREATE VIEW statement.","title":"Creating Views"},{"location":"sql/project/","text":"The PROJECT Entity \u00b6 MindsDB introduces the PROJECT entity that lets you create projects to store your ML experiments. Working with PROJECTS \u00b6 Creating PROJECTS \u00b6 You can create projects to store your models, making the structure like: project_alpha \u251c\u2500 models \u251c\u2500 models_versions \u251c\u2500 model_a \u251c\u2500 model_b project_beta \u251c\u2500 models \u251c\u2500 models_versions \u251c\u2500 model_c Here is how you create a project: CREATE DATABASE project_alpha ; Viewing PROJECTS \u00b6 There are two ways you can list all your databases and projects: Use the SHOW DATABASES command: SHOW DATABASES ; On execution, we get: + ----------------------+ | Database | + ----------------------+ | information_schema | | mindsdb | | project_alpha | | project_beta | + ----------------------+ Use the SHOW FULL DATABASES command to get more details: SHOW FULL DATABASES ; On execution, we get: + ----------------------+----------+-----------+ | Database | TYPE | ENGINE | + ----------------------+----------+-----------+ | information_schema | system | [ NULL ] | | mindsdb | project | [ NULL ] | | project_alpha | project | [ NULL ] | | project_beta | project | [ NULL ] | + ----------------------+----------+-----------+ Dropping PROJECTS \u00b6 Here is how you can remove a project: DROP DATABASE project_alpha ; On execution, we get: Query OK , 0 rows affected ( x . xxx sec ) Cannot Drop a Project Please note that if your project stores at least one model, it cannot be removed. In this case, you should first drop all the models belonging to this project, and then, you can remove the project. Working with MODELS \u00b6 Creating MODELS \u00b6 Here is how you create a model within the project: CREATE MODEL project_alpha . model_a FROM integration_name ( SELECT * FROM table_name ) PREDICT target ; On execution, we get: Query OK , 0 rows affected ( x . xxx sec ) Viewing MODELS \u00b6 To see all the models from all projects, run the command below. SHOW MODELS ; On execution, we get: + ---------+--------------+--------+--------+---------+-------------+---------------+------+------------------------+--------------------+ | NAME | PROJECT | STATUS | ACCURACY | PREDICT | UPDATE_STATUS | MINDSDB_VERSION | ERROR | SELECT_DATA_QUERY | TRAINING_OPTIONS | + ---------+--------------+--------+--------+---------+-------------+---------------+------+------------------------+--------------------+ | model_a | project_alpha | complete | 0 . 999 | target | up_to_date | 22 . 10 . 2 . 1 | [ NULL ] | SELECT * FROM table_name | { 'target' : 'target' } | | model_b | project_alpha | complete | 0 . 999 | target | up_to_date | 22 . 10 . 2 . 1 | [ NULL ] | SELECT * FROM table_name | { 'target' : 'target' } | | model_c | project_beta | complete | 0 . 999 | target | up_to_date | 22 . 10 . 2 . 1 | [ NULL ] | SELECT * FROM table_name | { 'target' : 'target' } | + ---------+--------------+--------+--------+---------+-------------+---------------+------+------------------------+--------------------+ And if you want to list all the models from a defined project, run either of the commands below. SHOW MODELS FROM project_alpha ; -- or SELECT * FROM project_alpha . models ; On execution, we get: + ---------+--------------+--------+--------+---------+-------------+---------------+------+------------------------+--------------------+ | NAME | PROJECT | STATUS | ACCURACY | PREDICT | UPDATE_STATUS | MINDSDB_VERSION | ERROR | SELECT_DATA_QUERY | TRAINING_OPTIONS | + ---------+--------------+--------+--------+---------+-------------+---------------+------+------------------------+--------------------+ | model_a | project_alpha | complete | 0 . 999 | target | up_to_date | 22 . 10 . 2 . 1 | [ NULL ] | SELECT * FROM table_name | { 'target' : 'target' } | | model_b | project_alpha | complete | 0 . 999 | target | up_to_date | 22 . 10 . 2 . 1 | [ NULL ] | SELECT * FROM table_name | { 'target' : 'target' } | + ---------+--------------+--------+--------+---------+-------------+---------------+------+------------------------+--------------------+ Here is how to run a detailed search: SHOW MODELS FROM project_alpha LIKE 'model_a' WHERE status = 'complete' ; On execution, we get: + ---------+--------------+--------+--------+---------+-------------+---------------+------+------------------------+--------------------+ | NAME | PROJECT | STATUS | ACCURACY | PREDICT | UPDATE_STATUS | MINDSDB_VERSION | ERROR | SELECT_DATA_QUERY | TRAINING_OPTIONS | + ---------+--------------+--------+--------+---------+-------------+---------------+------+------------------------+--------------------+ | model_a | project_alpha | complete | 0 . 999 | target | up_to_date | 22 . 10 . 2 . 1 | [ NULL ] | SELECT * FROM table_name | { 'target' : 'target' } | + ---------+--------------+--------+--------+---------+-------------+---------------+------+------------------------+--------------------+ Dropping MODELS \u00b6 To drop a model, run this command: DROP MODEL project_alpha . model_a ; On execution, we get: Query OK , 0 rows affected ( x . xxx sec ) Working with MODELS_VERSION \u00b6 There is a models_versions table for each project that stores all the versions of your models. Here is how to query for all model versions from all the projects: SELECT * FROM information_schema . models_versions ; On execution, we get: + -------+-------------+------+-------+--------+--------+-------+-------------+---------------+------+------------------------+--------------------+ | NAME | PROJECT | ACTIVE | VERSION | STATUS | ACCURACY | PREDICT | UPDATE_STATUS | MINDSDB_VERSION | ERROR | SELECT_DATA_QUERY | TRAINING_OPTIONS | + -------+-------------+------+-------+--------+--------+-------+-------------+---------------+------+------------------------+--------------------+ | model_a | project_alpha | true | 1 | complete | 0 . 999 | target | up_to_date | 22 . 10 . 2 . 1 | [ NULL ] | SELECT * FROM table_name | { 'target' : 'target' } | | model_b | project_alpha | true | 1 | complete | 0 . 999 | target | up_to_date | 22 . 10 . 2 . 1 | [ NULL ] | SELECT * FROM table_name | { 'target' : 'target' } | | model_c | project_beta | true | 1 | complete | 0 . 999 | target | up_to_date | 22 . 10 . 2 . 1 | [ NULL ] | SELECT * FROM table_name | { 'target' : 'target' } | + -------+-------------+------+-------+--------+--------+-------+-------------+---------------+------+------------------------+--------------------+ Example If there is more training data available, you don't need to recreate your model. Instead, use the RETRAIN command. RETRAIN project_alpha . model_b ; After the retraining process completes, here is what you get: SELECT * FROM information_schema . models_versions ; On execution, we get: + -------+-------------+------+-------+--------+--------+-------+-------------+---------------+------+------------------------+--------------------+ | NAME | PROJECT | ACTIVE | VERSION | STATUS | ACCURACY | PREDICT | UPDATE_STATUS | MINDSDB_VERSION | ERROR | SELECT_DATA_QUERY | TRAINING_OPTIONS | + -------+-------------+------+-------+--------+--------+-------+-------------+---------------+------+------------------------+--------------------+ | model_a | project_alpha | true | 1 | complete | 0 . 999 | target | up_to_date | 22 . 10 . 2 . 1 | [ NULL ] | SELECT * FROM table_name | { 'target' : 'target' } | | model_b | project_alpha | false | 1 | complete | 0 . 999 | target | up_to_date | 22 . 10 . 2 . 1 | [ NULL ] | SELECT * FROM table_name | { 'target' : 'target' } | | model_b | project_alpha | true | 2 | complete | 0 . 999 | target | up_to_date | 22 . 10 . 2 . 1 | [ NULL ] | SELECT * FROM table_name | { 'target' : 'target' } | | model_c | project_beta | true | 1 | complete | 0 . 999 | target | up_to_date | 22 . 10 . 2 . 1 | [ NULL ] | SELECT * FROM table_name | { 'target' : 'target' } | + -------+-------------+------+-------+--------+--------+-------+-------------+---------------+------+------------------------+--------------------+ Now, the model_b model has two records storing its two versions, out of which one is active. You can also query for model versions of a project using this SELECT statement: SELECT * FROM project_alpha . models_versions ; On execution, we get: + -------+-------------+------+-------+--------+--------+-------+-------------+---------------+------+------------------------+--------------------+ | NAME | PROJECT | ACTIVE | VERSION | STATUS | ACCURACY | PREDICT | UPDATE_STATUS | MINDSDB_VERSION | ERROR | SELECT_DATA_QUERY | TRAINING_OPTIONS | + -------+-------------+------+-------+--------+--------+-------+-------------+---------------+------+------------------------+--------------------+ | model_a | project_alpha | true | 1 | complete | 0 . 999 | target | up_to_date | 22 . 10 . 2 . 1 | [ NULL ] | SELECT * FROM table_name | { 'target' : 'target' } | | model_b | project_alpha | false | 1 | complete | 0 . 999 | target | up_to_date | 22 . 10 . 2 . 1 | [ NULL ] | SELECT * FROM table_name | { 'target' : 'target' } | | model_b | project_alpha | true | 2 | complete | 0 . 999 | target | up_to_date | 22 . 10 . 2 . 1 | [ NULL ] | SELECT * FROM table_name | { 'target' : 'target' } | + -------+-------------+------+-------+--------+--------+-------+-------------+---------------+------+------------------------+--------------------+ Working with TABLES \u00b6 The models that you create with the CREATE MODEL command are simple tables within a project. Therefore, you can use the SHOW [FULL] TABLES commands to query for them. Here is how to query for tables from all databases/projects/schemas: SELECT table_schema , table_name , table_type FROM information_schema . tables WHERE table_type IN ( 'BASE TABLE' , 'MODEL' ); On execution, we get: + --------------+----------------+------------+ | table_schema | table_name | table_type + --------------+----------------+------------+ | mindsdb | models | BASE TABLE | | mindsdb | models_versions | BASE TABLE | | project_alpha | models | BASE TABLE | | project_alpha | models_versions | BASE TABLE | | project_beta | models | BASE TABLE | | project_beta | models_versions | BASE TABLE | | project_alpha | model_a | MODEL | | project_alpha | model_b | MODEL | | project_beta | model_c | MODEL | + --------------+----------------+------------+ Default Tables Please note that each project contains two tables by default. These are the models table and the models_versions table. There are also shortcut commands to query for the tables: Querying for tables from the default project: SHOW TABLES ; On execution, we get: + ---------------------+ | Tables_in_mindsdb | + ---------------------+ | models | | models_versions | + ---------------------+ Or, to get more details: SHOW FULL TABLES ; On execution, we get: + ---------------------+-----------+ | Tables_in_mindsdb | Table_type | + ---------------------+-----------+ | models | BASE TABLE | | models_versions | BASE TABLE | + ---------------------+-----------+ How to Set a Default Project The default project is set to mindsdb . If you want to change it, run the USE project_name; command. Querying for tables from a defined project: SHOW TABLES FROM project_alpha ; On execution, we get: + -------------------------+ | Tables_in_project_alpha | + -------------------------+ | models | | models_versions | | model_a | | model_b | + -------------------------+ Or, to get more details: SHOW FULL TABLES FROM project_alpha ; On execution, we get: + -------------------------+-----------+ | Tables_in_project_alpha | Table_type | + -------------------------+-----------+ | models | BASE TABLE | | models_versions | BASE TABLE | | model_a | MODEL | | model_b | MODEL | + -------------------------+-----------+","title":"PROJECTS"},{"location":"sql/project/#the-project-entity","text":"MindsDB introduces the PROJECT entity that lets you create projects to store your ML experiments.","title":"The PROJECT Entity"},{"location":"sql/project/#working-with-projects","text":"","title":"Working with PROJECTS"},{"location":"sql/project/#creating-projects","text":"You can create projects to store your models, making the structure like: project_alpha \u251c\u2500 models \u251c\u2500 models_versions \u251c\u2500 model_a \u251c\u2500 model_b project_beta \u251c\u2500 models \u251c\u2500 models_versions \u251c\u2500 model_c Here is how you create a project: CREATE DATABASE project_alpha ;","title":"Creating PROJECTS"},{"location":"sql/project/#viewing-projects","text":"There are two ways you can list all your databases and projects: Use the SHOW DATABASES command: SHOW DATABASES ; On execution, we get: + ----------------------+ | Database | + ----------------------+ | information_schema | | mindsdb | | project_alpha | | project_beta | + ----------------------+ Use the SHOW FULL DATABASES command to get more details: SHOW FULL DATABASES ; On execution, we get: + ----------------------+----------+-----------+ | Database | TYPE | ENGINE | + ----------------------+----------+-----------+ | information_schema | system | [ NULL ] | | mindsdb | project | [ NULL ] | | project_alpha | project | [ NULL ] | | project_beta | project | [ NULL ] | + ----------------------+----------+-----------+","title":"Viewing PROJECTS"},{"location":"sql/project/#dropping-projects","text":"Here is how you can remove a project: DROP DATABASE project_alpha ; On execution, we get: Query OK , 0 rows affected ( x . xxx sec ) Cannot Drop a Project Please note that if your project stores at least one model, it cannot be removed. In this case, you should first drop all the models belonging to this project, and then, you can remove the project.","title":"Dropping PROJECTS"},{"location":"sql/project/#working-with-models","text":"","title":"Working with MODELS"},{"location":"sql/project/#creating-models","text":"Here is how you create a model within the project: CREATE MODEL project_alpha . model_a FROM integration_name ( SELECT * FROM table_name ) PREDICT target ; On execution, we get: Query OK , 0 rows affected ( x . xxx sec )","title":"Creating MODELS"},{"location":"sql/project/#viewing-models","text":"To see all the models from all projects, run the command below. SHOW MODELS ; On execution, we get: + ---------+--------------+--------+--------+---------+-------------+---------------+------+------------------------+--------------------+ | NAME | PROJECT | STATUS | ACCURACY | PREDICT | UPDATE_STATUS | MINDSDB_VERSION | ERROR | SELECT_DATA_QUERY | TRAINING_OPTIONS | + ---------+--------------+--------+--------+---------+-------------+---------------+------+------------------------+--------------------+ | model_a | project_alpha | complete | 0 . 999 | target | up_to_date | 22 . 10 . 2 . 1 | [ NULL ] | SELECT * FROM table_name | { 'target' : 'target' } | | model_b | project_alpha | complete | 0 . 999 | target | up_to_date | 22 . 10 . 2 . 1 | [ NULL ] | SELECT * FROM table_name | { 'target' : 'target' } | | model_c | project_beta | complete | 0 . 999 | target | up_to_date | 22 . 10 . 2 . 1 | [ NULL ] | SELECT * FROM table_name | { 'target' : 'target' } | + ---------+--------------+--------+--------+---------+-------------+---------------+------+------------------------+--------------------+ And if you want to list all the models from a defined project, run either of the commands below. SHOW MODELS FROM project_alpha ; -- or SELECT * FROM project_alpha . models ; On execution, we get: + ---------+--------------+--------+--------+---------+-------------+---------------+------+------------------------+--------------------+ | NAME | PROJECT | STATUS | ACCURACY | PREDICT | UPDATE_STATUS | MINDSDB_VERSION | ERROR | SELECT_DATA_QUERY | TRAINING_OPTIONS | + ---------+--------------+--------+--------+---------+-------------+---------------+------+------------------------+--------------------+ | model_a | project_alpha | complete | 0 . 999 | target | up_to_date | 22 . 10 . 2 . 1 | [ NULL ] | SELECT * FROM table_name | { 'target' : 'target' } | | model_b | project_alpha | complete | 0 . 999 | target | up_to_date | 22 . 10 . 2 . 1 | [ NULL ] | SELECT * FROM table_name | { 'target' : 'target' } | + ---------+--------------+--------+--------+---------+-------------+---------------+------+------------------------+--------------------+ Here is how to run a detailed search: SHOW MODELS FROM project_alpha LIKE 'model_a' WHERE status = 'complete' ; On execution, we get: + ---------+--------------+--------+--------+---------+-------------+---------------+------+------------------------+--------------------+ | NAME | PROJECT | STATUS | ACCURACY | PREDICT | UPDATE_STATUS | MINDSDB_VERSION | ERROR | SELECT_DATA_QUERY | TRAINING_OPTIONS | + ---------+--------------+--------+--------+---------+-------------+---------------+------+------------------------+--------------------+ | model_a | project_alpha | complete | 0 . 999 | target | up_to_date | 22 . 10 . 2 . 1 | [ NULL ] | SELECT * FROM table_name | { 'target' : 'target' } | + ---------+--------------+--------+--------+---------+-------------+---------------+------+------------------------+--------------------+","title":"Viewing MODELS"},{"location":"sql/project/#dropping-models","text":"To drop a model, run this command: DROP MODEL project_alpha . model_a ; On execution, we get: Query OK , 0 rows affected ( x . xxx sec )","title":"Dropping MODELS"},{"location":"sql/project/#working-with-models_version","text":"There is a models_versions table for each project that stores all the versions of your models. Here is how to query for all model versions from all the projects: SELECT * FROM information_schema . models_versions ; On execution, we get: + -------+-------------+------+-------+--------+--------+-------+-------------+---------------+------+------------------------+--------------------+ | NAME | PROJECT | ACTIVE | VERSION | STATUS | ACCURACY | PREDICT | UPDATE_STATUS | MINDSDB_VERSION | ERROR | SELECT_DATA_QUERY | TRAINING_OPTIONS | + -------+-------------+------+-------+--------+--------+-------+-------------+---------------+------+------------------------+--------------------+ | model_a | project_alpha | true | 1 | complete | 0 . 999 | target | up_to_date | 22 . 10 . 2 . 1 | [ NULL ] | SELECT * FROM table_name | { 'target' : 'target' } | | model_b | project_alpha | true | 1 | complete | 0 . 999 | target | up_to_date | 22 . 10 . 2 . 1 | [ NULL ] | SELECT * FROM table_name | { 'target' : 'target' } | | model_c | project_beta | true | 1 | complete | 0 . 999 | target | up_to_date | 22 . 10 . 2 . 1 | [ NULL ] | SELECT * FROM table_name | { 'target' : 'target' } | + -------+-------------+------+-------+--------+--------+-------+-------------+---------------+------+------------------------+--------------------+ Example If there is more training data available, you don't need to recreate your model. Instead, use the RETRAIN command. RETRAIN project_alpha . model_b ; After the retraining process completes, here is what you get: SELECT * FROM information_schema . models_versions ; On execution, we get: + -------+-------------+------+-------+--------+--------+-------+-------------+---------------+------+------------------------+--------------------+ | NAME | PROJECT | ACTIVE | VERSION | STATUS | ACCURACY | PREDICT | UPDATE_STATUS | MINDSDB_VERSION | ERROR | SELECT_DATA_QUERY | TRAINING_OPTIONS | + -------+-------------+------+-------+--------+--------+-------+-------------+---------------+------+------------------------+--------------------+ | model_a | project_alpha | true | 1 | complete | 0 . 999 | target | up_to_date | 22 . 10 . 2 . 1 | [ NULL ] | SELECT * FROM table_name | { 'target' : 'target' } | | model_b | project_alpha | false | 1 | complete | 0 . 999 | target | up_to_date | 22 . 10 . 2 . 1 | [ NULL ] | SELECT * FROM table_name | { 'target' : 'target' } | | model_b | project_alpha | true | 2 | complete | 0 . 999 | target | up_to_date | 22 . 10 . 2 . 1 | [ NULL ] | SELECT * FROM table_name | { 'target' : 'target' } | | model_c | project_beta | true | 1 | complete | 0 . 999 | target | up_to_date | 22 . 10 . 2 . 1 | [ NULL ] | SELECT * FROM table_name | { 'target' : 'target' } | + -------+-------------+------+-------+--------+--------+-------+-------------+---------------+------+------------------------+--------------------+ Now, the model_b model has two records storing its two versions, out of which one is active. You can also query for model versions of a project using this SELECT statement: SELECT * FROM project_alpha . models_versions ; On execution, we get: + -------+-------------+------+-------+--------+--------+-------+-------------+---------------+------+------------------------+--------------------+ | NAME | PROJECT | ACTIVE | VERSION | STATUS | ACCURACY | PREDICT | UPDATE_STATUS | MINDSDB_VERSION | ERROR | SELECT_DATA_QUERY | TRAINING_OPTIONS | + -------+-------------+------+-------+--------+--------+-------+-------------+---------------+------+------------------------+--------------------+ | model_a | project_alpha | true | 1 | complete | 0 . 999 | target | up_to_date | 22 . 10 . 2 . 1 | [ NULL ] | SELECT * FROM table_name | { 'target' : 'target' } | | model_b | project_alpha | false | 1 | complete | 0 . 999 | target | up_to_date | 22 . 10 . 2 . 1 | [ NULL ] | SELECT * FROM table_name | { 'target' : 'target' } | | model_b | project_alpha | true | 2 | complete | 0 . 999 | target | up_to_date | 22 . 10 . 2 . 1 | [ NULL ] | SELECT * FROM table_name | { 'target' : 'target' } | + -------+-------------+------+-------+--------+--------+-------+-------------+---------------+------+------------------------+--------------------+","title":"Working with MODELS_VERSION"},{"location":"sql/project/#working-with-tables","text":"The models that you create with the CREATE MODEL command are simple tables within a project. Therefore, you can use the SHOW [FULL] TABLES commands to query for them. Here is how to query for tables from all databases/projects/schemas: SELECT table_schema , table_name , table_type FROM information_schema . tables WHERE table_type IN ( 'BASE TABLE' , 'MODEL' ); On execution, we get: + --------------+----------------+------------+ | table_schema | table_name | table_type + --------------+----------------+------------+ | mindsdb | models | BASE TABLE | | mindsdb | models_versions | BASE TABLE | | project_alpha | models | BASE TABLE | | project_alpha | models_versions | BASE TABLE | | project_beta | models | BASE TABLE | | project_beta | models_versions | BASE TABLE | | project_alpha | model_a | MODEL | | project_alpha | model_b | MODEL | | project_beta | model_c | MODEL | + --------------+----------------+------------+ Default Tables Please note that each project contains two tables by default. These are the models table and the models_versions table. There are also shortcut commands to query for the tables: Querying for tables from the default project: SHOW TABLES ; On execution, we get: + ---------------------+ | Tables_in_mindsdb | + ---------------------+ | models | | models_versions | + ---------------------+ Or, to get more details: SHOW FULL TABLES ; On execution, we get: + ---------------------+-----------+ | Tables_in_mindsdb | Table_type | + ---------------------+-----------+ | models | BASE TABLE | | models_versions | BASE TABLE | + ---------------------+-----------+ How to Set a Default Project The default project is set to mindsdb . If you want to change it, run the USE project_name; command. Querying for tables from a defined project: SHOW TABLES FROM project_alpha ; On execution, we get: + -------------------------+ | Tables_in_project_alpha | + -------------------------+ | models | | models_versions | | model_a | | model_b | + -------------------------+ Or, to get more details: SHOW FULL TABLES FROM project_alpha ; On execution, we get: + -------------------------+-----------+ | Tables_in_project_alpha | Table_type | + -------------------------+-----------+ | models | BASE TABLE | | models_versions | BASE TABLE | | model_a | MODEL | | model_b | MODEL | + -------------------------+-----------+","title":"Working with TABLES"},{"location":"sql/table-structure/","text":"MindsDB Default Structure \u00b6 On start-up, MindsDB consists of one system database ( information_schema ), one default project ( mindsdb ), and two base tables ( models and models_versions ) that belong to the default project. You can verify it by running the following SQL commands: SHOW [ FULL ] DATABASES ; On execution, we get: + ----------------------+---------+--------+ | Database | TYPE | ENGINE | + ----------------------+---------+--------+ | information_schema | system | [ NULL ] | | mindsdb | project | [ NULL ] | | files | data | files | + ----------------------+---------+--------+ And: SHOW [ FULL ] TABLES ; On execution, we get: + ----------------------+-------------+ | Tables_in_mindsdb | Table_type | + ----------------------+-------------+ | models | BASE TABLE | | models_versions | BASE TABLE | + ----------------------+-------------+ The information_schema Database \u00b6 The information_schema database contains all the system tables, such as databases , tables , columns , ml_engines , etc. You can query for any system information using this query template: SELECT * FROM information_schema . table_name ; Don't forget to replace table_name with the table of your interest. Let's query for the following: The databases table lists all the databases including their name, type, and engine. SELECT * FROM information_schema . databases ; On execution, we get: + ------------------+-------+--------+ | NAME | TYPE | ENGINE | + ------------------+-------+--------+ | information_schema | system | [ NULL ] | | mindsdb | project | [ NULL ] | | my_new_project | project | [ NULL ] | | files | data | files | + ------------------+-------+--------+ The handlers table lists all the available ML handlers. SELECT * FROM information_schema . handlers ; -- or SHOW HANDLERS ; On execution, we get: + ------------------+------------+---------------------------------+-------+------------------------------------------------------------------------------+----------------+----------------------+------+ | NAME | TITLE | DESCRIPTION | VERSION | CONNECTION_ARGS | IMPORT_SUCCESS | IMPORT_ERROR | FIELD8 | + ------------------+------------+---------------------------------+-------+------------------------------------------------------------------------------+----------------+----------------------+------+ | merlion | Merlion | MindsDB handler for Merlion | 0 . 0 . 1 | [ NULL ] | true | [ NULL ] | | | byom | BYOM | MindsDB handler for BYOM | 0 . 0 . 1 | { 'model_code' : { 'type' : 'path' , 'description' : 'The path name to model code' }} | true | [ NULL ] | | | ludwig | Ludwig | MindsDB handler for Ludwig AutoML | 0 . 0 . 2 | [ NULL ] | false | No module named 'dask' | | | lightwood | Lightwood | [ NULL ] | 1 . 0 . 0 | [ NULL ] | true | [ NULL ] | | | huggingface | Hugging Face | MindsDB handler for Higging Face | 0 . 0 . 1 | [ NULL ] | true | [ NULL ] | | + ------------------+------------+---------------------------------+-------+------------------------------------------------------------------------------+----------------+----------------------+------+ The ml_engines table lists all the ML engines. These are the instantiated ML handlers. Check out our docs on the CREATE ML _ENGINE command to learn more. SELECT * FROM information_schema . ml_engines ; -- or SHOW ML_ENGINES ; On execution, we get: + ------------------+-----------+----------------+ | NAME | HANDLER | CONNECTION_DATA | + ------------------+-----------+----------------+ | huggingface | huggingface | { 'password' : '' } | | lightwood | lightwood | { 'password' : '' } | + ------------------+-----------+----------------+ The mindsdb Project \u00b6 You create models and views within projects. The default project is mindsdb . But you can create your projects using the CREATE DATABASE statement, as below: CREATE DATABASE my_new_project ; Here is how to create a model within your project: CREATE MODEL my_new_project . my_model FROM integration_name ( SELECT * FROM table_name ) PREDICT target ; And here is how to create a view within your project: CREATE VIEW my_new_project . my_view ( SELECT * FROM integration_name . table_name ); Please replace the integration_name and table_name placeholders with your database name and your table name respectively. What If Names of a Model and a View are the Same? Please note that if you use the same name for a model and a view stored in one project, then MindsDB adds _view to the view name. Now you can verify that the model and view are within your project: SHOW FULL TABLES FROM my_new_project ; On execution, we get: + ------------------------------+-------------+ | Tables_in_my_new_project | Table_type | + ------------------------------+-------------+ | models | BASE TABLE | | models_versions | BASE TABLE | | my_model | MODEL | | my_view | VIEW | + ------------------------------+-------------+ The models and models_versions Tables \u00b6 The mindsdb project and every project that you create contain these two tables by default: The models table stores information about the modes within the project, such as name , status , accuracy , and more. The models_versions table stores information about all present and past versions of each model. For more information on the models and models_versions tables, visit our docs on the PROJECT entity . The files Database \u00b6 It is another default database that stores all the files uploaded by you to MindsDB Cloud. Here is how you can upload files to MindsDB .","title":"MindsDB Structure"},{"location":"sql/table-structure/#mindsdb-default-structure","text":"On start-up, MindsDB consists of one system database ( information_schema ), one default project ( mindsdb ), and two base tables ( models and models_versions ) that belong to the default project. You can verify it by running the following SQL commands: SHOW [ FULL ] DATABASES ; On execution, we get: + ----------------------+---------+--------+ | Database | TYPE | ENGINE | + ----------------------+---------+--------+ | information_schema | system | [ NULL ] | | mindsdb | project | [ NULL ] | | files | data | files | + ----------------------+---------+--------+ And: SHOW [ FULL ] TABLES ; On execution, we get: + ----------------------+-------------+ | Tables_in_mindsdb | Table_type | + ----------------------+-------------+ | models | BASE TABLE | | models_versions | BASE TABLE | + ----------------------+-------------+","title":"MindsDB Default Structure"},{"location":"sql/table-structure/#the-information_schema-database","text":"The information_schema database contains all the system tables, such as databases , tables , columns , ml_engines , etc. You can query for any system information using this query template: SELECT * FROM information_schema . table_name ; Don't forget to replace table_name with the table of your interest. Let's query for the following: The databases table lists all the databases including their name, type, and engine. SELECT * FROM information_schema . databases ; On execution, we get: + ------------------+-------+--------+ | NAME | TYPE | ENGINE | + ------------------+-------+--------+ | information_schema | system | [ NULL ] | | mindsdb | project | [ NULL ] | | my_new_project | project | [ NULL ] | | files | data | files | + ------------------+-------+--------+ The handlers table lists all the available ML handlers. SELECT * FROM information_schema . handlers ; -- or SHOW HANDLERS ; On execution, we get: + ------------------+------------+---------------------------------+-------+------------------------------------------------------------------------------+----------------+----------------------+------+ | NAME | TITLE | DESCRIPTION | VERSION | CONNECTION_ARGS | IMPORT_SUCCESS | IMPORT_ERROR | FIELD8 | + ------------------+------------+---------------------------------+-------+------------------------------------------------------------------------------+----------------+----------------------+------+ | merlion | Merlion | MindsDB handler for Merlion | 0 . 0 . 1 | [ NULL ] | true | [ NULL ] | | | byom | BYOM | MindsDB handler for BYOM | 0 . 0 . 1 | { 'model_code' : { 'type' : 'path' , 'description' : 'The path name to model code' }} | true | [ NULL ] | | | ludwig | Ludwig | MindsDB handler for Ludwig AutoML | 0 . 0 . 2 | [ NULL ] | false | No module named 'dask' | | | lightwood | Lightwood | [ NULL ] | 1 . 0 . 0 | [ NULL ] | true | [ NULL ] | | | huggingface | Hugging Face | MindsDB handler for Higging Face | 0 . 0 . 1 | [ NULL ] | true | [ NULL ] | | + ------------------+------------+---------------------------------+-------+------------------------------------------------------------------------------+----------------+----------------------+------+ The ml_engines table lists all the ML engines. These are the instantiated ML handlers. Check out our docs on the CREATE ML _ENGINE command to learn more. SELECT * FROM information_schema . ml_engines ; -- or SHOW ML_ENGINES ; On execution, we get: + ------------------+-----------+----------------+ | NAME | HANDLER | CONNECTION_DATA | + ------------------+-----------+----------------+ | huggingface | huggingface | { 'password' : '' } | | lightwood | lightwood | { 'password' : '' } | + ------------------+-----------+----------------+","title":"The information_schema Database"},{"location":"sql/table-structure/#the-mindsdb-project","text":"You create models and views within projects. The default project is mindsdb . But you can create your projects using the CREATE DATABASE statement, as below: CREATE DATABASE my_new_project ; Here is how to create a model within your project: CREATE MODEL my_new_project . my_model FROM integration_name ( SELECT * FROM table_name ) PREDICT target ; And here is how to create a view within your project: CREATE VIEW my_new_project . my_view ( SELECT * FROM integration_name . table_name ); Please replace the integration_name and table_name placeholders with your database name and your table name respectively. What If Names of a Model and a View are the Same? Please note that if you use the same name for a model and a view stored in one project, then MindsDB adds _view to the view name. Now you can verify that the model and view are within your project: SHOW FULL TABLES FROM my_new_project ; On execution, we get: + ------------------------------+-------------+ | Tables_in_my_new_project | Table_type | + ------------------------------+-------------+ | models | BASE TABLE | | models_versions | BASE TABLE | | my_model | MODEL | | my_view | VIEW | + ------------------------------+-------------+","title":"The mindsdb Project"},{"location":"sql/table-structure/#the-models-and-models_versions-tables","text":"The mindsdb project and every project that you create contain these two tables by default: The models table stores information about the modes within the project, such as name , status , accuracy , and more. The models_versions table stores information about all present and past versions of each model. For more information on the models and models_versions tables, visit our docs on the PROJECT entity .","title":"The models and models_versions Tables"},{"location":"sql/table-structure/#the-files-database","text":"It is another default database that stores all the files uploaded by you to MindsDB Cloud. Here is how you can upload files to MindsDB .","title":"The files Database"},{"location":"sql/api/describe/","text":"DESCRIBE MODEL Statement \u00b6 The DESCRIBE MODEL statement is used to display the attributes of an existing model. DESCRIBE MODEL ... FEATURES Statement \u00b6 Description \u00b6 The DESCRIBE MODEL mindsdb.[predictor_name].features statement displays how the model encoded the data before the training process. Syntax \u00b6 Here is the syntax: DESCRIBE MODEL mindsdb .[ predictor_name ]. features ; On execution, we get: + --------------+-------------+--------------+-------------+ | column | type | encoder | role | + --------------+-------------+--------------+-------------+ | column_name | column_type | encoder_used | column_role | + --------------+-------------+--------------+-------------+ Where: Name Description [predictor_name] Name of the model to be described. column Data columns that were used to create the model. type Data type of the column. encoder Encoder type used for the column. role Role of the column ( feature or target ). Example \u00b6 Let's look at an example using the home_rentals_model model. DESCRIBE MODEL mindsdb . home_rentals_model . features ; On execution, we get: + ---------------------+-------------+----------------+---------+ | column | type | encoder | role | + ---------------------+-------------+----------------+---------+ | number_of_rooms | categorical | OneHotEncoder | feature | | number_of_bathrooms | binary | BinaryEncoder | feature | | sqft | integer | NumericEncoder | feature | | location | categorical | OneHotEncoder | feature | | days_on_market | integer | NumericEncoder | feature | | neighborhood | categorical | OneHotEncoder | feature | | rental_price | integer | NumericEncoder | target | + ---------------------+-------------+----------------+---------+ Here the rental_price column is the target column to be predicted. As for the feature columns, these are used to train the ML model to predict the value of the rental_price column. DESCRIBE MODEL ... MODEL Statement \u00b6 Description \u00b6 The DESCRIBE MODEL mindsdb.[predictor_name].model statement displays the performance of the candidate models. Syntax \u00b6 Here is the syntax: DESCRIBE MODEL mindsdb .[ predictor_name ]. model ; On execution, we get: + -----------------+-------------+---------------+-----------+---------------------+ | name | performance | training_time | selected | accuracy_functions | + -----------------+-------------+---------------+-----------+---------------------+ | candidate_model | performance | training_time | selected | accuracy_functions | + -----------------+-------------+---------------+-----------+---------------------+ Where: Name Description [predictor_name] Name of the model to be described. name Name of the candidate model. performance Accuracy value from 0 to 1, depending on the type of the model. training_time Time elapsed for the training of the model. selected 1 for the best performing model and 0 for the rest. accuracy_functions It stores the r2_score value for regression predictions, the balanced_accuracy_score value for classification predictions, and the bounded_ts_accuracy value for time series predictions. The values vary between 0 and 1, where 1 indicates a perfect predictor, based on results obtained for a held out portion of data. Example \u00b6 Let's look at an example using the home_rentals_model model. DESCRIBE MODEL mindsdb . home_rentals_model . model ; On execution, we get: + ------------+--------------------+----------------------+----------+---------------------+ | name | performance | training_time | selected | accuracy_functions | + ------------+--------------------+----------------------+----------+---------------------+ | Neural | 0 . 9861694189913056 | 3 . 1538941860198975 | 0 | [ 'r2_score' ] | | LightGBM | 0 . 9991920992432087 | 15 . 671080827713013 | 1 | [ 'r2_score' ] | | Regression | 0 . 9983390488042778 | 0 . 016761064529418945 | 0 | [ 'r2_score' ] | + ------------+--------------------+----------------------+----------+---------------------+ The value of the accuracy_functions column is stored in the performance column. For example, the r2_score value of the Neural model is 0.9861694189913056 . DESCRIBE MODEL ... ENSEMBLE Statement \u00b6 Description \u00b6 The DESCRIBE MODEL mindsdb.[predictor_name].ensemble statement displays the parameters used to select the best candidate model. Syntax \u00b6 Here is the syntax: DESCRIBE MODEL mindsdb .[ predictor_name ]. ensemble ; On execution, we get: + -----------------+ | ensemble | + -----------------+ | { JSON } | + -----------------+ Where: Name Description [predictor_name] Name of the model to be described. ensemble Object of the JSON type describing the parameters used to select the best candidate model. Example \u00b6 Let's look at an example using the home_rentals_model model. DESCRIBE MODEL mindsdb . home_rentals_model . ensemble ; On execution, we get: + ----------------------------------------------------------------------+ | ensemble | + ----------------------------------------------------------------------+ | { | | \"encoders\" : { | | \"rental_price\" : { | | \"module\" : \"NumericEncoder\" , | | \"args\" : { | | \"is_target\" : \"True\" , | | \"positive_domain\" : \"$statistical_analysis.positive_domain\" | | } | | } , | | \"number_of_rooms\" : { | | \"module\" : \"OneHotEncoder\" , | | \"args\" : {} | | } , | | \"number_of_bathrooms\" : { | | \"module\" : \"BinaryEncoder\" , | | \"args\" : {} | | } , | | \"sqft\" : { | | \"module\" : \"NumericEncoder\" , | | \"args\" : {} | | } , | | \"location\" : { | | \"module\" : \"OneHotEncoder\" , | | \"args\" : {} | | } , | | \"days_on_market\" : { | | \"module\" : \"NumericEncoder\" , | | \"args\" : {} | | } , | | \"neighborhood\" : { | | \"module\" : \"OneHotEncoder\" , | | \"args\" : {} | | } | | } , | | \"dtype_dict\" : { | | \"number_of_rooms\" : \"categorical\" , | | \"number_of_bathrooms\" : \"binary\" , | | \"sqft\" : \"integer\" , | | \"location\" : \"categorical\" , | | \"days_on_market\" : \"integer\" , | | \"neighborhood\" : \"categorical\" , | | \"rental_price\" : \"integer\" | | } , | | \"dependency_dict\" : {} , | | \"model\" : { | | \"module\" : \"BestOf\" , | | \"args\" : { | | \"submodels\" : [ | | { | | \"module\" : \"Neural\" , | | \"args\" : { | | \"fit_on_dev\" : true , | | \"stop_after\" : \"$problem_definition.seconds_per_mixer\" , | | \"search_hyperparameters\" : true | | } | | } , | | { | | \"module\" : \"LightGBM\" , | | \"args\" : { | | \"stop_after\" : \"$problem_definition.seconds_per_mixer\" , | | \"fit_on_dev\" : true | | } | | } , | | { | | \"module\" : \"Regression\" , | | \"args\" : { | | \"stop_after\" : \"$problem_definition.seconds_per_mixer\" | | } | | } | | ], | | \"args\" : \"$pred_args\" , | | \"accuracy_functions\" : \"$accuracy_functions\" , | | \"ts_analysis\" : null | | } | | } , | | \"problem_definition\" : { | | \"target\" : \"rental_price\" , | | \"pct_invalid\" : 2 , | | \"unbias_target\" : true , | | \"seconds_per_mixer\" : 57024 . 0 , | | \"seconds_per_encoder\" : null , | | \"expected_additional_time\" : 8 . 687719106674194 , | | \"time_aim\" : 259200 , | | \"target_weights\" : null , | | \"positive_domain\" : false , | | \"timeseries_settings\" : { | | \"is_timeseries\" : false , | | \"order_by\" : null , | | \"window\" : null , | | \"group_by\" : null , | | \"use_previous_target\" : true , | | \"horizon\" : null , | | \"historical_columns\" : null , | | \"target_type\" : \"\" , | | \"allow_incomplete_history\" : true , | | \"eval_cold_start\" : true , | | \"interval_periods\" : [] | | } , | | \"anomaly_detection\" : false , | | \"use_default_analysis\" : true , | | \"ignore_features\" : [], | | \"fit_on_all\" : true , | | \"strict_mode\" : true , | | \"seed_nr\" : 420 | | } , | | \"identifiers\" : {} , | | \"accuracy_functions\" : [ | | \"r2_score\" | | ] | | } | + ----------------------------------------------------------------------+ Need More Info? If you need more information on how to DESCRIBE MODEL or understand the results, feel free to ask us on the community Slack workspace .","title":"DESCRIBE MODEL"},{"location":"sql/api/describe/#describe-model-statement","text":"The DESCRIBE MODEL statement is used to display the attributes of an existing model.","title":"DESCRIBE MODEL Statement"},{"location":"sql/api/describe/#describe-model-features-statement","text":"","title":"DESCRIBE MODEL ... FEATURES Statement"},{"location":"sql/api/describe/#description","text":"The DESCRIBE MODEL mindsdb.[predictor_name].features statement displays how the model encoded the data before the training process.","title":"Description"},{"location":"sql/api/describe/#syntax","text":"Here is the syntax: DESCRIBE MODEL mindsdb .[ predictor_name ]. features ; On execution, we get: + --------------+-------------+--------------+-------------+ | column | type | encoder | role | + --------------+-------------+--------------+-------------+ | column_name | column_type | encoder_used | column_role | + --------------+-------------+--------------+-------------+ Where: Name Description [predictor_name] Name of the model to be described. column Data columns that were used to create the model. type Data type of the column. encoder Encoder type used for the column. role Role of the column ( feature or target ).","title":"Syntax"},{"location":"sql/api/describe/#example","text":"Let's look at an example using the home_rentals_model model. DESCRIBE MODEL mindsdb . home_rentals_model . features ; On execution, we get: + ---------------------+-------------+----------------+---------+ | column | type | encoder | role | + ---------------------+-------------+----------------+---------+ | number_of_rooms | categorical | OneHotEncoder | feature | | number_of_bathrooms | binary | BinaryEncoder | feature | | sqft | integer | NumericEncoder | feature | | location | categorical | OneHotEncoder | feature | | days_on_market | integer | NumericEncoder | feature | | neighborhood | categorical | OneHotEncoder | feature | | rental_price | integer | NumericEncoder | target | + ---------------------+-------------+----------------+---------+ Here the rental_price column is the target column to be predicted. As for the feature columns, these are used to train the ML model to predict the value of the rental_price column.","title":"Example"},{"location":"sql/api/describe/#describe-model-model-statement","text":"","title":"DESCRIBE MODEL ... MODEL Statement"},{"location":"sql/api/describe/#description_1","text":"The DESCRIBE MODEL mindsdb.[predictor_name].model statement displays the performance of the candidate models.","title":"Description"},{"location":"sql/api/describe/#syntax_1","text":"Here is the syntax: DESCRIBE MODEL mindsdb .[ predictor_name ]. model ; On execution, we get: + -----------------+-------------+---------------+-----------+---------------------+ | name | performance | training_time | selected | accuracy_functions | + -----------------+-------------+---------------+-----------+---------------------+ | candidate_model | performance | training_time | selected | accuracy_functions | + -----------------+-------------+---------------+-----------+---------------------+ Where: Name Description [predictor_name] Name of the model to be described. name Name of the candidate model. performance Accuracy value from 0 to 1, depending on the type of the model. training_time Time elapsed for the training of the model. selected 1 for the best performing model and 0 for the rest. accuracy_functions It stores the r2_score value for regression predictions, the balanced_accuracy_score value for classification predictions, and the bounded_ts_accuracy value for time series predictions. The values vary between 0 and 1, where 1 indicates a perfect predictor, based on results obtained for a held out portion of data.","title":"Syntax"},{"location":"sql/api/describe/#example_1","text":"Let's look at an example using the home_rentals_model model. DESCRIBE MODEL mindsdb . home_rentals_model . model ; On execution, we get: + ------------+--------------------+----------------------+----------+---------------------+ | name | performance | training_time | selected | accuracy_functions | + ------------+--------------------+----------------------+----------+---------------------+ | Neural | 0 . 9861694189913056 | 3 . 1538941860198975 | 0 | [ 'r2_score' ] | | LightGBM | 0 . 9991920992432087 | 15 . 671080827713013 | 1 | [ 'r2_score' ] | | Regression | 0 . 9983390488042778 | 0 . 016761064529418945 | 0 | [ 'r2_score' ] | + ------------+--------------------+----------------------+----------+---------------------+ The value of the accuracy_functions column is stored in the performance column. For example, the r2_score value of the Neural model is 0.9861694189913056 .","title":"Example"},{"location":"sql/api/describe/#describe-model-ensemble-statement","text":"","title":"DESCRIBE MODEL ... ENSEMBLE Statement"},{"location":"sql/api/describe/#description_2","text":"The DESCRIBE MODEL mindsdb.[predictor_name].ensemble statement displays the parameters used to select the best candidate model.","title":"Description"},{"location":"sql/api/describe/#syntax_2","text":"Here is the syntax: DESCRIBE MODEL mindsdb .[ predictor_name ]. ensemble ; On execution, we get: + -----------------+ | ensemble | + -----------------+ | { JSON } | + -----------------+ Where: Name Description [predictor_name] Name of the model to be described. ensemble Object of the JSON type describing the parameters used to select the best candidate model.","title":"Syntax"},{"location":"sql/api/describe/#example_2","text":"Let's look at an example using the home_rentals_model model. DESCRIBE MODEL mindsdb . home_rentals_model . ensemble ; On execution, we get: + ----------------------------------------------------------------------+ | ensemble | + ----------------------------------------------------------------------+ | { | | \"encoders\" : { | | \"rental_price\" : { | | \"module\" : \"NumericEncoder\" , | | \"args\" : { | | \"is_target\" : \"True\" , | | \"positive_domain\" : \"$statistical_analysis.positive_domain\" | | } | | } , | | \"number_of_rooms\" : { | | \"module\" : \"OneHotEncoder\" , | | \"args\" : {} | | } , | | \"number_of_bathrooms\" : { | | \"module\" : \"BinaryEncoder\" , | | \"args\" : {} | | } , | | \"sqft\" : { | | \"module\" : \"NumericEncoder\" , | | \"args\" : {} | | } , | | \"location\" : { | | \"module\" : \"OneHotEncoder\" , | | \"args\" : {} | | } , | | \"days_on_market\" : { | | \"module\" : \"NumericEncoder\" , | | \"args\" : {} | | } , | | \"neighborhood\" : { | | \"module\" : \"OneHotEncoder\" , | | \"args\" : {} | | } | | } , | | \"dtype_dict\" : { | | \"number_of_rooms\" : \"categorical\" , | | \"number_of_bathrooms\" : \"binary\" , | | \"sqft\" : \"integer\" , | | \"location\" : \"categorical\" , | | \"days_on_market\" : \"integer\" , | | \"neighborhood\" : \"categorical\" , | | \"rental_price\" : \"integer\" | | } , | | \"dependency_dict\" : {} , | | \"model\" : { | | \"module\" : \"BestOf\" , | | \"args\" : { | | \"submodels\" : [ | | { | | \"module\" : \"Neural\" , | | \"args\" : { | | \"fit_on_dev\" : true , | | \"stop_after\" : \"$problem_definition.seconds_per_mixer\" , | | \"search_hyperparameters\" : true | | } | | } , | | { | | \"module\" : \"LightGBM\" , | | \"args\" : { | | \"stop_after\" : \"$problem_definition.seconds_per_mixer\" , | | \"fit_on_dev\" : true | | } | | } , | | { | | \"module\" : \"Regression\" , | | \"args\" : { | | \"stop_after\" : \"$problem_definition.seconds_per_mixer\" | | } | | } | | ], | | \"args\" : \"$pred_args\" , | | \"accuracy_functions\" : \"$accuracy_functions\" , | | \"ts_analysis\" : null | | } | | } , | | \"problem_definition\" : { | | \"target\" : \"rental_price\" , | | \"pct_invalid\" : 2 , | | \"unbias_target\" : true , | | \"seconds_per_mixer\" : 57024 . 0 , | | \"seconds_per_encoder\" : null , | | \"expected_additional_time\" : 8 . 687719106674194 , | | \"time_aim\" : 259200 , | | \"target_weights\" : null , | | \"positive_domain\" : false , | | \"timeseries_settings\" : { | | \"is_timeseries\" : false , | | \"order_by\" : null , | | \"window\" : null , | | \"group_by\" : null , | | \"use_previous_target\" : true , | | \"horizon\" : null , | | \"historical_columns\" : null , | | \"target_type\" : \"\" , | | \"allow_incomplete_history\" : true , | | \"eval_cold_start\" : true , | | \"interval_periods\" : [] | | } , | | \"anomaly_detection\" : false , | | \"use_default_analysis\" : true , | | \"ignore_features\" : [], | | \"fit_on_all\" : true , | | \"strict_mode\" : true , | | \"seed_nr\" : 420 | | } , | | \"identifiers\" : {} , | | \"accuracy_functions\" : [ | | \"r2_score\" | | ] | | } | + ----------------------------------------------------------------------+ Need More Info? If you need more information on how to DESCRIBE MODEL or understand the results, feel free to ask us on the community Slack workspace .","title":"Example"},{"location":"sql/api/drop/","text":"DROP MODEL Statement \u00b6 Description \u00b6 The DROP MODEL statement deletes the model table. Syntax \u00b6 Here is the syntax: DROP MODEL [ predictor_name ]; On execution, we get: Query OK , 0 rows affected ( 0 . 058 sec ) Where: Name Description [predictor_name] Name of the model to be deleted. Example \u00b6 Let's list all the available predictor tables. SELECT name FROM mindsdb . models ; On execution, we get: + ---------------------+ | name | + ---------------------+ | other_model | | home_rentals_model | + ---------------------+ Now we delete the home_rentals_model table. DROP MODEL home_rentals_model ; On execution, we get: Query OK , 0 rows affected ( 0 . 058 sec ) We can check if the deletion was successful by querying the mindsdb.models table again. SELECT name FROM mindsdb . models ; On execution, we get: + ---------------------+ | name | + ---------------------+ | other_model | + ---------------------+","title":"DROP MODEL"},{"location":"sql/api/drop/#drop-model-statement","text":"","title":"DROP MODEL Statement"},{"location":"sql/api/drop/#description","text":"The DROP MODEL statement deletes the model table.","title":"Description"},{"location":"sql/api/drop/#syntax","text":"Here is the syntax: DROP MODEL [ predictor_name ]; On execution, we get: Query OK , 0 rows affected ( 0 . 058 sec ) Where: Name Description [predictor_name] Name of the model to be deleted.","title":"Syntax"},{"location":"sql/api/drop/#example","text":"Let's list all the available predictor tables. SELECT name FROM mindsdb . models ; On execution, we get: + ---------------------+ | name | + ---------------------+ | other_model | | home_rentals_model | + ---------------------+ Now we delete the home_rentals_model table. DROP MODEL home_rentals_model ; On execution, we get: Query OK , 0 rows affected ( 0 . 058 sec ) We can check if the deletion was successful by querying the mindsdb.models table again. SELECT name FROM mindsdb . models ; On execution, we get: + ---------------------+ | name | + ---------------------+ | other_model | + ---------------------+","title":"Example"},{"location":"sql/api/insert/","text":"INSERT INTO Statement \u00b6 Description \u00b6 The INSERT INTO statement inserts data into a table. The data comes from a subselect query. It is commonly used to input prediction results into a database table. Syntax \u00b6 Here is the syntax: INSERT INTO [ integration_name ].[ table_name ] ( SELECT ...); Please note that the destination table ( [integration_name].[table_name] ) must exist and contain all the columns where the data is to be inserted. And the steps followed by the syntax: It executes a subselect query to get the output dataset. It uses the INSERT INTO statement to insert the output of the ( SELECT ...) query into the [integration_name].[table_name] table. On execution, we get: Query OK , 0 row ( s ) updated - x . xxxs Example \u00b6 We want to save the prediction results into the int1 . tbl1 table. Here is the schema structure used throughout this example: int1 \u2514\u2500\u2500 tbl1 mindsdb \u2514\u2500\u2500 predictor_name int2 \u2514\u2500\u2500 tbl2 Where: Name Description int1 Integration where the table that stores prediction results resides. tbl1 Table that stores prediction results. predictor_name Name of the model. int2 Integration where the data source table used in the inner SELECT statement resides. tbl2 Data source table used in the inner SELECT statement. Let's execute the query. INSERT INTO int1 . tbl1 ( SELECT * FROM int2 . tbl2 AS ta JOIN mindsdb . predictor_name AS tb WHERE ta . date > '2015-12-31' ); On execution, we get: Query OK , 0 row ( s ) updated - x . xxxs","title":"INSERT INTO"},{"location":"sql/api/insert/#insert-into-statement","text":"","title":"INSERT INTO Statement"},{"location":"sql/api/insert/#description","text":"The INSERT INTO statement inserts data into a table. The data comes from a subselect query. It is commonly used to input prediction results into a database table.","title":"Description"},{"location":"sql/api/insert/#syntax","text":"Here is the syntax: INSERT INTO [ integration_name ].[ table_name ] ( SELECT ...); Please note that the destination table ( [integration_name].[table_name] ) must exist and contain all the columns where the data is to be inserted. And the steps followed by the syntax: It executes a subselect query to get the output dataset. It uses the INSERT INTO statement to insert the output of the ( SELECT ...) query into the [integration_name].[table_name] table. On execution, we get: Query OK , 0 row ( s ) updated - x . xxxs","title":"Syntax"},{"location":"sql/api/insert/#example","text":"We want to save the prediction results into the int1 . tbl1 table. Here is the schema structure used throughout this example: int1 \u2514\u2500\u2500 tbl1 mindsdb \u2514\u2500\u2500 predictor_name int2 \u2514\u2500\u2500 tbl2 Where: Name Description int1 Integration where the table that stores prediction results resides. tbl1 Table that stores prediction results. predictor_name Name of the model. int2 Integration where the data source table used in the inner SELECT statement resides. tbl2 Data source table used in the inner SELECT statement. Let's execute the query. INSERT INTO int1 . tbl1 ( SELECT * FROM int2 . tbl2 AS ta JOIN mindsdb . predictor_name AS tb WHERE ta . date > '2015-12-31' ); On execution, we get: Query OK , 0 row ( s ) updated - x . xxxs","title":"Example"},{"location":"sql/api/join/","text":"JOIN Statement \u00b6 Description \u00b6 The JOIN clause combines rows from the database table and the model table on a column defined in its implementation. It is used to make bulk predictions, as shown in the examples. Syntax \u00b6 Here is the syntax: SELECT t .[ column_name ], p .[ column_name ] ... FROM [ integration_name ].[ table_name ] AS t JOIN mindsdb .[ predictor_name ] AS p ; On execution, we get: + -----------------+-----------------+ | t .[ column_name ] | p .[ column_name ] | + -----------------+-----------------+ | t .[ value ] | p .[ value ] | + -----------------+-----------------+ Where: Name Description [integration_name].[table_name] Name of the data source table used as input for making predictions. mindsdb.[predictor_name] Name of the model table used to make predictions. p.value Predicted value stored in the output table. Example 1 \u00b6 Let's join the home_rentals table with the home_rentals_model model using this statement: SELECT t . rental_price AS real_price , m . rental_price AS predicted_price , t . number_of_rooms , t . number_of_bathrooms , t . sqft , t . location , t . days_on_market FROM example_db . demo_data . home_rentals AS t JOIN mindsdb . home_rentals_model AS m LIMIT 100 ; On execution, we get: + ------------+-----------------+-----------------+---------------------+------+----------+----------------+ | real_price | predicted_price | number_of_rooms | number_of_bathrooms | sqft | location | days_on_market | + ------------+-----------------+-----------------+---------------------+------+----------+----------------+ | 3901 | 3886 | 2 | 1 | 917 | great | 13 | | 2042 | 2007 | 0 | 1 | 194 | great | 10 | | 1871 | 1865 | 1 | 1 | 543 | poor | 18 | | 3026 | 3020 | 2 | 1 | 503 | good | 10 | | 4774 | 4748 | 3 | 2 | 1066 | good | 13 | | 4382 | 4388 | 3 | 2 | 816 | poor | 25 | | 2269 | 2272 | 0 | 1 | 461 | great | 6 | | 2284 | 2272 | 1 | 1 | 333 | great | 6 | | 5420 | 5437 | 3 | 2 | 1124 | great | 9 | | 5016 | 4998 | 3 | 2 | 1204 | good | 7 | | 1421 | 1427 | 0 | 1 | 538 | poor | 43 | | 3476 | 3466 | 2 | 1 | 890 | good | 6 | | 5271 | 5255 | 3 | 2 | 975 | great | 6 | | 3001 | 2993 | 2 | 1 | 564 | good | 13 | | 4682 | 4692 | 3 | 2 | 953 | good | 10 | | 1783 | 1738 | 1 | 1 | 493 | poor | 24 | | 1548 | 1543 | 1 | 1 | 601 | poor | 47 | | 1492 | 1491 | 0 | 1 | 191 | good | 12 | | 2431 | 2419 | 0 | 1 | 511 | great | 1 | | 4237 | 4257 | 3 | 2 | 916 | poor | 36 | + ------------+-----------------+-----------------+---------------------+------+----------+----------------+ Example 2 \u00b6 Let's create and train a time series predictor using this statement: CREATE MODEL mindsdb . house_sales_model FROM example_db ( SELECT * FROM demo_data . house_sales ) PREDICT MA ORDER BY saledate GROUP BY bedrooms , type -- the target column to be predicted stores one row per quarter WINDOW 8 -- using data from the last two years to make forecasts (last 8 rows) HORIZON 4 ; -- making forecasts for the next year (next 4 rows) On execution, we get: Query OK , 0 rows affected ( x . xxx sec ) Now, you can query it to get the predictions like this: SELECT m . saledate as date , m . ma AS forecast FROM mindsdb . house_sales_model AS m JOIN example_db . demo_data . house_sales AS t WHERE t . saledate > LATEST AND t . type = 'house' LIMIT 4 ; On execution, we get: + ----------+------------------+ | date | forecast | + ----------+------------------+ | 2019 - 12 - 31 | 517506 . 31349071994 | | 2019 - 12 - 31 | 627822 . 6592658638 | | 2019 - 12 - 31 | 953426 . 9545788583 | | 2019 - 12 - 31 | 767252 . 4205039773 | + ----------+------------------+","title":"JOIN"},{"location":"sql/api/join/#join-statement","text":"","title":"JOIN Statement"},{"location":"sql/api/join/#description","text":"The JOIN clause combines rows from the database table and the model table on a column defined in its implementation. It is used to make bulk predictions, as shown in the examples.","title":"Description"},{"location":"sql/api/join/#syntax","text":"Here is the syntax: SELECT t .[ column_name ], p .[ column_name ] ... FROM [ integration_name ].[ table_name ] AS t JOIN mindsdb .[ predictor_name ] AS p ; On execution, we get: + -----------------+-----------------+ | t .[ column_name ] | p .[ column_name ] | + -----------------+-----------------+ | t .[ value ] | p .[ value ] | + -----------------+-----------------+ Where: Name Description [integration_name].[table_name] Name of the data source table used as input for making predictions. mindsdb.[predictor_name] Name of the model table used to make predictions. p.value Predicted value stored in the output table.","title":"Syntax"},{"location":"sql/api/join/#example-1","text":"Let's join the home_rentals table with the home_rentals_model model using this statement: SELECT t . rental_price AS real_price , m . rental_price AS predicted_price , t . number_of_rooms , t . number_of_bathrooms , t . sqft , t . location , t . days_on_market FROM example_db . demo_data . home_rentals AS t JOIN mindsdb . home_rentals_model AS m LIMIT 100 ; On execution, we get: + ------------+-----------------+-----------------+---------------------+------+----------+----------------+ | real_price | predicted_price | number_of_rooms | number_of_bathrooms | sqft | location | days_on_market | + ------------+-----------------+-----------------+---------------------+------+----------+----------------+ | 3901 | 3886 | 2 | 1 | 917 | great | 13 | | 2042 | 2007 | 0 | 1 | 194 | great | 10 | | 1871 | 1865 | 1 | 1 | 543 | poor | 18 | | 3026 | 3020 | 2 | 1 | 503 | good | 10 | | 4774 | 4748 | 3 | 2 | 1066 | good | 13 | | 4382 | 4388 | 3 | 2 | 816 | poor | 25 | | 2269 | 2272 | 0 | 1 | 461 | great | 6 | | 2284 | 2272 | 1 | 1 | 333 | great | 6 | | 5420 | 5437 | 3 | 2 | 1124 | great | 9 | | 5016 | 4998 | 3 | 2 | 1204 | good | 7 | | 1421 | 1427 | 0 | 1 | 538 | poor | 43 | | 3476 | 3466 | 2 | 1 | 890 | good | 6 | | 5271 | 5255 | 3 | 2 | 975 | great | 6 | | 3001 | 2993 | 2 | 1 | 564 | good | 13 | | 4682 | 4692 | 3 | 2 | 953 | good | 10 | | 1783 | 1738 | 1 | 1 | 493 | poor | 24 | | 1548 | 1543 | 1 | 1 | 601 | poor | 47 | | 1492 | 1491 | 0 | 1 | 191 | good | 12 | | 2431 | 2419 | 0 | 1 | 511 | great | 1 | | 4237 | 4257 | 3 | 2 | 916 | poor | 36 | + ------------+-----------------+-----------------+---------------------+------+----------+----------------+","title":"Example 1"},{"location":"sql/api/join/#example-2","text":"Let's create and train a time series predictor using this statement: CREATE MODEL mindsdb . house_sales_model FROM example_db ( SELECT * FROM demo_data . house_sales ) PREDICT MA ORDER BY saledate GROUP BY bedrooms , type -- the target column to be predicted stores one row per quarter WINDOW 8 -- using data from the last two years to make forecasts (last 8 rows) HORIZON 4 ; -- making forecasts for the next year (next 4 rows) On execution, we get: Query OK , 0 rows affected ( x . xxx sec ) Now, you can query it to get the predictions like this: SELECT m . saledate as date , m . ma AS forecast FROM mindsdb . house_sales_model AS m JOIN example_db . demo_data . house_sales AS t WHERE t . saledate > LATEST AND t . type = 'house' LIMIT 4 ; On execution, we get: + ----------+------------------+ | date | forecast | + ----------+------------------+ | 2019 - 12 - 31 | 517506 . 31349071994 | | 2019 - 12 - 31 | 627822 . 6592658638 | | 2019 - 12 - 31 | 953426 . 9545788583 | | 2019 - 12 - 31 | 767252 . 4205039773 | + ----------+------------------+","title":"Example 2"},{"location":"sql/api/retrain/","text":"RETRAIN Statement \u00b6 Description \u00b6 The RETRAIN statement is used to retrain the already trained predictors with the new data. The predictor is updated to leverage the new data in optimizing its predictive capabilities. Retraining takes at least as much time as the training process of the predictor did because now the dataset used to retrain has new or updated data. Syntax \u00b6 Here is the syntax: RETRAIN mindsdb .[ predictor_name ]; On execution, we get: Query OK , 0 rows affected ( 0 . 058 sec ) When to RETRAIN the Model? \u00b6 It is advised to RETRAIN the predictor whenever the update_status column value from the mindsdb.models table is set to available . Here is when the update_status column value is set to available : When the new version of MindsDB is available that causes the predictor to become obsolete. When the new data is available in the table that was used to train the predictor. To find out whether you need to retrain your model, query the mindsdb.models table and look for the update_status column. Here are the possible values of the update_status column: Name Description available It indicates that the model should be updated. updating It indicates that the retraining process of the model takes place. up_to_date It indicates that your model is up to date and does not need to be retrained. Let's run the query. SELECT name , update_status FROM mindsdb . models WHERE name = '[predictor_name]' ; On execution, we get: + ------------------+---------------+ | name | update_status | + ------------------+---------------+ | [ predictor_name ] | up_to_date | + ------------------+---------------+ Where: Name Description [predictor_name] Name of the model to be retrained. update_status Column informing whether the model needs to be retrained. Example \u00b6 Let's look at an example using the home_rentals_model model. First, we check the status of the predictor. SELECT name , update_status FROM mindsdb . models WHERE name = 'home_rentals_model' ; On execution, we get: + --------------------+---------------+ | name | update_status | + --------------------+---------------+ | home_rentals_model | available | + --------------------+---------------+ The available value of the update_status column informs us that the new data is available, and we can retrain the model. RETRAIN mindsdb . home_rentals_model ; On execution, we get: Query OK , 0 rows affected ( 0 . 058 sec ) Now, let's check the status again. SELECT name , update_status FROM mindsdb . models WHERE name = 'home_rentals_model' ; On execution, we get: + --------------------+---------------+ | name | update_status | + --------------------+---------------+ | home_rentals_model | updating | + --------------------+---------------+ And after the retraining process is completed: SELECT name , update_status FROM mindsdb . models WHERE name = 'home_rentals_model' ; On execution, we get: + --------------------+---------------+ | name | update_status | + --------------------+---------------+ | home_rentals_model | up_to_date | + --------------------+---------------+","title":"RETRAIN"},{"location":"sql/api/retrain/#retrain-statement","text":"","title":"RETRAIN Statement"},{"location":"sql/api/retrain/#description","text":"The RETRAIN statement is used to retrain the already trained predictors with the new data. The predictor is updated to leverage the new data in optimizing its predictive capabilities. Retraining takes at least as much time as the training process of the predictor did because now the dataset used to retrain has new or updated data.","title":"Description"},{"location":"sql/api/retrain/#syntax","text":"Here is the syntax: RETRAIN mindsdb .[ predictor_name ]; On execution, we get: Query OK , 0 rows affected ( 0 . 058 sec )","title":"Syntax"},{"location":"sql/api/retrain/#when-to-retrain-the-model","text":"It is advised to RETRAIN the predictor whenever the update_status column value from the mindsdb.models table is set to available . Here is when the update_status column value is set to available : When the new version of MindsDB is available that causes the predictor to become obsolete. When the new data is available in the table that was used to train the predictor. To find out whether you need to retrain your model, query the mindsdb.models table and look for the update_status column. Here are the possible values of the update_status column: Name Description available It indicates that the model should be updated. updating It indicates that the retraining process of the model takes place. up_to_date It indicates that your model is up to date and does not need to be retrained. Let's run the query. SELECT name , update_status FROM mindsdb . models WHERE name = '[predictor_name]' ; On execution, we get: + ------------------+---------------+ | name | update_status | + ------------------+---------------+ | [ predictor_name ] | up_to_date | + ------------------+---------------+ Where: Name Description [predictor_name] Name of the model to be retrained. update_status Column informing whether the model needs to be retrained.","title":"When to RETRAIN the Model?"},{"location":"sql/api/retrain/#example","text":"Let's look at an example using the home_rentals_model model. First, we check the status of the predictor. SELECT name , update_status FROM mindsdb . models WHERE name = 'home_rentals_model' ; On execution, we get: + --------------------+---------------+ | name | update_status | + --------------------+---------------+ | home_rentals_model | available | + --------------------+---------------+ The available value of the update_status column informs us that the new data is available, and we can retrain the model. RETRAIN mindsdb . home_rentals_model ; On execution, we get: Query OK , 0 rows affected ( 0 . 058 sec ) Now, let's check the status again. SELECT name , update_status FROM mindsdb . models WHERE name = 'home_rentals_model' ; On execution, we get: + --------------------+---------------+ | name | update_status | + --------------------+---------------+ | home_rentals_model | updating | + --------------------+---------------+ And after the retraining process is completed: SELECT name , update_status FROM mindsdb . models WHERE name = 'home_rentals_model' ; On execution, we get: + --------------------+---------------+ | name | update_status | + --------------------+---------------+ | home_rentals_model | up_to_date | + --------------------+---------------+","title":"Example"},{"location":"sql/api/select/","text":"SELECT statement \u00b6 Description \u00b6 The SELECT statement fetches predictions from the model table. The data is returned on the fly and not saved. But there are ways to save predictions data! You can save your predictions as a view using the CREATE VIEW statement. Please note that a view is a saved query and does not store data like a table. Another way is to insert your predictions into a table using the INSERT INTO statement. Syntax \u00b6 Single Prediction \u00b6 Here is the syntax for fetching a single prediction from the model table: SELECT [ target_variable ], [ target_variable ] _explain FROM mindsdb .[ predictor_name ] WHERE [ column ] = [ value ] AND [ column ] = [ value ]; Grammar Matters Here are some points to keep in mind while writing queries in MindsDB: 1. The [column]=[value] pairs may be joined by AND or OR keywords. 2. Do not use any quotations for numerical values. 3. Use single quotes for strings. On execution, we get: + -------------------+-----------------------------------------------------------------------------------------------------------------------------------------------+ | [ target_variable ] | [ target_variable ] _explain | + -------------------+-----------------------------------------------------------------------------------------------------------------------------------------------+ | [ predicted_value ] | { \"predicted_value\" : 4394 , \"confidence\" : 0 . 99 , \"anomaly\" : null , \"truth\" : null , \"confidence_lower_bound\" : 4313 , \"confidence_upper_bound\" : 4475 } | + -------------------+-----------------------------------------------------------------------------------------------------------------------------------------------+ Where: Name Description [target_variable] Name of the column to be predicted. [target_variable]_explain Object of the JSON type that contains the predicted_value and additional information such as confidence , anomaly , truth , confidence_lower_bound , confidence_upper_bound . [predictor_name] Name of the model used to make the prediction. WHERE [ column ] = [ value ] AND ... WHERE clause used to pass the data values for which the prediction is made. Bulk Predictions \u00b6 Here is the syntax for making predictions in bulk by joining the data source table with the model table: SELECT m .[ target_variable ], t .[ column1 ], t .[ column2 ] FROM [ integration_name ].[ table_name ] AS t JOIN mindsdb .[ predictor_name ] AS m ; On execution, we get: + ----------------------+-------------+-------------+ | [ target_variable ] | [ column1 ] | [ column2 ] | + ----------------------+-------------+-------------+ | [ predicted_value_1 ] | value1 . 1 | value2 . 1 | | [ predicted_value_2 ] | value1 . 2 | value2 . 2 | | [ predicted_value_3 ] | value1 . 3 | value2 . 3 | | [ predicted_value_4 ] | value1 . 4 | value2 . 4 | | [ predicted_value_5 ] | value1 . 5 | value2 . 5 | + ----------------------+-------------+-------------+ Where: Name Description m.[target_variable] Name of the column to be predicted. The m. in front indicates that this column comes from the mindsdb.[predictor_name] table. t.[column1], t.[column2] Columns from the data source table ( [integration_name].[table_name] ) that you want to see in the output. [integration_name].[table_name] Data source table that is joined with the model table ( mindsdb.[predictor_name] ). [predictor_name] Name of the model used to make predictions. Please note that in the case of bulk predictions, we do not pass the data values for which the prediction is made. It is because bulk predictions use all data available in the data source table. Example \u00b6 Single Prediction \u00b6 Let's predict the rental_price value using the home_rentals_model model for the property having sqft=823 , location='good' , neighborhood='downtown' , and days_on_market=10 . SELECT sqft , location , neighborhood , days_on_market , rental_price , rental_price_explain FROM mindsdb . home_rentals_model1 WHERE sqft = 823 AND location = 'good' AND neighborhood = 'downtown' AND days_on_market = 10 ; On execution, we get: + -------+----------+--------------+----------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------+ | sqft | location | neighborhood | days_on_market | rental_price | rental_price_explain | + -------+----------+--------------+----------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------+ | 823 | good | downtown | 10 | 4394 | { \"predicted_value\" : 4394 , \"confidence\" : 0 . 99 , \"anomaly\" : null , \"truth\" : null , \"confidence_lower_bound\" : 4313 , \"confidence_upper_bound\" : 4475 } | + -------+----------+--------------+----------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------+ Bulk Predictions \u00b6 Now let's make bulk predictions to predict the rental_price value using the home_rentals_model model joined with the data source table. SELECT t . sqft , t . location , t . neighborhood , t . days_on_market , t . rental_price AS real_price , m . rental_price AS predicted_rental_price FROM example_db . demo_data . home_rentals AS t JOIN mindsdb . home_rentals_model AS m LIMIT 5 ; On execution, we get: + -------+----------+-----------------+----------------+--------------+-----------------------------+ | sqft | location | neighborhood | days_on_market | real_price | predicted_rental_price | + -------+----------+-----------------+----------------+--------------+-----------------------------+ | 917 | great | berkeley_hills | 13 | 3901 | 3886 | | 194 | great | berkeley_hills | 10 | 2042 | 2007 | | 543 | poor | westbrae | 18 | 1871 | 1865 | | 503 | good | downtown | 10 | 3026 | 3020 | | 1066 | good | thowsand_oaks | 13 | 4774 | 4748 | + -------+----------+-----------------+----------------+--------------+-----------------------------+ Select from integration \u00b6 Simple select \u00b6 In this example query contains only tables from one integration and therefore will be sent to integration database (integration name will be cut from table name) SELECT location , max ( sqft ) FROM example_db . demo_data . home_rentals GROUP BY location LIMIT 5 ; Raw select from integration \u00b6 It is also possible to send raw query to integration. It can be useful when query to integration can not be parsed as sql Syntax: SELECT ... FROM < integration_name > ( < raw query > ) Example of select from mongo integration using mongo query SELECT * FROM mongo ( db . house_sales2 . find (). limit ( 1 ) ) Complex queries \u00b6 Subselect on data from integration. It can be useful in cases when integration engine doesn't support some functions, for example grouping. In that case all data from raw select are passed to mindsdb and then subselect performs on them inside mindsdb SELECT type , max ( bedrooms ), last ( MA ) FROM mongo ( db . house_sales2 . find (). limit ( 300 ) ) GROUP BY 1 Unions It is possible to use UNION / UNION ALL operators. It this case every subselect from union will be fetched and merged to one result-set on mindsdb side SELECT data . time as date , data . target FROM datasource . table_name as data UNION ALL SELECT model . time as date , model . target as target FROM mindsdb . model as model JOIN datasource . table_name as t WHERE t . time > LATEST AND t . group = 'value' ;","title":"SELECT"},{"location":"sql/api/select/#select-statement","text":"","title":"SELECT statement"},{"location":"sql/api/select/#description","text":"The SELECT statement fetches predictions from the model table. The data is returned on the fly and not saved. But there are ways to save predictions data! You can save your predictions as a view using the CREATE VIEW statement. Please note that a view is a saved query and does not store data like a table. Another way is to insert your predictions into a table using the INSERT INTO statement.","title":"Description"},{"location":"sql/api/select/#syntax","text":"","title":"Syntax"},{"location":"sql/api/select/#single-prediction","text":"Here is the syntax for fetching a single prediction from the model table: SELECT [ target_variable ], [ target_variable ] _explain FROM mindsdb .[ predictor_name ] WHERE [ column ] = [ value ] AND [ column ] = [ value ]; Grammar Matters Here are some points to keep in mind while writing queries in MindsDB: 1. The [column]=[value] pairs may be joined by AND or OR keywords. 2. Do not use any quotations for numerical values. 3. Use single quotes for strings. On execution, we get: + -------------------+-----------------------------------------------------------------------------------------------------------------------------------------------+ | [ target_variable ] | [ target_variable ] _explain | + -------------------+-----------------------------------------------------------------------------------------------------------------------------------------------+ | [ predicted_value ] | { \"predicted_value\" : 4394 , \"confidence\" : 0 . 99 , \"anomaly\" : null , \"truth\" : null , \"confidence_lower_bound\" : 4313 , \"confidence_upper_bound\" : 4475 } | + -------------------+-----------------------------------------------------------------------------------------------------------------------------------------------+ Where: Name Description [target_variable] Name of the column to be predicted. [target_variable]_explain Object of the JSON type that contains the predicted_value and additional information such as confidence , anomaly , truth , confidence_lower_bound , confidence_upper_bound . [predictor_name] Name of the model used to make the prediction. WHERE [ column ] = [ value ] AND ... WHERE clause used to pass the data values for which the prediction is made.","title":"Single Prediction"},{"location":"sql/api/select/#bulk-predictions","text":"Here is the syntax for making predictions in bulk by joining the data source table with the model table: SELECT m .[ target_variable ], t .[ column1 ], t .[ column2 ] FROM [ integration_name ].[ table_name ] AS t JOIN mindsdb .[ predictor_name ] AS m ; On execution, we get: + ----------------------+-------------+-------------+ | [ target_variable ] | [ column1 ] | [ column2 ] | + ----------------------+-------------+-------------+ | [ predicted_value_1 ] | value1 . 1 | value2 . 1 | | [ predicted_value_2 ] | value1 . 2 | value2 . 2 | | [ predicted_value_3 ] | value1 . 3 | value2 . 3 | | [ predicted_value_4 ] | value1 . 4 | value2 . 4 | | [ predicted_value_5 ] | value1 . 5 | value2 . 5 | + ----------------------+-------------+-------------+ Where: Name Description m.[target_variable] Name of the column to be predicted. The m. in front indicates that this column comes from the mindsdb.[predictor_name] table. t.[column1], t.[column2] Columns from the data source table ( [integration_name].[table_name] ) that you want to see in the output. [integration_name].[table_name] Data source table that is joined with the model table ( mindsdb.[predictor_name] ). [predictor_name] Name of the model used to make predictions. Please note that in the case of bulk predictions, we do not pass the data values for which the prediction is made. It is because bulk predictions use all data available in the data source table.","title":"Bulk Predictions"},{"location":"sql/api/select/#example","text":"","title":"Example"},{"location":"sql/api/select/#single-prediction_1","text":"Let's predict the rental_price value using the home_rentals_model model for the property having sqft=823 , location='good' , neighborhood='downtown' , and days_on_market=10 . SELECT sqft , location , neighborhood , days_on_market , rental_price , rental_price_explain FROM mindsdb . home_rentals_model1 WHERE sqft = 823 AND location = 'good' AND neighborhood = 'downtown' AND days_on_market = 10 ; On execution, we get: + -------+----------+--------------+----------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------+ | sqft | location | neighborhood | days_on_market | rental_price | rental_price_explain | + -------+----------+--------------+----------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------+ | 823 | good | downtown | 10 | 4394 | { \"predicted_value\" : 4394 , \"confidence\" : 0 . 99 , \"anomaly\" : null , \"truth\" : null , \"confidence_lower_bound\" : 4313 , \"confidence_upper_bound\" : 4475 } | + -------+----------+--------------+----------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------+","title":"Single Prediction"},{"location":"sql/api/select/#bulk-predictions_1","text":"Now let's make bulk predictions to predict the rental_price value using the home_rentals_model model joined with the data source table. SELECT t . sqft , t . location , t . neighborhood , t . days_on_market , t . rental_price AS real_price , m . rental_price AS predicted_rental_price FROM example_db . demo_data . home_rentals AS t JOIN mindsdb . home_rentals_model AS m LIMIT 5 ; On execution, we get: + -------+----------+-----------------+----------------+--------------+-----------------------------+ | sqft | location | neighborhood | days_on_market | real_price | predicted_rental_price | + -------+----------+-----------------+----------------+--------------+-----------------------------+ | 917 | great | berkeley_hills | 13 | 3901 | 3886 | | 194 | great | berkeley_hills | 10 | 2042 | 2007 | | 543 | poor | westbrae | 18 | 1871 | 1865 | | 503 | good | downtown | 10 | 3026 | 3020 | | 1066 | good | thowsand_oaks | 13 | 4774 | 4748 | + -------+----------+-----------------+----------------+--------------+-----------------------------+","title":"Bulk Predictions"},{"location":"sql/api/select/#select-from-integration","text":"","title":"Select from integration"},{"location":"sql/api/select/#simple-select","text":"In this example query contains only tables from one integration and therefore will be sent to integration database (integration name will be cut from table name) SELECT location , max ( sqft ) FROM example_db . demo_data . home_rentals GROUP BY location LIMIT 5 ;","title":"Simple select"},{"location":"sql/api/select/#raw-select-from-integration","text":"It is also possible to send raw query to integration. It can be useful when query to integration can not be parsed as sql Syntax: SELECT ... FROM < integration_name > ( < raw query > ) Example of select from mongo integration using mongo query SELECT * FROM mongo ( db . house_sales2 . find (). limit ( 1 ) )","title":"Raw select from integration"},{"location":"sql/api/select/#complex-queries","text":"Subselect on data from integration. It can be useful in cases when integration engine doesn't support some functions, for example grouping. In that case all data from raw select are passed to mindsdb and then subselect performs on them inside mindsdb SELECT type , max ( bedrooms ), last ( MA ) FROM mongo ( db . house_sales2 . find (). limit ( 300 ) ) GROUP BY 1 Unions It is possible to use UNION / UNION ALL operators. It this case every subselect from union will be fetched and merged to one result-set on mindsdb side SELECT data . time as date , data . target FROM datasource . table_name as data UNION ALL SELECT model . time as date , model . target as target FROM mindsdb . model as model JOIN datasource . table_name as t WHERE t . time > LATEST AND t . group = 'value' ;","title":"Complex queries"},{"location":"sql/api/select_files/","text":"SELECT * FROM files .[ file_name ] Statement \u00b6 Description \u00b6 The SELECT * FROM files .[ file_name ] statement is used to select data from a file. First, you upload a file to the MindsDB Cloud Editor by following this guide . And then, you can CREATE MODEL from the uploaded file. Syntax \u00b6 Here is the syntax: SELECT * FROM files .[ file_name ]; On execution, we get: + --------+--------+--------+--------+ | column | column | column | column | + --------+--------+--------+--------+ | value | value | value | value | + --------+--------+--------+--------+ Where: Name Description [file_name] Name of the file uploaded to the MindsDB Cloud Editor by following this guide . column Name of the column from the file. Example \u00b6 Once you uploaded your file by following this guide , you can query it like a table. SELECT * FROM files . home_rentals LIMIT 10 ; On execution, we get: + -----------------+---------------------+-------+----------+----------------+---------------+--------------+--------------+ | number_of_rooms | number_of_bathrooms | sqft | location | days_on_market | initial_price | neighborhood | rental_price | + -----------------+---------------------+-------+----------+----------------+---------------+--------------+--------------+ | 0 | 1 | 484 , 8 | great | 10 | 2271 | south_side | 2271 | | 1 | 1 | 674 | good | 1 | 2167 | downtown | 2167 | | 1 | 1 | 554 | poor | 19 | 1883 | westbrae | 1883 | | 0 | 1 | 529 | great | 3 | 2431 | south_side | 2431 | | 3 | 2 | 1219 | great | 3 | 5510 | south_side | 5510 | | 1 | 1 | 398 | great | 11 | 2272 | south_side | 2272 | | 3 | 2 | 1190 | poor | 58 | 4463 | westbrae | 4123 . 812 | | 1 | 1 | 730 | good | 0 | 2224 | downtown | 2224 | | 0 | 1 | 298 | great | 9 | 2104 | south_side | 2104 | | 2 | 1 | 878 | great | 8 | 3861 | south_side | 3861 | + -----------------+---------------------+-------+----------+----------------+---------------+--------------+--------------+ Now let's create a predictor using the uploaded file. You can learn more about the CREATE MODEL command here . CREATE MODEL mindsdb . home_rentals_model FROM files ( SELECT * from home_rentals ) PREDICT rental_price ; On execution, we get: Query OK , 0 rows affected ( x . xxx sec )","title":"SELECT from files"},{"location":"sql/api/select_files/#select-from-filesfile_name-statement","text":"","title":"SELECT * FROM files.[file_name] Statement"},{"location":"sql/api/select_files/#description","text":"The SELECT * FROM files .[ file_name ] statement is used to select data from a file. First, you upload a file to the MindsDB Cloud Editor by following this guide . And then, you can CREATE MODEL from the uploaded file.","title":"Description"},{"location":"sql/api/select_files/#syntax","text":"Here is the syntax: SELECT * FROM files .[ file_name ]; On execution, we get: + --------+--------+--------+--------+ | column | column | column | column | + --------+--------+--------+--------+ | value | value | value | value | + --------+--------+--------+--------+ Where: Name Description [file_name] Name of the file uploaded to the MindsDB Cloud Editor by following this guide . column Name of the column from the file.","title":"Syntax"},{"location":"sql/api/select_files/#example","text":"Once you uploaded your file by following this guide , you can query it like a table. SELECT * FROM files . home_rentals LIMIT 10 ; On execution, we get: + -----------------+---------------------+-------+----------+----------------+---------------+--------------+--------------+ | number_of_rooms | number_of_bathrooms | sqft | location | days_on_market | initial_price | neighborhood | rental_price | + -----------------+---------------------+-------+----------+----------------+---------------+--------------+--------------+ | 0 | 1 | 484 , 8 | great | 10 | 2271 | south_side | 2271 | | 1 | 1 | 674 | good | 1 | 2167 | downtown | 2167 | | 1 | 1 | 554 | poor | 19 | 1883 | westbrae | 1883 | | 0 | 1 | 529 | great | 3 | 2431 | south_side | 2431 | | 3 | 2 | 1219 | great | 3 | 5510 | south_side | 5510 | | 1 | 1 | 398 | great | 11 | 2272 | south_side | 2272 | | 3 | 2 | 1190 | poor | 58 | 4463 | westbrae | 4123 . 812 | | 1 | 1 | 730 | good | 0 | 2224 | downtown | 2224 | | 0 | 1 | 298 | great | 9 | 2104 | south_side | 2104 | | 2 | 1 | 878 | great | 8 | 3861 | south_side | 3861 | + -----------------+---------------------+-------+----------+----------------+---------------+--------------+--------------+ Now let's create a predictor using the uploaded file. You can learn more about the CREATE MODEL command here . CREATE MODEL mindsdb . home_rentals_model FROM files ( SELECT * from home_rentals ) PREDICT rental_price ; On execution, we get: Query OK , 0 rows affected ( x . xxx sec )","title":"Example"},{"location":"sql/api/stream/","text":"Work in progress This documentation is in progress. If you want to get access to the beta version, reach out to us on Slack .","title":"Stream"},{"location":"sql/api/update/","text":"UPDATE FROM SELECT Statement \u00b6 Description \u00b6 The UPDATE FROM SELECT statement updates data in existing table. The data comes from a subselect query. It can be used as alternative to 'create table' and 'insert into' for store predictions in distinct columns of existing rows Syntax \u00b6 Here is an example: UPDATE int2 . table2 SET predicted = source . result , FROM ( SELECT p . result , p . prod_id , p . shop_id FROM int1 . table1 as t JOIN mindsdb . pred1 as p ) AS source WHERE prod_id = source . prod_id and shop_id = source . shop_id And the steps followed by the syntax: It executes query from 'FROM' block to get the output dataset. In our example it is join of table table1 (from integration int1 ) with predictor pred1 . It also can be select from integration source is the alias for fetched data then it updates table2 from int2 using conditions from WHERE block and fields for update from SET block under the hood it splits input data to rows and execute this query for every row: UPDATE table2 SET predicted = < row . result > , WHERE prod_id = < row . prod_id > and shop_id = < row . shop_id > Note: in WHERE block it is better to use primary key for table or set of rows that can be a primary key (and identifies every row in table). Otherwise, it can lead to unexpected results when one row in destination table was updated several times from different rows in source table (because conditions from different rows are fit).","title":"UPDATE"},{"location":"sql/api/update/#update-from-select-statement","text":"","title":"UPDATE FROM SELECT Statement"},{"location":"sql/api/update/#description","text":"The UPDATE FROM SELECT statement updates data in existing table. The data comes from a subselect query. It can be used as alternative to 'create table' and 'insert into' for store predictions in distinct columns of existing rows","title":"Description"},{"location":"sql/api/update/#syntax","text":"Here is an example: UPDATE int2 . table2 SET predicted = source . result , FROM ( SELECT p . result , p . prod_id , p . shop_id FROM int1 . table1 as t JOIN mindsdb . pred1 as p ) AS source WHERE prod_id = source . prod_id and shop_id = source . shop_id And the steps followed by the syntax: It executes query from 'FROM' block to get the output dataset. In our example it is join of table table1 (from integration int1 ) with predictor pred1 . It also can be select from integration source is the alias for fetched data then it updates table2 from int2 using conditions from WHERE block and fields for update from SET block under the hood it splits input data to rows and execute this query for every row: UPDATE table2 SET predicted = < row . result > , WHERE prod_id = < row . prod_id > and shop_id = < row . shop_id > Note: in WHERE block it is better to use primary key for table or set of rows that can be a primary key (and identifies every row in table). Otherwise, it can lead to unexpected results when one row in destination table was updated several times from different rows in source table (because conditions from different rows are fit).","title":"Syntax"},{"location":"sql/api/use/","text":"USE statement \u00b6 The use integration_name statement provides an option to use the connected datasources and SELECT from the database tables. Even if you are connecting to MindsDB as MySQL database, you will still be able to preview or SELECT from your database. If you haven't created a datasource after connecting to your database check out the simple steps explained here . Preview the data \u00b6 To connect to your database use the created datasource: use integration_name Then, simply SELECT from the tables: SELECT * FROM table_name ;","title":"USE statement"},{"location":"sql/api/use/#use-statement","text":"The use integration_name statement provides an option to use the connected datasources and SELECT from the database tables. Even if you are connecting to MindsDB as MySQL database, you will still be able to preview or SELECT from your database. If you haven't created a datasource after connecting to your database check out the simple steps explained here .","title":"USE statement"},{"location":"sql/api/use/#preview-the-data","text":"To connect to your database use the created datasource: use integration_name Then, simply SELECT from the tables: SELECT * FROM table_name ;","title":"Preview the data"},{"location":"sql/create/databases/","text":"CREATE DATABASE Statement \u00b6 Description \u00b6 MindsDB lets you connect to your favorite databases, data warehouses, data lakes, etc., via the CREATE DATABASE command. The MindsDB SQL API supports creating connections to integrations by passing the connection parameters specific per integration. You can find more in the Supported Integrations chapter. Syntax \u00b6 Let's review the syntax for the CREATE DATABASE command. CREATE DATABASE datasource_name [ WITH ] [ ENGINE [ = ] engine_name ] [, PARAMETERS [ = ] { \"key\" : \"value\" , ... } ]; On execution, we get: Query OK , 0 rows affected ( x . xxx sec ) Where: Name Description [datasource_name] Identifier for the data source to be created. [engine_string] Engine to be selected depending on the database connection. PARAMETERS { \"key\" : \"value\" } object with the connection parameters specific for each engine. SQL Commands Resulting in the Same Output Please note that the keywords/statements enclosed within square brackets are optional. Also, by default, the engine is mindsdb if not provided otherwise. That yields the following SQL commands to result in the same output. CREATE DATABASE db ; CREATE DATABASE db ENGINE 'mindsdb' ; CREATE DATABASE db ENGINE = 'mindsdb' ; CREATE DATABASE db WITH ENGINE 'mindsdb' ; CREATE DATABASE db WITH ENGINE = 'mindsdb' ; Example \u00b6 Connecting a Data Source \u00b6 Here is an example of how to connect to a MySQL database. CREATE DATABASE mysql_datasource WITH ENGINE = 'mariadb' , PARAMETERS = { \"user\" : \"root\" , \"port\" : 3307 , \"password\" : \"password\" , \"host\" : \"127.0.0.1\" , \"database\" : \"my_database\" } ; On execution, we get: Query OK , 0 rows affected ( 8 . 878 sec ) Listing Linked Databases \u00b6 You can list all the linked databases using the command below. SHOW DATABASES ; On execution, we get: + --------------------+ | Database | + --------------------+ | information_schema | | mindsdb | | files | | mysql_datasource | + --------------------+ Getting Linked Databases Metadata \u00b6 You can get metadata about the linked databases by querying the mindsdb.datasources table. SELECT * FROM mindsdb . datasources ; On execution, we get: + ------------------+---------------+--------------+------+-----------+ | name | database_type | host | port | user | + ------------------+---------------+--------------+------+-----------+ | mysql_datasource | mysql | 3 . 220 . 66 . 106 | 3306 | root | + ------------------+---------------+--------------+------+-----------+ Making your Local Database Available to MindsDB \u00b6 When connecting your local database to MindsDB Cloud, you should expose the local database server to be publicly accessible. It is easy to accomplish using Ngrok Tunnel . The free tier offers all you need to get started. The installation instructions are easy to follow. Head over to the downloads page and choose your operating system. Follow the instructions for installation. Then create a free account at Ngrok to get an auth token that you can use to configure your Ngrok instance. Once installed and configured, run the following command to obtain the host and port for your localhost at [port-number] . ngrok tcp [ port-number ] Here is an example. Assuming that you run a PostgreSQL database at localhost:5432 , use the following command: ngrok tcp 5432 On execution, we get: Session Status online Account myaccount ( Plan: Free ) Version 2 .3.40 Region United States ( us ) Web Interface http://127.0.0.1:4040 Forwarding tcp://4.tcp.ngrok.io:15093 -> localhost 5432 Now you can access your local database at 4.tcp.ngrok.io:15093 instead of localhost:5432 . So to connect your local database to the MindsDB GUI, use the Forwarding information. The host is 4.tcp.ngrok.io , and the port is 15093 . Proceed to create a database connection in the MindsDB GUI by executing the CREATE DATABASE statement with the host and port number obtained from Ngrok. CREATE DATABASE psql_datasource WITH ENGINE = 'postgres' , PARAMETERS = { \"user\" : \"postgres\" , \"port\" : 15093 , \"password\" : \"password\" , \"host\" : \"4.tcp.ngrok.io\" , \"database\" : \"postgres\" } ; Please note that the Ngrok tunnel loses connection when stopped or canceled. To reconnect your local database to MindsDB, you should create an Ngrok tunnel again. In the free tier, Ngrok changes the host and port values each time you launch the program, so you need to reconnect your database in the MindsDB Cloud by passing the new host and port values obtained from Ngrok. Before resetting the database connection, drop the previously connected data source using the DROP DATABASE statement. DROP DATABASE psql_datasource ; After dropping the data source and reconnecting your local database, you can use the predictors that you trained using the previously connected data source. However, if you have to RETRAIN your predictors, please ensure the database connection has the same name you used when creating the predictor to avoid failing to retrain. Work in progress Please note that this feature is a beta version. If you have questions about the supported data sources or experience some issues, reach out to us on Slack or open a GitHub issue . Supported Integrations \u00b6 The list of databases supported by MindsDB keeps growing. Here are the currently supported integrations: You can find particular databases' handler files here to see their connection arguments. For example, to see the latest updates to the Oracle handler, check Oracle's readme.md file here . Let's look at sample codes showing how to connect to each of the supported integrations. Airtable \u00b6 Template Example CREATE DATABASE airtable_datasource --- display name for the database WITH ENGINE = 'airtable' , --- name of the MindsDB handler PARAMETERS = { \"base_id\" : \" \" , --- the Airtable base ID \"table_name\" : \" \" , --- the Airtable table name \"api_key\" : \" \" --- the API key for the Airtable API } ; CREATE DATABASE airtable_datasource WITH ENGINE = 'airtable' , PARAMETERS = { \"base_id\" : \"appve10klsda2\" , \"table_name\" : \"my_table\" , \"api_key\" : \"KdJX2Q5km%5b$T$sQYm^gvN\" } ; Amazon Redshift \u00b6 Template Example CREATE DATABASE amazonredshift_datasource --- display name for the database WITH ENGINE = 'amazonredshift' , --- name of the MindsDB handler PARAMETERS = { \"host\" : \" \" , --- host name or IP address of the Redshift cluster \"port\" : , --- port used when connecting to the Redshift cluster \"database\" : \" \" , --- database name used when connecting to the Redshift cluster \"user\" : \" \" , --- user to authenticate with the Redshift cluster \"password\" : \" \" --- password used to authenticate with the Redshift cluster } ; CREATE DATABASE amazonredshift_datasource WITH ENGINE = 'amazonredshift' , PARAMETERS = { \"host\" : \"127.0.0.1\" , \"port\" : 5439 , \"database\" : \"test\" , \"user\" : \"amazonredshift\" , \"password\" : \"password\" } ; AWS DynamoDB \u00b6 Template Example CREATE DATABASE dynamodb_datasource --- display name for the database WITH ENGINE = 'dynamodb' , --- name of the MindsDB handler PARAMETERS = { \"aws_access_key_id\" : \" \" , --- the AWS access key \"aws_secret_access_key\" : \" \" , --- the AWS secret access key \"region_name\" : \" \" --- the AWS region } ; CREATE DATABASE dynamodb_datasource WITH ENGINE = 'dynamodb' , PARAMETERS = { \"aws_access_key_id\" : \"PCAQ2LJDOSWLNSQKOCPW\" , \"aws_secret_access_key\" : \"U/VjewPlNopsDmmwItl34r2neyC6WhZpUiip57i\" , \"region_name\" : \"us-east-1\" } ; Cassandra \u00b6 Template Example CREATE DATABASE cassandra_datasource --- display name for the database WITH ENGINE = 'cassandra' , --- name of the MindsDB handler PARAMETERS = { \"host\" : \" \" , --- host name or IP address \"port\" : , --- port used to make TCP/IP connection \"user\" : \" \" , --- database user \"password\" : \" \" , --- database password \"keyspace\" : \" \" , --- database name \"protocol_version\" : , --- optional, protocol version (defaults to 4 if left blank) \"secure_connect_bundle\" : { --- optional, secure connect bundle file \"path\" : \" \" --- either \"path\" or \"url\" } } ; CREATE DATABASE cassandra_datasource WITH ENGINE = 'cassandra' , PARAMETERS = { \"host\" : \"127.0.0.1\" , \"port\" : 9043 , \"user\" : \"user\" , \"password\" : \"password\" , \"keyspace\" : \"test_data\" , \"protocol_version\" : 4 } ; Ckan \u00b6 Template Example CREATE DATABASE ckan_datasource --- display name for the database WITH ENGINE = 'ckan' , --- name of the MindsDB handler PARAMETERS = { \"url\" : \" \" , --- host name, IP address, or a URL \"apikey\" : \" \" --- the API key used for authentication } ; CREATE DATABASE ckan_datasource WITH ENGINE = 'ckan' , PARAMETERS = { \"url\" : \"http://demo.ckan.org/api/3/action/\" , \"apikey\" : \"YOUR_API_KEY\" } ; ClickHouse \u00b6 Template Example CREATE DATABASE clickhouse_datasource --- display name for the database WITH ENGINE = 'clickhouse' , --- name of the MindsDB handler PARAMETERS = { \"host\" : \" \" , --- host name or IP address \"port\" : , --- port used to make TCP/IP connection \"database\" : \" \" , --- database name \"user\" : \" \" , --- database user \"password\" : \" \" , --- database password \"protocol\" : \" \" --- optional, http or https (defaults to `native`) } ; CREATE DATABASE clickhouse_datasource WITH ENGINE = 'clickhouse' , PARAMETERS = { \"host\" : \"127.0.0.1\" , \"port\" : 9000 , \"database\" : \"test_data\" , \"user\" : \"root\" , \"password\" : \"password\" } ; Cockroach Labs \u00b6 Template Example CREATE DATABASE cockroach_datasource --- display name for the database WITH ENGINE = 'cockroachdb' , --- name of the MindsDB handler PARAMETERS = { \"host\" : \" \" , --- host name or IP address \"port\" : , --- port used to make TCP/IP connection \"database\" : \" \" , --- database name \"user\" : \" \" , --- database user \"password\" : \" \" , --- database password \"publish\" : \" \" --- optional, publish } ; CREATE DATABASE cockroach_datasource WITH ENGINE = 'cockroachdb' , PARAMETERS = { \"host\" : \"127.0.0.1\" , \"port\" : 26257 , \"database\" : \"cockroachdb\" , \"user\" : \"username\" , \"password\" : \"password\" } ; Couchbase \u00b6 Template Example CREATE DATABASE couchbase_datasource --- display name for the database WITH ENGINE = 'couchbase' , --- name of the MindsDB handler PARAMETERS = { \"host\" : \" \" , --- host name or IP address of the Couchbase server \"user\" : \" \" , --- user to authenticate with the Couchbase server \"password\" : \" \" , --- password used to authenticate with the Couchbase server \"bucket\" : \" \" , --- bucket name \"scope\" : \" \" --- scope used to query (defaults to `_default` if left blank) } ; --- a scope in Couchbase is equivalent to a schema in MySQL CREATE DATABASE couchbase_datasource WITH ENGINE = 'couchbase' , PARAMETERS = { \"host\" : \"127.0.0.1\" , \"user\" : \"couchbase\" , \"password\" : \"password\" , \"bucket\" : \"test-bucket\" } ; Crate.io \u00b6 Template Example CREATE DATABASE cratedb_datasource --- display name for the database WITH ENGINE = 'crate' , --- name of the MindsDB handler PARAMETERS = { \"host\" : \" \" , --- host name or IP address \"port\" : , --- port used to make TCP/IP connection \"user\" : \" \" , --- database user \"password\" : \" \" , --- database password \"schema_name\" : \" \" --- database schema name (defaults to `doc` if left blank) } ; CREATE DATABASE cratedb_datasource WITH ENGINE = 'crate' , PARAMETERS = { \"host\" : \"127.0.0.1\" , \"port\" : 4200 , \"user\" : \"crate\" , \"password\" : \"password\" , \"schema_name\" : \"doc\" } ; D0lt \u00b6 Template Example CREATE DATABASE d0lt_datasource --- display name for the database WITH ENGINE = 'd0lt' , --- name of the MindsDB handler PARAMETERS = { \"host\" : \" \" , --- host name or IP address \"port\" : , --- port used to make TCP/IP connection \"database\" : \" \" , --- database name \"user\" : \" \" , --- database user \"password\" : \" \" , --- database password \"ssl\" : , --- optional, the `ssl` parameter value indicates whether SSL is enabled (`True`) or disabled (`False`) \"ssl_ca\" : { --- optional, SSL Certificate Authority \"path\" : \" \" --- either \"path\" or \"url\" } , \"ssl_cert\" : { --- optional, SSL certificates \"url\" : \" \" --- either \"path\" or \"url\" } , \"ssl_key\" : { --- optional, SSL keys \"path\" : \" \" --- either \"path\" or \"url\" } } ; CREATE DATABASE d0lt_datasource WITH ENGINE = 'd0lt' , PARAMETERS = { \"host\" : \"127.0.0.1\" , \"port\" : 3306 , \"database\" : \"information_schema\" , \"user\" : \"root\" , \"password\" : \"password\" } ; Databricks \u00b6 Template Example CREATE DATABASE databricks_datasource --- display name for the database WITH ENGINE = 'databricks' , --- name of the MindsDB handler PARAMETERS = { \"server_hostname\" : \" \" , --- server hostname of the cluster or SQL warehouse \"http_path\" : \" \" , --- http path to the cluster or SQL warehouse \"access_token\" : \" \" , --- personal Databricks access token \"schema\" : \" \" , --- schema name (defaults to `default` if left blank) \"session_configuration\" : \" \" , --- optional, dictionary of Spark session configuration parameters \"http_headers\" : \" \" , --- optional, additional (key, value) pairs to set in HTTP headers on every RPC request the client makes \"catalog\" : \" \" --- catalog (defaults to `hive_metastore` if left blank) } ; CREATE DATABASE databricks_datasource WITH ENGINE = 'databricks' , PARAMETERS = { \"server_hostname\" : \"adb-1234567890123456.7.azuredatabricks.net\" , \"http_path\" : \"sql/protocolv1/o/1234567890123456/1234-567890-test123\" , \"access_token\" : \"dapi1234567890ab1cde2f3ab456c7d89efa\" , \"schema\" : \"example_db\" } ; Datastax \u00b6 Template Example CREATE DATABASE datastax_datasource --- display name for the database WITH ENGINE = 'astra' , --- name of the MindsDB handler PARAMETERS = { \"user\" : \" \" , --- user to be authenticated \"password\" : \" \" , --- password for authentication \"secure_connection_bundle\" : { --- secure connection bundle zip file \"path\" : \" \" --- either \"path\" or \"url\" } , \"host\" : \" \" , --- optional, host name or IP address \"port\" : , --- optional, port used to make TCP/IP connection \"protocol_version\" : , --- optional, protocol version \"keyspace\" : \" \" --- optional, keyspace } ; CREATE DATABASE datastax_datasource WITH ENGINE = 'astra' , PARAMETERS = { \"host\" : \"127.0.0.1\" , \"port\" : 7077 , \"user\" : \"datastax\" , \"password\" : \"password\" , \"secure_connection_bundle\" : { \"path\" : \"/home/Downloads/file.zip\" } } ; DB2 \u00b6 Template Example CREATE DATABASE db2_datasource --- display name for the database WITH ENGINE = 'DB2' , --- name of the MindsDB handler PARAMETERS = { \"host\" : \" \" , --- host name or IP address \"port\" : , --- port used to make TCP/IP connection \"database\" : \" \" , --- database name \"user\" : \" \" , --- database user \"password\" : \" \" , --- database password \"schema_name\" : \" \" --- database schema name } ; CREATE DATABASE db2_datasource WITH ENGINE = 'DB2' , PARAMETERS = { \"host\" : \"127.0.0.1\" , \"port\" : 25000 , \"database\" : \"BOOKS\" , \"user\" : \"db2admin\" , \"password\" : \"password\" , \"schema_name\" : \"db2admin\" } ; Druid \u00b6 Template Example CREATE DATABASE druid_datasource --- display name for the database WITH ENGINE = 'druid' , --- name of the MindsDB handler PARAMETERS = { \"host\" : \" \" , --- host name or IP address of Apache Druid \"port\" : , --- port where Apache Druid runs \"user\" : \" \" , --- optional, user to authenticate with Apache Druid \"password\" : \" \" , --- optional, password used to authenticate with Apache Druid \"path\" : \" \" , --- query path \"scheme\" : \" \" --- the URI scheme (defaults to `http` if left blank) } ; CREATE DATABASE druid_datasource WITH ENGINE = 'druid' , PARAMETERS = { \"host\" : \"127.0.0.1\" , \"port\" : 8888 , \"path\" : \"/druid/v2/sql/\" , \"scheme\" : \"http\" } ; Elastic Search \u00b6 Template Example CREATE DATABASE elastic_datasource --- display name for the database WITH ENGINE = 'elasticsearch' , --- name of the MindsDB handler PARAMETERS = { \"hosts\" : \" \" , --- one or more host names or IP addresses of the Elasticsearch server \"username\" : \" \" , --- optional, username to authenticate with the Elasticsearch server \"password\" : \" \" , --- optional, password used to authenticate with the Elasticsearch server \"cloud_id\" : \" \" --- optional, unique ID of your hosted Elasticsearch cluster (must be provided when \"hosts\" is left blank) } ; CREATE DATABASE elastic_datasource WITH ENGINE = 'elasticsearch' , PARAMETERS = { \"hosts\" : \"localhost:9200\" } ; Firebird \u00b6 Template Example CREATE DATABASE firebird_datasource --- display name for the database WITH ENGINE = 'firebird' , --- name of the MindsDB handler PARAMETERS = { \"host\" : \" \" , --- host name or IP address of the Firebird server \"database\" : \" \" , --- database name \"user\" : \" \" , --- user to authenticate with the Firebird server \"password\" : \" \" --- password used to authenticate with the Firebird server } ; CREATE DATABASE firebird_datasource WITH ENGINE = 'firebird' , PARAMETERS = { \"host\" : \"127.0.0.1\" , \"database\" : \"test\" , \"user\" : \"firebird\" , \"password\" : \"password\" } ; Google BigQuery \u00b6 Template Example for Self-Hosted MindsDB Example for MindsDB Cloud CREATE DATABASE bigquery_datasource --- display name for the database WITH ENGINE = 'bigquery' , --- name of the MindsDB handler PARAMETERS = { \"project_id\" : \" \" , --- globally unique project identifier \"service_account_keys\" : { --- service account keys file \"path\" : \" \" --- either \"path\" or \"url\" } } ; CREATE DATABASE bigquery_datasource WITH ENGINE = 'bigquery' , PARAMETERS = { \"project_id\" : \"badger-345908\" , \"service_account_keys\" : { \"path\" : \"/home/Downloads/badger-345908.json\" } } ; CREATE DATABASE bigquery_datasource WITH ENGINE = 'bigquery' , PARAMETERS = { \"project_id\" : \"badger-345908\" , \"service_account_keys\" : { \"url\" : \"https://url/badger-345908.json\" } } ; Hive \u00b6 Template Example CREATE DATABASE hive_datasource --- display name for the database WITH ENGINE = 'hive' , --- name of the MindsDB handler PARAMETERS = { \"user\" : \" \" , --- database user \"password\" : \" \" , --- database password \"host\" : \" \" , --- host name or IP address \"port\" : , --- port used to make TCP/IP connection \"database\" : \" \" , --- database name \"auth\" : \" \" --- defaults to CUSTOM if not provided; check for options here: https://pypi.org/project/PyHive/ } ; CREATE DATABASE hive_datasource WITH ENGINE = 'hive' , PARAMETERS = { \"user\" : \"hive\" , \"password\" : \"password\" , \"host\" : \"127.0.0.1\" , \"port\" : 10000 , \"database\" : \"hive_db\" , \"auth\" : \"CUSTOM\" } ; Informix \u00b6 Template Example CREATE DATABASE informix_datasource --- display name for the database WITH ENGINE = 'informix' , --- name of the MindsDB handler PARAMETERS = { \"server\" : \" \" , --- server name \"host\" : \" \" , --- host name or IP address \"port\" : , --- port used to make TCP/IP connection \"database\" : \" \" , --- database name \"user\" : \" \" , --- database user \"password\" : \" \" , --- database password \"schema_name\" : \" \" , --- database schema name \"logging_enabled\" : --- indicates whether logging is enabled (defaults to `True` if left blank) } ; CREATE DATABASE informix_datasource WITH ENGINE = 'informix' , PARAMETERS = { \"server\" : \"server\" , \"host\" : \"127.0.0.1\" , \"port\" : 9091 , \"database\" : \"stores_demo\" , \"user\" : \"informix\" , \"password\" : \"password\" , \"schema_name\" : \"demo_schema\" , \"logging_enabled\" : False } ; MariaDB \u00b6 Template Example CREATE DATABASE maria_datasource --- display name for the database WITH ENGINE = 'mariadb' , --- name of the MindsDB handler PARAMETERS = { \"host\" : \" \" , --- host IP address or URL \"port\" : , --- port used to make TCP/IP connection \"database\" : \" \" , --- database name \"user\" : \" \" , --- database user \"password\" : \" \" , --- database password \"ssl\" : , --- optional, the `ssl` parameter value indicates whether SSL is enabled (`True`) or disabled (`False`) \"ssl_ca\" : { --- optional, SSL Certificate Authority \"path\" : \" \" --- either \"path\" or \"url\" } , \"ssl_cert\" : { --- optional, SSL certificates \"url\" : \" \" --- either \"path\" or \"url\" } , \"ssl_key\" : { --- optional, SSL keys \"path\" : \" \" --- either \"path\" or \"url\" } } ; CREATE DATABASE maria_datasource WITH ENGINE = 'mariadb' , PARAMETERS = { \"host\" : \"127.0.0.1\" , \"port\" : 3306 , \"database\" : \"mariadb\" , \"user\" : \"root\" , \"password\" : \"password\" } ; MariaDB SkySQL \u00b6 Template Example CREATE DATABASE skysql --- display name for the database WITH ENGINE = 'mariadb' , --- name of the MindsDB handler PARAMETERS = { \"user\" : \" \" , --- database user \"password\" : \" \" , --- database password \"host\" : \" \" , --- host IP address or URL \"port\" : , --- port used to make TCP/IP connection \"ssl\" : , --- optional, the `ssl` parameter value indicates whether SSL is enabled (`True`) or disabled (`False`) \"ssl-ca\" : { --- optional, SSL Certificate Authority \"path\" : \" \" --- either \"path\" or \"url\" } , \"database\" : \" \" --- database name } ; CREATE DATABASE skysql_datasource WITH ENGINE = 'mariadb' , PARAMETERS = { \"host\" : \"mindsdbtest.mdb0002956.db1.skysql.net\" , \"port\" : 5001 , \"database\" : \"mindsdb_data\" , \"user\" : \"DB00007539\" , \"password\" : \"password\" , \"ssl-ca\" : { \"url\" : \"https://mindsdb-web-builds.s3.amazonaws.com/aws_skysql_chain.pem\" } } ; For more information on how to connect MariaDB SkySQL and MindsDB, visit our doc page here . Matrix One \u00b6 Template Example CREATE DATABASE matrixone_datasource --- display name for the database WITH ENGINE = 'matrixone' , --- name of the MindsDB handler PARAMETERS = { \"host\" : \" \" , --- host IP address or URL \"port\" : , --- port used to make TCP/IP connection \"database\" : \" \" , --- database name \"user\" : \" \" , --- database user \"password\" : \" \" , --- database password \"ssl\" : , --- optional, the `ssl` parameter value indicates whether SSL is enabled (`True`) or disabled (`False`) \"ssl_ca\" : { --- optional, SSL Certificate Authority \"path\" : \" \" --- either \"path\" or \"url\" } , \"ssl_cert\" : { --- optional, SSL certificates \"url\" : \" \" --- either \"path\" or \"url\" } , \"ssl_key\" : { --- optional, SSL keys \"path\" : \" \" --- either \"path\" or \"url\" } } ; CREATE DATABASE matrixone_datasource WITH ENGINE = 'matrixone' , PARAMETERS = { \"host\" : \"127.0.0.1\" , \"port\" : 6001 , \"database\" : \"mo_catalog\" , \"user\" : \"matrixone\" , \"password\" : \"password\" } ; Microsoft Access \u00b6 Template Example CREATE DATABASE access_datasource --- display name for the database WITH ENGINE = 'access' , --- name of the MindsDB handler PARAMETERS = { \"db_file\" : \" \" --- path to the database file to be used } ; CREATE DATABASE access_datasource WITH ENGINE = 'access' , PARAMETERS = { \"db_file\" : \"example_db.accdb\" } ; Microsoft SQL Server \u00b6 Template Example CREATE DATABASE mssql_datasource --- display name for the database WITH ENGINE = 'mssql' , --- name of the MindsDB handler PARAMETERS = { \"host\" : \" \" , --- host name or IP address \"port\" : , --- port used to make TCP/IP connection \"database\" : \" \" , --- database name \"user\" : \" \" , --- database user \"password\" : \" \" --- database password } ; CREATE DATABASE mssql_datasource WITH ENGINE = 'mssql' , PARAMETERS = { \"host\" : \"127.0.0.1\" , \"port\" : 1433 , \"database\" : \"master\" , \"user\" : \"sa\" , \"password\" : \"password\" } ; MonetDB \u00b6 Template Example CREATE DATABASE monetdb_datasource --- display name for the database WITH ENGINE = 'monetdb' , --- name of the MindsDB handler PARAMETERS = { \"host\" : \" \" , --- host name or IP address \"port\" : , --- port used to make TCP/IP connection \"database\" : \" \" , --- database name \"user\" : \" \" , --- database user \"password\" : \" \" , --- database password \"schema_name\" : \" \" --- database schema name (defaults to the current schema if left blank) } ; CREATE DATABASE monetdb_datasource WITH ENGINE = 'monetdb' , PARAMETERS = { \"host\" : \"127.0.0.1\" , \"port\" : 50000 , \"database\" : \"demo\" , \"user\" : \"monetdb\" , \"password\" : \"password\" , \"schema_name\" : \"sys\" } ; MongoDB \u00b6 Template Example CREATE DATABASE mongo_datasource --- display name for the database WITH ENGINE = 'mongo' , --- name of the MindsDB handler PARAMETERS = { \"host\" : \" \" , --- host name or IP address \"port\" : , --- port used to make TCP/IP connection \"user\" : \" \" , --- database user \"password\" : \" \" --- database password } ; CREATE DATABASE mongo_datasource WITH ENGINE = 'mongo' , PARAMETERS = { \"host\" : \"127.0.0.1\" , \"port\" : 27017 , \"user\" : \"mongo\" , \"password\" : \"password\" } ; Follow the Mongo API documentation for details. MySQL \u00b6 Template Example CREATE DATABASE mysql_datasource --- display name for the database WITH ENGINE = 'mysql' , --- name of the MindsDB handler PARAMETERS = { \"host\" : \" \" , --- host name or IP address \"port\" : , --- port used to make TCP/IP connection \"database\" : \" \" , --- database name \"user\" : \" \" , --- database user \"password\" : \" \" , --- database password \"ssl\" : , --- optional, the `ssl` parameter value indicates whether SSL is enabled (`True`) or disabled (`False`) \"ssl_ca\" : { --- optional, SSL Certificate Authority \"path\" : \" \" --- either \"path\" or \"url\" } , \"ssl_cert\" : { --- optional, SSL certificates \"url\" : \" \" --- either \"path\" or \"url\" } , \"ssl_key\" : { --- optional, SSL keys \"path\" : \" \" --- either \"path\" or \"url\" } } ; CREATE DATABASE mysql_datasource WITH ENGINE = 'mysql' , PARAMETERS = { \"host\" : \"127.0.0.1\" , \"port\" : 3306 , \"database\" : \"mysql\" , \"user\" : \"root\" , \"password\" : \"password\" } ; Oracle \u00b6 Template Example CREATE DATABASE oracle_datasource --- display name for the database WITH ENGINE = 'oracle' , --- name of the MindsDB handler PARAMETERS = { \"host\" : \" \" , --- host name or IP address \"port\" : , --- port used to make TCP/IP connection \"sid\" : \" \" , --- unique identifier of the database instance \"service_name\" : \" \" , --- optional, database service name (must be provided when \"sid\" is left blank) \"user\" : \" \" , --- database user \"password\" : \" \" --- database password } ; CREATE DATABASE oracle_datasource WITH ENGINE = 'oracle' , PARAMETERS = { \"host\" : \"127.0.0.1\" , \"port\" : 1521 , \"sid\" : \"ORCL\" , \"user\" : \"sys\" , \"password\" : \"password\" } ; Pinot \u00b6 Template Example CREATE DATABASE pinot_datasource --- display name for the database WITH ENGINE = 'pinot' , --- name of the MindsDB handler PARAMETERS = { \"host\" : \" \" , --- host name or IP address of the Apache Pinot cluster \"broker_port\" : , --- port where the broker of the Apache Pinot cluster runs \"controller_port\" : , --- port where the controller of the Apache Pinot cluster runs \"path\" : \" \" , --- query path \"scheme\" : \" \" , --- scheme (defaults to `http` if left blank) \"username\" : \" \" , --- optional, user \"password\" : \" \" , --- optional, password \"verify_ssl\" : \" \" --- optional, verify SSL } ; CREATE DATABASE pinot_datasource WITH ENGINE = 'pinot' , PARAMETERS = { \"host\" : \"127.0.0.1\" , \"broker_port\" : 8000 , \"controller_port\" : 9000 , \"path\" : \"/query/sql\" , \"scheme\" : \"http\" } ; PostgreSQL \u00b6 Template Example CREATE DATABASE psql_datasource --- display name for the database WITH ENGINE = 'postgres' , --- name of the MindsDB handler PARAMETERS = { \"host\" : \" \" , --- host name or IP address \"port\" : , --- port used to make TCP/IP connection \"database\" : \" \" , --- database name \"user\" : \" \" , --- database user \"password\" : \" \" --- database password } ; CREATE DATABASE psql_datasource WITH ENGINE = 'postgres' , PARAMETERS = { \"host\" : \"127.0.0.1\" , \"port\" : 5432 , \"database\" : \"postgres\" , \"user\" : \"postgres\" , \"password\" : \"password\" } ; QuestDB \u00b6 Template Example CREATE DATABASE questdb_datasource --- display name for the database WITH ENGINE = 'questdb' , --- name of the MindsDB handler PARAMETERS = { \"host\" : \" \" , --- host name or IP address \"port\" : , --- port used to make TCP/IP connection \"database\" : \" \" , --- database name \"user\" : \" \" , --- database user \"password\" : \" \" , --- database password \"public\" : --- value of `True` or `False` (defaults to `True` if left blank) } ; CREATE DATABASE questdb_datasource WITH ENGINE = 'questdb' , PARAMETERS = { \"host\" : \"127.0.0.1\" , \"port\" : 8812 , \"database\" : \"qdb\" , \"user\" : \"admin\" , \"password\" : \"password\" } ; S3 \u00b6 Template Example CREATE DATABASE amazons3_datasource --- display name for the database WITH ENGINE = 's3' , --- name of the MindsDB handler PARAMETERS = { \"aws_access_key_id\" : \" \" , --- the AWS access key \"aws_secret_access_key\" : \" \" , --- the AWS secret access key \"region_name\" : \" \" , --- the AWS region \"bucket\" : \" \" , --- name of the S3 bucket \"key\" : \" \" , --- key of the object to be queried \"input_serialization\" : \" \" --- format of the data to be queried } ; CREATE DATABASE amazons3_datasource WITH ENGINE = 's3' , PARAMETERS = { \"aws_access_key_id\" : \"PCAQ2LJDOSWLNSQKOCPW\" , \"aws_secret_access_key\" : \"U/VjewPlNopsDmmwItl34r2neyC6WhZpUiip57i\" , \"region_name\" : \"us-east-1\" , \"bucket\" : \"mindsdb-bucket\" , \"key\" : \"iris.csv\" , \"input_serialization\" : \"{'CSV': {'FileHeaderInfo': 'NONE'}}\" } ; SAP HANA \u00b6 Template Example CREATE DATABASE sap_hana_datasource --- display name for the database WITH ENGINE = 'hana' , --- name of the MindsDB handler PARAMETERS = { \"host\" : \" \" , --- host name or IP address \"port\" : , --- port used to make TCP/IP connection \"user\" : \" \" , --- user \"password\" : \" \" , --- password \"schema\" : \" \" , --- database schema name (defaults to the current schema if left blank) \"encrypt\" : --- indicates whether connection is encrypted (required for cloud usage) } ; CREATE DATABASE sap_hana_datasource WITH ENGINE = 'hana' , PARAMETERS = { \"host\" : \"<uuid>.hana.trial-us10.hanacloud.ondemand.com\" , \"port\" : \"443\" , \"user\" : \"DBADMIN\" , \"password\" : \"password\" , \"schema\" : \"MINDSDB\" , \"encrypt\" : True } ; Scylla \u00b6 Template Example CREATE DATABASE scylladb_datasource --- display name for the database WITH ENGINE = 'scylladb' , --- name of the MindsDB handler PARAMETERS = { \"host\" : \" \" , --- host name or IP address \"port\" : , --- port used to make TCP/IP connection \"user\" : \" \" , --- user \"password\" : \" \" , --- password \"protocol_version\" : , --- optional, protocol version (defaults to 4 if left blank) \"keyspace\" : \" \" , --- keyspace name (it is the top level container for tables) \"secure_connect_bundle\" : { --- secure connect bundle file \"path\" : \" \" --- either \"path\" or \"url\" } } ; CREATE DATABASE scylladb_datasource WITH ENGINE = 'scylladb' , PARAMETERS = { \"host\" : \"127.0.0.1\" , \"port\" : 7199 , \"user\" : \"user@mindsdb.com\" , \"password\" : \"password\" , \"protocol_version\" : 4 , \"keyspace\" : \"keyspace_name\" , \"secure_connect_bundle\" : { \"path\" : \"/home/zoran/Downloads/secure-connect-mindsdb.zip\" } } ; SingleStore \u00b6 Template Example CREATE DATABASE singlestore_datasource --- display name for the database WITH ENGINE = 'singlestore' , --- name of the MindsDB handler PARAMETERS = { \"host\" : \" \" , --- host name or IP address \"port\" : , --- port used to make TCP/IP connection \"database\" : \" \" , --- database name \"user\" : \" \" , --- database user \"password\" : \" \" , --- database password \"ssl\" : , --- optional, the `ssl` parameter value indicates whether SSL is enabled (`True`) or disabled (`False`) \"ssl_ca\" : { --- optional, SSL Certificate Authority \"path\" : \" \" --- either \"path\" or \"url\" } , \"ssl_cert\" : { --- optional, SSL certificates \"url\" : \" \" --- either \"path\" or \"url\" } , \"ssl_key\" : { --- optional, SSL keys \"path\" : \" \" --- either \"path\" or \"url\" } } ; CREATE DATABASE singlestore_datasource WITH ENGINE = 'singlestore' , PARAMETERS = { \"host\" : \"127.0.0.1\" , \"port\" : 3306 , \"database\" : \"singlestore\" , \"user\" : \"root\" , \"password\" : \"password\" } ; Snowflake \u00b6 Template Example CREATE DATABASE snowflake_datasource --- display name for the database WITH ENGINE = 'snowflake' , --- name of the MindsDB handler PARAMETERS = { \"host\" : \" \" , --- host name or IP address \"port\" : , --- port used to make TCP/IP connection \"database\" : \" \" , --- database name \"user\" : \" \" , --- database user \"password\" : \" \" , --- database password \"account\" : \" \" , --- the Snowflake account \"schema\" : \" \" , --- schema name (defaults to `public` if left blank) \"protocol\" : \" \" , --- protocol (defaults to `https` if left blank) \"warehouse\" : \" \" --- the warehouse account } ; CREATE DATABASE snowflake_datasource WITH ENGINE = 'snowflake' , PARAMETERS = { \"host\" : \"127.0.0.1\" , \"port\" : 443 , \"database\" : \"snowflake\" , \"user\" : \"user\" , \"password\" : \"password\" , \"account\" : \"account\" , \"schema\" : \"public\" , \"protocol\" : \"https\" , \"warehouse\" : \"warehouse\" } ; SQLite \u00b6 Template Example CREATE DATABASE sqlite_datasource --- display name for the database WITH ENGINE = 'sqlite' , --- name of the MindsDB handler PARAMETERS = { \"db_file\" : \" \" --- path to the database file to be used } ; CREATE DATABASE sqlite_datasource WITH ENGINE = 'sqlite' , PARAMETERS = { \"db_file\" : \"example.db\" } ; Supabase \u00b6 Template Example CREATE DATABASE supabase_datasource --- display name for the database WITH ENGINE = 'supabase' , --- name of the MindsDB handler PARAMETERS = { \"host\" : \" \" , --- host name or IP address \"port\" : , --- port used to make TCP/IP connection \"database\" : \" \" , --- database name \"user\" : \" \" , --- database user \"password\" : \" \" , --- database password } ; CREATE DATABASE supabase_datasource WITH ENGINE = 'supabase' , PARAMETERS = { \"host\" : \"127.0.0.1\" , \"port\" : 54321 , \"database\" : \"test\" , \"user\" : \"supabase\" , \"password\" : \"password\" } ; Teradata \u00b6 Template Example CREATE DATABASE display_name --- display name for the database WITH ENGINE = 'teradata' , --- name of the MindsDB handler PARAMETERS = { \"host\" : \" \" , --- host name or IP address \"user\" : \" \" , --- database user \"password\" : \" \" , --- database password \"database\" : \" \" , --- database name \"port\" : --- port used to make TCP/IP connection } ; CREATE DATABASE display_name WITH ENGINE = 'teradata' , PARAMETERS = { \"host\" : \"127.0.0.1\" , \"user\" : \"teradata\" , \"password\" : \"password\" , \"database\" : \"teradata_db\" , \"port\" : 1025 } ; TiDB \u00b6 Template Example CREATE DATABASE tidb_datasource --- display name for the database WITH ENGINE = 'tidb' , --- name of the MindsDB handler PARAMETERS = { \"host\" : \" \" , --- host name or IP address \"port\" : , --- port used to make TCP/IP connection \"database\" : \" \" , --- database name \"user\" : \" \" , --- database user \"password\" : \" \" , --- database password } ; CREATE DATABASE tidb_datasource WITH ENGINE = 'tidb' , PARAMETERS = { \"host\" : \"127.0.0.1\" , \"port\" : 4000 , \"database\" : \"tidb\" , \"user\" : \"root\" , \"password\" : \"password\" } ; Timescaledb \u00b6 Template Example CREATE DATABASE display_name --- display name for the database WITH ENGINE = 'timescaledb' , --- name of the MindsDB handler PARAMETERS = { \"user\" : \" \" , --- database user \"password\" : \" \" , --- database password \"host\" : \" \" , --- host name or IP address \"port\" : , --- port used to make TCP/IP connection \"database\" : \" \" --- database name } ; CREATE DATABASE display_name WITH ENGINE = 'timescaledb' , PARAMETERS = { \"user\" : \"timescaledb\" , \"password\" : \"password\" , \"host\" : \"127.0.0.1\" , \"port\" : 36806 , \"database\" : \"timescaledb_db\" } ; Trino \u00b6 Template Example 1 Example 2 CREATE DATABASE trino_datasource --- display name for the database WITH ENGINE = 'trino' , --- name of the MindsDB handler PARAMETERS = { \"host\" : \" \" , --- host name or IP address \"port\" : , --- port used to make TCP/IP connection \"auth\" : \" \" , --- optional, authentication method, currently only `basic` is supported \"http_scheme\" : \" \" , --- optional, `http`(default) or `https` \"user\" : \" \" , --- database user \"password\" : \" \" , --- database password \"catalog\" : \" \" , --- optional, catalog \"schema\" : \" \" --- optional, schema \"with\" : --- optional, default WITH-clause (properties) for ALL tables --- this parameter is experimental and might be changed or removed in future release } ; CREATE DATABASE trino_datasource WITH ENGINE = 'trino' , PARAMETERS = { \"host\" : \"127.0.0.1\" , \"port\" : 8080 , \"user\" : \"trino\" , \"password\" : \"password\" , \"catalog\" : \"default\" , \"schema\" : \"test\" } ; CREATE DATABASE trino_datasource WITH ENGINE = 'trino' , PARAMETERS = { \"host\" : \"127.0.0.1\" , \"port\" : 443 , \"auth\" : \"basic\" , \"http_scheme\" : \"https\" , \"user\" : \"trino\" , \"password\" : \"password\" , \"catalog\" : \"default\" , \"schema\" : \"test\" , \"with\" : \"with (transactional = true)\" } ; Vertica \u00b6 Template Example CREATE DATABASE vertica_datasource --- display name for the database WITH ENGINE = 'vertica' , --- name of the MindsDB handler PARAMETERS = { \"host\" : \" \" , --- host name or IP address \"port\" : , --- port used to make TCP/IP connection \"database\" : \" \" , --- database name \"user\" : \" \" , --- database user \"password\" : \" \" , --- database password \"schema_name\" : \" \" --- database schema name } ; CREATE DATABASE vertica_datasource WITH ENGINE = 'vertica' , PARAMETERS = { \"host\" : \"127.0.0.1\" , \"port\" : 5433 , \"database\" : \"VMart\" , \"user\" : \"vertica\" , \"password\" : \"password\" , \"schema_name\" : \"public\" } ; Yugabyte \u00b6 Template Example CREATE DATABASE display_name --- display name for the database WITH ENGINE = 'yugabyte' , --- name of the MindsDB handler PARAMETERS = { \"user\" : \" \" , --- database user \"password\" : \" \" , --- database password \"host\" : \" \" , --- host name or IP address \"port\" : , --- port used to make TCP/IP connection \"database\" : \" \" --- database name } ; CREATE DATABASE display_name WITH ENGINE = 'yugabyte' , PARAMETERS = { \"user\" : \"yugabyte\" , \"password\" : \"password\" , \"host\" : \"127.0.0.1\" , \"port\" : 5433 , \"database\" : \"yugabyte_db\" } ; \"host\": \" .hana.trial-us10.hanacloud.ondemand.com\", \"port\": \"443\", \"schema\": \"MINDSDB\", \"encrypt\": true }; ``` openGauss \u00b6 Template Example CREATE DATABASE opengauss_datasource --- display name for the database WITH ENGINE = 'opengauss' , --- name of the MindsDB handler PARAMETERS = { \"host\" : \" \" , --- host name or IP address \"port\" : , --- port used to make TCP/IP connection \"database\" : \" \" , --- database name \"user\" : \" \" , --- database user \"password\" : \" \" , --- database password } ; CREATE DATABASE opengauss_datasource WITH ENGINE = 'opengauss' , PARAMETERS = { \"host\" : \"127.0.0.1\" , \"port\" : 5432 , \"database\" : \"opengauss\" , \"user\" : \"mindsdb\" , \"password\" : \"password\" } ;","title":"DATABASE"},{"location":"sql/create/databases/#create-database-statement","text":"","title":"CREATE DATABASE Statement"},{"location":"sql/create/databases/#description","text":"MindsDB lets you connect to your favorite databases, data warehouses, data lakes, etc., via the CREATE DATABASE command. The MindsDB SQL API supports creating connections to integrations by passing the connection parameters specific per integration. You can find more in the Supported Integrations chapter.","title":"Description"},{"location":"sql/create/databases/#syntax","text":"Let's review the syntax for the CREATE DATABASE command. CREATE DATABASE datasource_name [ WITH ] [ ENGINE [ = ] engine_name ] [, PARAMETERS [ = ] { \"key\" : \"value\" , ... } ]; On execution, we get: Query OK , 0 rows affected ( x . xxx sec ) Where: Name Description [datasource_name] Identifier for the data source to be created. [engine_string] Engine to be selected depending on the database connection. PARAMETERS { \"key\" : \"value\" } object with the connection parameters specific for each engine. SQL Commands Resulting in the Same Output Please note that the keywords/statements enclosed within square brackets are optional. Also, by default, the engine is mindsdb if not provided otherwise. That yields the following SQL commands to result in the same output. CREATE DATABASE db ; CREATE DATABASE db ENGINE 'mindsdb' ; CREATE DATABASE db ENGINE = 'mindsdb' ; CREATE DATABASE db WITH ENGINE 'mindsdb' ; CREATE DATABASE db WITH ENGINE = 'mindsdb' ;","title":"Syntax"},{"location":"sql/create/databases/#example","text":"","title":"Example"},{"location":"sql/create/databases/#connecting-a-data-source","text":"Here is an example of how to connect to a MySQL database. CREATE DATABASE mysql_datasource WITH ENGINE = 'mariadb' , PARAMETERS = { \"user\" : \"root\" , \"port\" : 3307 , \"password\" : \"password\" , \"host\" : \"127.0.0.1\" , \"database\" : \"my_database\" } ; On execution, we get: Query OK , 0 rows affected ( 8 . 878 sec )","title":"Connecting a Data Source"},{"location":"sql/create/databases/#listing-linked-databases","text":"You can list all the linked databases using the command below. SHOW DATABASES ; On execution, we get: + --------------------+ | Database | + --------------------+ | information_schema | | mindsdb | | files | | mysql_datasource | + --------------------+","title":"Listing Linked Databases"},{"location":"sql/create/databases/#getting-linked-databases-metadata","text":"You can get metadata about the linked databases by querying the mindsdb.datasources table. SELECT * FROM mindsdb . datasources ; On execution, we get: + ------------------+---------------+--------------+------+-----------+ | name | database_type | host | port | user | + ------------------+---------------+--------------+------+-----------+ | mysql_datasource | mysql | 3 . 220 . 66 . 106 | 3306 | root | + ------------------+---------------+--------------+------+-----------+","title":"Getting Linked Databases Metadata"},{"location":"sql/create/databases/#making-your-local-database-available-to-mindsdb","text":"When connecting your local database to MindsDB Cloud, you should expose the local database server to be publicly accessible. It is easy to accomplish using Ngrok Tunnel . The free tier offers all you need to get started. The installation instructions are easy to follow. Head over to the downloads page and choose your operating system. Follow the instructions for installation. Then create a free account at Ngrok to get an auth token that you can use to configure your Ngrok instance. Once installed and configured, run the following command to obtain the host and port for your localhost at [port-number] . ngrok tcp [ port-number ] Here is an example. Assuming that you run a PostgreSQL database at localhost:5432 , use the following command: ngrok tcp 5432 On execution, we get: Session Status online Account myaccount ( Plan: Free ) Version 2 .3.40 Region United States ( us ) Web Interface http://127.0.0.1:4040 Forwarding tcp://4.tcp.ngrok.io:15093 -> localhost 5432 Now you can access your local database at 4.tcp.ngrok.io:15093 instead of localhost:5432 . So to connect your local database to the MindsDB GUI, use the Forwarding information. The host is 4.tcp.ngrok.io , and the port is 15093 . Proceed to create a database connection in the MindsDB GUI by executing the CREATE DATABASE statement with the host and port number obtained from Ngrok. CREATE DATABASE psql_datasource WITH ENGINE = 'postgres' , PARAMETERS = { \"user\" : \"postgres\" , \"port\" : 15093 , \"password\" : \"password\" , \"host\" : \"4.tcp.ngrok.io\" , \"database\" : \"postgres\" } ; Please note that the Ngrok tunnel loses connection when stopped or canceled. To reconnect your local database to MindsDB, you should create an Ngrok tunnel again. In the free tier, Ngrok changes the host and port values each time you launch the program, so you need to reconnect your database in the MindsDB Cloud by passing the new host and port values obtained from Ngrok. Before resetting the database connection, drop the previously connected data source using the DROP DATABASE statement. DROP DATABASE psql_datasource ; After dropping the data source and reconnecting your local database, you can use the predictors that you trained using the previously connected data source. However, if you have to RETRAIN your predictors, please ensure the database connection has the same name you used when creating the predictor to avoid failing to retrain. Work in progress Please note that this feature is a beta version. If you have questions about the supported data sources or experience some issues, reach out to us on Slack or open a GitHub issue .","title":"Making your Local Database Available to MindsDB"},{"location":"sql/create/databases/#supported-integrations","text":"The list of databases supported by MindsDB keeps growing. Here are the currently supported integrations: You can find particular databases' handler files here to see their connection arguments. For example, to see the latest updates to the Oracle handler, check Oracle's readme.md file here . Let's look at sample codes showing how to connect to each of the supported integrations.","title":"Supported Integrations"},{"location":"sql/create/databases/#airtable","text":"Template Example CREATE DATABASE airtable_datasource --- display name for the database WITH ENGINE = 'airtable' , --- name of the MindsDB handler PARAMETERS = { \"base_id\" : \" \" , --- the Airtable base ID \"table_name\" : \" \" , --- the Airtable table name \"api_key\" : \" \" --- the API key for the Airtable API } ; CREATE DATABASE airtable_datasource WITH ENGINE = 'airtable' , PARAMETERS = { \"base_id\" : \"appve10klsda2\" , \"table_name\" : \"my_table\" , \"api_key\" : \"KdJX2Q5km%5b$T$sQYm^gvN\" } ;","title":"Airtable"},{"location":"sql/create/databases/#amazon-redshift","text":"Template Example CREATE DATABASE amazonredshift_datasource --- display name for the database WITH ENGINE = 'amazonredshift' , --- name of the MindsDB handler PARAMETERS = { \"host\" : \" \" , --- host name or IP address of the Redshift cluster \"port\" : , --- port used when connecting to the Redshift cluster \"database\" : \" \" , --- database name used when connecting to the Redshift cluster \"user\" : \" \" , --- user to authenticate with the Redshift cluster \"password\" : \" \" --- password used to authenticate with the Redshift cluster } ; CREATE DATABASE amazonredshift_datasource WITH ENGINE = 'amazonredshift' , PARAMETERS = { \"host\" : \"127.0.0.1\" , \"port\" : 5439 , \"database\" : \"test\" , \"user\" : \"amazonredshift\" , \"password\" : \"password\" } ;","title":"Amazon Redshift"},{"location":"sql/create/databases/#aws-dynamodb","text":"Template Example CREATE DATABASE dynamodb_datasource --- display name for the database WITH ENGINE = 'dynamodb' , --- name of the MindsDB handler PARAMETERS = { \"aws_access_key_id\" : \" \" , --- the AWS access key \"aws_secret_access_key\" : \" \" , --- the AWS secret access key \"region_name\" : \" \" --- the AWS region } ; CREATE DATABASE dynamodb_datasource WITH ENGINE = 'dynamodb' , PARAMETERS = { \"aws_access_key_id\" : \"PCAQ2LJDOSWLNSQKOCPW\" , \"aws_secret_access_key\" : \"U/VjewPlNopsDmmwItl34r2neyC6WhZpUiip57i\" , \"region_name\" : \"us-east-1\" } ;","title":"AWS DynamoDB"},{"location":"sql/create/databases/#cassandra","text":"Template Example CREATE DATABASE cassandra_datasource --- display name for the database WITH ENGINE = 'cassandra' , --- name of the MindsDB handler PARAMETERS = { \"host\" : \" \" , --- host name or IP address \"port\" : , --- port used to make TCP/IP connection \"user\" : \" \" , --- database user \"password\" : \" \" , --- database password \"keyspace\" : \" \" , --- database name \"protocol_version\" : , --- optional, protocol version (defaults to 4 if left blank) \"secure_connect_bundle\" : { --- optional, secure connect bundle file \"path\" : \" \" --- either \"path\" or \"url\" } } ; CREATE DATABASE cassandra_datasource WITH ENGINE = 'cassandra' , PARAMETERS = { \"host\" : \"127.0.0.1\" , \"port\" : 9043 , \"user\" : \"user\" , \"password\" : \"password\" , \"keyspace\" : \"test_data\" , \"protocol_version\" : 4 } ;","title":"Cassandra"},{"location":"sql/create/databases/#ckan","text":"Template Example CREATE DATABASE ckan_datasource --- display name for the database WITH ENGINE = 'ckan' , --- name of the MindsDB handler PARAMETERS = { \"url\" : \" \" , --- host name, IP address, or a URL \"apikey\" : \" \" --- the API key used for authentication } ; CREATE DATABASE ckan_datasource WITH ENGINE = 'ckan' , PARAMETERS = { \"url\" : \"http://demo.ckan.org/api/3/action/\" , \"apikey\" : \"YOUR_API_KEY\" } ;","title":"Ckan"},{"location":"sql/create/databases/#clickhouse","text":"Template Example CREATE DATABASE clickhouse_datasource --- display name for the database WITH ENGINE = 'clickhouse' , --- name of the MindsDB handler PARAMETERS = { \"host\" : \" \" , --- host name or IP address \"port\" : , --- port used to make TCP/IP connection \"database\" : \" \" , --- database name \"user\" : \" \" , --- database user \"password\" : \" \" , --- database password \"protocol\" : \" \" --- optional, http or https (defaults to `native`) } ; CREATE DATABASE clickhouse_datasource WITH ENGINE = 'clickhouse' , PARAMETERS = { \"host\" : \"127.0.0.1\" , \"port\" : 9000 , \"database\" : \"test_data\" , \"user\" : \"root\" , \"password\" : \"password\" } ;","title":"ClickHouse"},{"location":"sql/create/databases/#cockroach-labs","text":"Template Example CREATE DATABASE cockroach_datasource --- display name for the database WITH ENGINE = 'cockroachdb' , --- name of the MindsDB handler PARAMETERS = { \"host\" : \" \" , --- host name or IP address \"port\" : , --- port used to make TCP/IP connection \"database\" : \" \" , --- database name \"user\" : \" \" , --- database user \"password\" : \" \" , --- database password \"publish\" : \" \" --- optional, publish } ; CREATE DATABASE cockroach_datasource WITH ENGINE = 'cockroachdb' , PARAMETERS = { \"host\" : \"127.0.0.1\" , \"port\" : 26257 , \"database\" : \"cockroachdb\" , \"user\" : \"username\" , \"password\" : \"password\" } ;","title":"Cockroach Labs"},{"location":"sql/create/databases/#couchbase","text":"Template Example CREATE DATABASE couchbase_datasource --- display name for the database WITH ENGINE = 'couchbase' , --- name of the MindsDB handler PARAMETERS = { \"host\" : \" \" , --- host name or IP address of the Couchbase server \"user\" : \" \" , --- user to authenticate with the Couchbase server \"password\" : \" \" , --- password used to authenticate with the Couchbase server \"bucket\" : \" \" , --- bucket name \"scope\" : \" \" --- scope used to query (defaults to `_default` if left blank) } ; --- a scope in Couchbase is equivalent to a schema in MySQL CREATE DATABASE couchbase_datasource WITH ENGINE = 'couchbase' , PARAMETERS = { \"host\" : \"127.0.0.1\" , \"user\" : \"couchbase\" , \"password\" : \"password\" , \"bucket\" : \"test-bucket\" } ;","title":"Couchbase"},{"location":"sql/create/databases/#crateio","text":"Template Example CREATE DATABASE cratedb_datasource --- display name for the database WITH ENGINE = 'crate' , --- name of the MindsDB handler PARAMETERS = { \"host\" : \" \" , --- host name or IP address \"port\" : , --- port used to make TCP/IP connection \"user\" : \" \" , --- database user \"password\" : \" \" , --- database password \"schema_name\" : \" \" --- database schema name (defaults to `doc` if left blank) } ; CREATE DATABASE cratedb_datasource WITH ENGINE = 'crate' , PARAMETERS = { \"host\" : \"127.0.0.1\" , \"port\" : 4200 , \"user\" : \"crate\" , \"password\" : \"password\" , \"schema_name\" : \"doc\" } ;","title":"Crate.io"},{"location":"sql/create/databases/#d0lt","text":"Template Example CREATE DATABASE d0lt_datasource --- display name for the database WITH ENGINE = 'd0lt' , --- name of the MindsDB handler PARAMETERS = { \"host\" : \" \" , --- host name or IP address \"port\" : , --- port used to make TCP/IP connection \"database\" : \" \" , --- database name \"user\" : \" \" , --- database user \"password\" : \" \" , --- database password \"ssl\" : , --- optional, the `ssl` parameter value indicates whether SSL is enabled (`True`) or disabled (`False`) \"ssl_ca\" : { --- optional, SSL Certificate Authority \"path\" : \" \" --- either \"path\" or \"url\" } , \"ssl_cert\" : { --- optional, SSL certificates \"url\" : \" \" --- either \"path\" or \"url\" } , \"ssl_key\" : { --- optional, SSL keys \"path\" : \" \" --- either \"path\" or \"url\" } } ; CREATE DATABASE d0lt_datasource WITH ENGINE = 'd0lt' , PARAMETERS = { \"host\" : \"127.0.0.1\" , \"port\" : 3306 , \"database\" : \"information_schema\" , \"user\" : \"root\" , \"password\" : \"password\" } ;","title":"D0lt"},{"location":"sql/create/databases/#databricks","text":"Template Example CREATE DATABASE databricks_datasource --- display name for the database WITH ENGINE = 'databricks' , --- name of the MindsDB handler PARAMETERS = { \"server_hostname\" : \" \" , --- server hostname of the cluster or SQL warehouse \"http_path\" : \" \" , --- http path to the cluster or SQL warehouse \"access_token\" : \" \" , --- personal Databricks access token \"schema\" : \" \" , --- schema name (defaults to `default` if left blank) \"session_configuration\" : \" \" , --- optional, dictionary of Spark session configuration parameters \"http_headers\" : \" \" , --- optional, additional (key, value) pairs to set in HTTP headers on every RPC request the client makes \"catalog\" : \" \" --- catalog (defaults to `hive_metastore` if left blank) } ; CREATE DATABASE databricks_datasource WITH ENGINE = 'databricks' , PARAMETERS = { \"server_hostname\" : \"adb-1234567890123456.7.azuredatabricks.net\" , \"http_path\" : \"sql/protocolv1/o/1234567890123456/1234-567890-test123\" , \"access_token\" : \"dapi1234567890ab1cde2f3ab456c7d89efa\" , \"schema\" : \"example_db\" } ;","title":"Databricks"},{"location":"sql/create/databases/#datastax","text":"Template Example CREATE DATABASE datastax_datasource --- display name for the database WITH ENGINE = 'astra' , --- name of the MindsDB handler PARAMETERS = { \"user\" : \" \" , --- user to be authenticated \"password\" : \" \" , --- password for authentication \"secure_connection_bundle\" : { --- secure connection bundle zip file \"path\" : \" \" --- either \"path\" or \"url\" } , \"host\" : \" \" , --- optional, host name or IP address \"port\" : , --- optional, port used to make TCP/IP connection \"protocol_version\" : , --- optional, protocol version \"keyspace\" : \" \" --- optional, keyspace } ; CREATE DATABASE datastax_datasource WITH ENGINE = 'astra' , PARAMETERS = { \"host\" : \"127.0.0.1\" , \"port\" : 7077 , \"user\" : \"datastax\" , \"password\" : \"password\" , \"secure_connection_bundle\" : { \"path\" : \"/home/Downloads/file.zip\" } } ;","title":"Datastax"},{"location":"sql/create/databases/#db2","text":"Template Example CREATE DATABASE db2_datasource --- display name for the database WITH ENGINE = 'DB2' , --- name of the MindsDB handler PARAMETERS = { \"host\" : \" \" , --- host name or IP address \"port\" : , --- port used to make TCP/IP connection \"database\" : \" \" , --- database name \"user\" : \" \" , --- database user \"password\" : \" \" , --- database password \"schema_name\" : \" \" --- database schema name } ; CREATE DATABASE db2_datasource WITH ENGINE = 'DB2' , PARAMETERS = { \"host\" : \"127.0.0.1\" , \"port\" : 25000 , \"database\" : \"BOOKS\" , \"user\" : \"db2admin\" , \"password\" : \"password\" , \"schema_name\" : \"db2admin\" } ;","title":"DB2"},{"location":"sql/create/databases/#druid","text":"Template Example CREATE DATABASE druid_datasource --- display name for the database WITH ENGINE = 'druid' , --- name of the MindsDB handler PARAMETERS = { \"host\" : \" \" , --- host name or IP address of Apache Druid \"port\" : , --- port where Apache Druid runs \"user\" : \" \" , --- optional, user to authenticate with Apache Druid \"password\" : \" \" , --- optional, password used to authenticate with Apache Druid \"path\" : \" \" , --- query path \"scheme\" : \" \" --- the URI scheme (defaults to `http` if left blank) } ; CREATE DATABASE druid_datasource WITH ENGINE = 'druid' , PARAMETERS = { \"host\" : \"127.0.0.1\" , \"port\" : 8888 , \"path\" : \"/druid/v2/sql/\" , \"scheme\" : \"http\" } ;","title":"Druid"},{"location":"sql/create/databases/#elastic-search","text":"Template Example CREATE DATABASE elastic_datasource --- display name for the database WITH ENGINE = 'elasticsearch' , --- name of the MindsDB handler PARAMETERS = { \"hosts\" : \" \" , --- one or more host names or IP addresses of the Elasticsearch server \"username\" : \" \" , --- optional, username to authenticate with the Elasticsearch server \"password\" : \" \" , --- optional, password used to authenticate with the Elasticsearch server \"cloud_id\" : \" \" --- optional, unique ID of your hosted Elasticsearch cluster (must be provided when \"hosts\" is left blank) } ; CREATE DATABASE elastic_datasource WITH ENGINE = 'elasticsearch' , PARAMETERS = { \"hosts\" : \"localhost:9200\" } ;","title":"Elastic Search"},{"location":"sql/create/databases/#firebird","text":"Template Example CREATE DATABASE firebird_datasource --- display name for the database WITH ENGINE = 'firebird' , --- name of the MindsDB handler PARAMETERS = { \"host\" : \" \" , --- host name or IP address of the Firebird server \"database\" : \" \" , --- database name \"user\" : \" \" , --- user to authenticate with the Firebird server \"password\" : \" \" --- password used to authenticate with the Firebird server } ; CREATE DATABASE firebird_datasource WITH ENGINE = 'firebird' , PARAMETERS = { \"host\" : \"127.0.0.1\" , \"database\" : \"test\" , \"user\" : \"firebird\" , \"password\" : \"password\" } ;","title":"Firebird"},{"location":"sql/create/databases/#google-bigquery","text":"Template Example for Self-Hosted MindsDB Example for MindsDB Cloud CREATE DATABASE bigquery_datasource --- display name for the database WITH ENGINE = 'bigquery' , --- name of the MindsDB handler PARAMETERS = { \"project_id\" : \" \" , --- globally unique project identifier \"service_account_keys\" : { --- service account keys file \"path\" : \" \" --- either \"path\" or \"url\" } } ; CREATE DATABASE bigquery_datasource WITH ENGINE = 'bigquery' , PARAMETERS = { \"project_id\" : \"badger-345908\" , \"service_account_keys\" : { \"path\" : \"/home/Downloads/badger-345908.json\" } } ; CREATE DATABASE bigquery_datasource WITH ENGINE = 'bigquery' , PARAMETERS = { \"project_id\" : \"badger-345908\" , \"service_account_keys\" : { \"url\" : \"https://url/badger-345908.json\" } } ;","title":"Google BigQuery"},{"location":"sql/create/databases/#hive","text":"Template Example CREATE DATABASE hive_datasource --- display name for the database WITH ENGINE = 'hive' , --- name of the MindsDB handler PARAMETERS = { \"user\" : \" \" , --- database user \"password\" : \" \" , --- database password \"host\" : \" \" , --- host name or IP address \"port\" : , --- port used to make TCP/IP connection \"database\" : \" \" , --- database name \"auth\" : \" \" --- defaults to CUSTOM if not provided; check for options here: https://pypi.org/project/PyHive/ } ; CREATE DATABASE hive_datasource WITH ENGINE = 'hive' , PARAMETERS = { \"user\" : \"hive\" , \"password\" : \"password\" , \"host\" : \"127.0.0.1\" , \"port\" : 10000 , \"database\" : \"hive_db\" , \"auth\" : \"CUSTOM\" } ;","title":"Hive"},{"location":"sql/create/databases/#informix","text":"Template Example CREATE DATABASE informix_datasource --- display name for the database WITH ENGINE = 'informix' , --- name of the MindsDB handler PARAMETERS = { \"server\" : \" \" , --- server name \"host\" : \" \" , --- host name or IP address \"port\" : , --- port used to make TCP/IP connection \"database\" : \" \" , --- database name \"user\" : \" \" , --- database user \"password\" : \" \" , --- database password \"schema_name\" : \" \" , --- database schema name \"logging_enabled\" : --- indicates whether logging is enabled (defaults to `True` if left blank) } ; CREATE DATABASE informix_datasource WITH ENGINE = 'informix' , PARAMETERS = { \"server\" : \"server\" , \"host\" : \"127.0.0.1\" , \"port\" : 9091 , \"database\" : \"stores_demo\" , \"user\" : \"informix\" , \"password\" : \"password\" , \"schema_name\" : \"demo_schema\" , \"logging_enabled\" : False } ;","title":"Informix"},{"location":"sql/create/databases/#mariadb","text":"Template Example CREATE DATABASE maria_datasource --- display name for the database WITH ENGINE = 'mariadb' , --- name of the MindsDB handler PARAMETERS = { \"host\" : \" \" , --- host IP address or URL \"port\" : , --- port used to make TCP/IP connection \"database\" : \" \" , --- database name \"user\" : \" \" , --- database user \"password\" : \" \" , --- database password \"ssl\" : , --- optional, the `ssl` parameter value indicates whether SSL is enabled (`True`) or disabled (`False`) \"ssl_ca\" : { --- optional, SSL Certificate Authority \"path\" : \" \" --- either \"path\" or \"url\" } , \"ssl_cert\" : { --- optional, SSL certificates \"url\" : \" \" --- either \"path\" or \"url\" } , \"ssl_key\" : { --- optional, SSL keys \"path\" : \" \" --- either \"path\" or \"url\" } } ; CREATE DATABASE maria_datasource WITH ENGINE = 'mariadb' , PARAMETERS = { \"host\" : \"127.0.0.1\" , \"port\" : 3306 , \"database\" : \"mariadb\" , \"user\" : \"root\" , \"password\" : \"password\" } ;","title":"MariaDB"},{"location":"sql/create/databases/#mariadb-skysql","text":"Template Example CREATE DATABASE skysql --- display name for the database WITH ENGINE = 'mariadb' , --- name of the MindsDB handler PARAMETERS = { \"user\" : \" \" , --- database user \"password\" : \" \" , --- database password \"host\" : \" \" , --- host IP address or URL \"port\" : , --- port used to make TCP/IP connection \"ssl\" : , --- optional, the `ssl` parameter value indicates whether SSL is enabled (`True`) or disabled (`False`) \"ssl-ca\" : { --- optional, SSL Certificate Authority \"path\" : \" \" --- either \"path\" or \"url\" } , \"database\" : \" \" --- database name } ; CREATE DATABASE skysql_datasource WITH ENGINE = 'mariadb' , PARAMETERS = { \"host\" : \"mindsdbtest.mdb0002956.db1.skysql.net\" , \"port\" : 5001 , \"database\" : \"mindsdb_data\" , \"user\" : \"DB00007539\" , \"password\" : \"password\" , \"ssl-ca\" : { \"url\" : \"https://mindsdb-web-builds.s3.amazonaws.com/aws_skysql_chain.pem\" } } ; For more information on how to connect MariaDB SkySQL and MindsDB, visit our doc page here .","title":"MariaDB SkySQL"},{"location":"sql/create/databases/#matrix-one","text":"Template Example CREATE DATABASE matrixone_datasource --- display name for the database WITH ENGINE = 'matrixone' , --- name of the MindsDB handler PARAMETERS = { \"host\" : \" \" , --- host IP address or URL \"port\" : , --- port used to make TCP/IP connection \"database\" : \" \" , --- database name \"user\" : \" \" , --- database user \"password\" : \" \" , --- database password \"ssl\" : , --- optional, the `ssl` parameter value indicates whether SSL is enabled (`True`) or disabled (`False`) \"ssl_ca\" : { --- optional, SSL Certificate Authority \"path\" : \" \" --- either \"path\" or \"url\" } , \"ssl_cert\" : { --- optional, SSL certificates \"url\" : \" \" --- either \"path\" or \"url\" } , \"ssl_key\" : { --- optional, SSL keys \"path\" : \" \" --- either \"path\" or \"url\" } } ; CREATE DATABASE matrixone_datasource WITH ENGINE = 'matrixone' , PARAMETERS = { \"host\" : \"127.0.0.1\" , \"port\" : 6001 , \"database\" : \"mo_catalog\" , \"user\" : \"matrixone\" , \"password\" : \"password\" } ;","title":"Matrix One"},{"location":"sql/create/databases/#microsoft-access","text":"Template Example CREATE DATABASE access_datasource --- display name for the database WITH ENGINE = 'access' , --- name of the MindsDB handler PARAMETERS = { \"db_file\" : \" \" --- path to the database file to be used } ; CREATE DATABASE access_datasource WITH ENGINE = 'access' , PARAMETERS = { \"db_file\" : \"example_db.accdb\" } ;","title":"Microsoft Access"},{"location":"sql/create/databases/#microsoft-sql-server","text":"Template Example CREATE DATABASE mssql_datasource --- display name for the database WITH ENGINE = 'mssql' , --- name of the MindsDB handler PARAMETERS = { \"host\" : \" \" , --- host name or IP address \"port\" : , --- port used to make TCP/IP connection \"database\" : \" \" , --- database name \"user\" : \" \" , --- database user \"password\" : \" \" --- database password } ; CREATE DATABASE mssql_datasource WITH ENGINE = 'mssql' , PARAMETERS = { \"host\" : \"127.0.0.1\" , \"port\" : 1433 , \"database\" : \"master\" , \"user\" : \"sa\" , \"password\" : \"password\" } ;","title":"Microsoft SQL Server"},{"location":"sql/create/databases/#monetdb","text":"Template Example CREATE DATABASE monetdb_datasource --- display name for the database WITH ENGINE = 'monetdb' , --- name of the MindsDB handler PARAMETERS = { \"host\" : \" \" , --- host name or IP address \"port\" : , --- port used to make TCP/IP connection \"database\" : \" \" , --- database name \"user\" : \" \" , --- database user \"password\" : \" \" , --- database password \"schema_name\" : \" \" --- database schema name (defaults to the current schema if left blank) } ; CREATE DATABASE monetdb_datasource WITH ENGINE = 'monetdb' , PARAMETERS = { \"host\" : \"127.0.0.1\" , \"port\" : 50000 , \"database\" : \"demo\" , \"user\" : \"monetdb\" , \"password\" : \"password\" , \"schema_name\" : \"sys\" } ;","title":"MonetDB"},{"location":"sql/create/databases/#mongodb","text":"Template Example CREATE DATABASE mongo_datasource --- display name for the database WITH ENGINE = 'mongo' , --- name of the MindsDB handler PARAMETERS = { \"host\" : \" \" , --- host name or IP address \"port\" : , --- port used to make TCP/IP connection \"user\" : \" \" , --- database user \"password\" : \" \" --- database password } ; CREATE DATABASE mongo_datasource WITH ENGINE = 'mongo' , PARAMETERS = { \"host\" : \"127.0.0.1\" , \"port\" : 27017 , \"user\" : \"mongo\" , \"password\" : \"password\" } ; Follow the Mongo API documentation for details.","title":"MongoDB"},{"location":"sql/create/databases/#mysql","text":"Template Example CREATE DATABASE mysql_datasource --- display name for the database WITH ENGINE = 'mysql' , --- name of the MindsDB handler PARAMETERS = { \"host\" : \" \" , --- host name or IP address \"port\" : , --- port used to make TCP/IP connection \"database\" : \" \" , --- database name \"user\" : \" \" , --- database user \"password\" : \" \" , --- database password \"ssl\" : , --- optional, the `ssl` parameter value indicates whether SSL is enabled (`True`) or disabled (`False`) \"ssl_ca\" : { --- optional, SSL Certificate Authority \"path\" : \" \" --- either \"path\" or \"url\" } , \"ssl_cert\" : { --- optional, SSL certificates \"url\" : \" \" --- either \"path\" or \"url\" } , \"ssl_key\" : { --- optional, SSL keys \"path\" : \" \" --- either \"path\" or \"url\" } } ; CREATE DATABASE mysql_datasource WITH ENGINE = 'mysql' , PARAMETERS = { \"host\" : \"127.0.0.1\" , \"port\" : 3306 , \"database\" : \"mysql\" , \"user\" : \"root\" , \"password\" : \"password\" } ;","title":"MySQL"},{"location":"sql/create/databases/#oracle","text":"Template Example CREATE DATABASE oracle_datasource --- display name for the database WITH ENGINE = 'oracle' , --- name of the MindsDB handler PARAMETERS = { \"host\" : \" \" , --- host name or IP address \"port\" : , --- port used to make TCP/IP connection \"sid\" : \" \" , --- unique identifier of the database instance \"service_name\" : \" \" , --- optional, database service name (must be provided when \"sid\" is left blank) \"user\" : \" \" , --- database user \"password\" : \" \" --- database password } ; CREATE DATABASE oracle_datasource WITH ENGINE = 'oracle' , PARAMETERS = { \"host\" : \"127.0.0.1\" , \"port\" : 1521 , \"sid\" : \"ORCL\" , \"user\" : \"sys\" , \"password\" : \"password\" } ;","title":"Oracle"},{"location":"sql/create/databases/#pinot","text":"Template Example CREATE DATABASE pinot_datasource --- display name for the database WITH ENGINE = 'pinot' , --- name of the MindsDB handler PARAMETERS = { \"host\" : \" \" , --- host name or IP address of the Apache Pinot cluster \"broker_port\" : , --- port where the broker of the Apache Pinot cluster runs \"controller_port\" : , --- port where the controller of the Apache Pinot cluster runs \"path\" : \" \" , --- query path \"scheme\" : \" \" , --- scheme (defaults to `http` if left blank) \"username\" : \" \" , --- optional, user \"password\" : \" \" , --- optional, password \"verify_ssl\" : \" \" --- optional, verify SSL } ; CREATE DATABASE pinot_datasource WITH ENGINE = 'pinot' , PARAMETERS = { \"host\" : \"127.0.0.1\" , \"broker_port\" : 8000 , \"controller_port\" : 9000 , \"path\" : \"/query/sql\" , \"scheme\" : \"http\" } ;","title":"Pinot"},{"location":"sql/create/databases/#postgresql","text":"Template Example CREATE DATABASE psql_datasource --- display name for the database WITH ENGINE = 'postgres' , --- name of the MindsDB handler PARAMETERS = { \"host\" : \" \" , --- host name or IP address \"port\" : , --- port used to make TCP/IP connection \"database\" : \" \" , --- database name \"user\" : \" \" , --- database user \"password\" : \" \" --- database password } ; CREATE DATABASE psql_datasource WITH ENGINE = 'postgres' , PARAMETERS = { \"host\" : \"127.0.0.1\" , \"port\" : 5432 , \"database\" : \"postgres\" , \"user\" : \"postgres\" , \"password\" : \"password\" } ;","title":"PostgreSQL"},{"location":"sql/create/databases/#questdb","text":"Template Example CREATE DATABASE questdb_datasource --- display name for the database WITH ENGINE = 'questdb' , --- name of the MindsDB handler PARAMETERS = { \"host\" : \" \" , --- host name or IP address \"port\" : , --- port used to make TCP/IP connection \"database\" : \" \" , --- database name \"user\" : \" \" , --- database user \"password\" : \" \" , --- database password \"public\" : --- value of `True` or `False` (defaults to `True` if left blank) } ; CREATE DATABASE questdb_datasource WITH ENGINE = 'questdb' , PARAMETERS = { \"host\" : \"127.0.0.1\" , \"port\" : 8812 , \"database\" : \"qdb\" , \"user\" : \"admin\" , \"password\" : \"password\" } ;","title":"QuestDB"},{"location":"sql/create/databases/#s3","text":"Template Example CREATE DATABASE amazons3_datasource --- display name for the database WITH ENGINE = 's3' , --- name of the MindsDB handler PARAMETERS = { \"aws_access_key_id\" : \" \" , --- the AWS access key \"aws_secret_access_key\" : \" \" , --- the AWS secret access key \"region_name\" : \" \" , --- the AWS region \"bucket\" : \" \" , --- name of the S3 bucket \"key\" : \" \" , --- key of the object to be queried \"input_serialization\" : \" \" --- format of the data to be queried } ; CREATE DATABASE amazons3_datasource WITH ENGINE = 's3' , PARAMETERS = { \"aws_access_key_id\" : \"PCAQ2LJDOSWLNSQKOCPW\" , \"aws_secret_access_key\" : \"U/VjewPlNopsDmmwItl34r2neyC6WhZpUiip57i\" , \"region_name\" : \"us-east-1\" , \"bucket\" : \"mindsdb-bucket\" , \"key\" : \"iris.csv\" , \"input_serialization\" : \"{'CSV': {'FileHeaderInfo': 'NONE'}}\" } ;","title":"S3"},{"location":"sql/create/databases/#sap-hana","text":"Template Example CREATE DATABASE sap_hana_datasource --- display name for the database WITH ENGINE = 'hana' , --- name of the MindsDB handler PARAMETERS = { \"host\" : \" \" , --- host name or IP address \"port\" : , --- port used to make TCP/IP connection \"user\" : \" \" , --- user \"password\" : \" \" , --- password \"schema\" : \" \" , --- database schema name (defaults to the current schema if left blank) \"encrypt\" : --- indicates whether connection is encrypted (required for cloud usage) } ; CREATE DATABASE sap_hana_datasource WITH ENGINE = 'hana' , PARAMETERS = { \"host\" : \"<uuid>.hana.trial-us10.hanacloud.ondemand.com\" , \"port\" : \"443\" , \"user\" : \"DBADMIN\" , \"password\" : \"password\" , \"schema\" : \"MINDSDB\" , \"encrypt\" : True } ;","title":"SAP HANA"},{"location":"sql/create/databases/#scylla","text":"Template Example CREATE DATABASE scylladb_datasource --- display name for the database WITH ENGINE = 'scylladb' , --- name of the MindsDB handler PARAMETERS = { \"host\" : \" \" , --- host name or IP address \"port\" : , --- port used to make TCP/IP connection \"user\" : \" \" , --- user \"password\" : \" \" , --- password \"protocol_version\" : , --- optional, protocol version (defaults to 4 if left blank) \"keyspace\" : \" \" , --- keyspace name (it is the top level container for tables) \"secure_connect_bundle\" : { --- secure connect bundle file \"path\" : \" \" --- either \"path\" or \"url\" } } ; CREATE DATABASE scylladb_datasource WITH ENGINE = 'scylladb' , PARAMETERS = { \"host\" : \"127.0.0.1\" , \"port\" : 7199 , \"user\" : \"user@mindsdb.com\" , \"password\" : \"password\" , \"protocol_version\" : 4 , \"keyspace\" : \"keyspace_name\" , \"secure_connect_bundle\" : { \"path\" : \"/home/zoran/Downloads/secure-connect-mindsdb.zip\" } } ;","title":"Scylla"},{"location":"sql/create/databases/#singlestore","text":"Template Example CREATE DATABASE singlestore_datasource --- display name for the database WITH ENGINE = 'singlestore' , --- name of the MindsDB handler PARAMETERS = { \"host\" : \" \" , --- host name or IP address \"port\" : , --- port used to make TCP/IP connection \"database\" : \" \" , --- database name \"user\" : \" \" , --- database user \"password\" : \" \" , --- database password \"ssl\" : , --- optional, the `ssl` parameter value indicates whether SSL is enabled (`True`) or disabled (`False`) \"ssl_ca\" : { --- optional, SSL Certificate Authority \"path\" : \" \" --- either \"path\" or \"url\" } , \"ssl_cert\" : { --- optional, SSL certificates \"url\" : \" \" --- either \"path\" or \"url\" } , \"ssl_key\" : { --- optional, SSL keys \"path\" : \" \" --- either \"path\" or \"url\" } } ; CREATE DATABASE singlestore_datasource WITH ENGINE = 'singlestore' , PARAMETERS = { \"host\" : \"127.0.0.1\" , \"port\" : 3306 , \"database\" : \"singlestore\" , \"user\" : \"root\" , \"password\" : \"password\" } ;","title":"SingleStore"},{"location":"sql/create/databases/#snowflake","text":"Template Example CREATE DATABASE snowflake_datasource --- display name for the database WITH ENGINE = 'snowflake' , --- name of the MindsDB handler PARAMETERS = { \"host\" : \" \" , --- host name or IP address \"port\" : , --- port used to make TCP/IP connection \"database\" : \" \" , --- database name \"user\" : \" \" , --- database user \"password\" : \" \" , --- database password \"account\" : \" \" , --- the Snowflake account \"schema\" : \" \" , --- schema name (defaults to `public` if left blank) \"protocol\" : \" \" , --- protocol (defaults to `https` if left blank) \"warehouse\" : \" \" --- the warehouse account } ; CREATE DATABASE snowflake_datasource WITH ENGINE = 'snowflake' , PARAMETERS = { \"host\" : \"127.0.0.1\" , \"port\" : 443 , \"database\" : \"snowflake\" , \"user\" : \"user\" , \"password\" : \"password\" , \"account\" : \"account\" , \"schema\" : \"public\" , \"protocol\" : \"https\" , \"warehouse\" : \"warehouse\" } ;","title":"Snowflake"},{"location":"sql/create/databases/#sqlite","text":"Template Example CREATE DATABASE sqlite_datasource --- display name for the database WITH ENGINE = 'sqlite' , --- name of the MindsDB handler PARAMETERS = { \"db_file\" : \" \" --- path to the database file to be used } ; CREATE DATABASE sqlite_datasource WITH ENGINE = 'sqlite' , PARAMETERS = { \"db_file\" : \"example.db\" } ;","title":"SQLite"},{"location":"sql/create/databases/#supabase","text":"Template Example CREATE DATABASE supabase_datasource --- display name for the database WITH ENGINE = 'supabase' , --- name of the MindsDB handler PARAMETERS = { \"host\" : \" \" , --- host name or IP address \"port\" : , --- port used to make TCP/IP connection \"database\" : \" \" , --- database name \"user\" : \" \" , --- database user \"password\" : \" \" , --- database password } ; CREATE DATABASE supabase_datasource WITH ENGINE = 'supabase' , PARAMETERS = { \"host\" : \"127.0.0.1\" , \"port\" : 54321 , \"database\" : \"test\" , \"user\" : \"supabase\" , \"password\" : \"password\" } ;","title":"Supabase"},{"location":"sql/create/databases/#teradata","text":"Template Example CREATE DATABASE display_name --- display name for the database WITH ENGINE = 'teradata' , --- name of the MindsDB handler PARAMETERS = { \"host\" : \" \" , --- host name or IP address \"user\" : \" \" , --- database user \"password\" : \" \" , --- database password \"database\" : \" \" , --- database name \"port\" : --- port used to make TCP/IP connection } ; CREATE DATABASE display_name WITH ENGINE = 'teradata' , PARAMETERS = { \"host\" : \"127.0.0.1\" , \"user\" : \"teradata\" , \"password\" : \"password\" , \"database\" : \"teradata_db\" , \"port\" : 1025 } ;","title":"Teradata"},{"location":"sql/create/databases/#tidb","text":"Template Example CREATE DATABASE tidb_datasource --- display name for the database WITH ENGINE = 'tidb' , --- name of the MindsDB handler PARAMETERS = { \"host\" : \" \" , --- host name or IP address \"port\" : , --- port used to make TCP/IP connection \"database\" : \" \" , --- database name \"user\" : \" \" , --- database user \"password\" : \" \" , --- database password } ; CREATE DATABASE tidb_datasource WITH ENGINE = 'tidb' , PARAMETERS = { \"host\" : \"127.0.0.1\" , \"port\" : 4000 , \"database\" : \"tidb\" , \"user\" : \"root\" , \"password\" : \"password\" } ;","title":"TiDB"},{"location":"sql/create/databases/#timescaledb","text":"Template Example CREATE DATABASE display_name --- display name for the database WITH ENGINE = 'timescaledb' , --- name of the MindsDB handler PARAMETERS = { \"user\" : \" \" , --- database user \"password\" : \" \" , --- database password \"host\" : \" \" , --- host name or IP address \"port\" : , --- port used to make TCP/IP connection \"database\" : \" \" --- database name } ; CREATE DATABASE display_name WITH ENGINE = 'timescaledb' , PARAMETERS = { \"user\" : \"timescaledb\" , \"password\" : \"password\" , \"host\" : \"127.0.0.1\" , \"port\" : 36806 , \"database\" : \"timescaledb_db\" } ;","title":"Timescaledb"},{"location":"sql/create/databases/#trino","text":"Template Example 1 Example 2 CREATE DATABASE trino_datasource --- display name for the database WITH ENGINE = 'trino' , --- name of the MindsDB handler PARAMETERS = { \"host\" : \" \" , --- host name or IP address \"port\" : , --- port used to make TCP/IP connection \"auth\" : \" \" , --- optional, authentication method, currently only `basic` is supported \"http_scheme\" : \" \" , --- optional, `http`(default) or `https` \"user\" : \" \" , --- database user \"password\" : \" \" , --- database password \"catalog\" : \" \" , --- optional, catalog \"schema\" : \" \" --- optional, schema \"with\" : --- optional, default WITH-clause (properties) for ALL tables --- this parameter is experimental and might be changed or removed in future release } ; CREATE DATABASE trino_datasource WITH ENGINE = 'trino' , PARAMETERS = { \"host\" : \"127.0.0.1\" , \"port\" : 8080 , \"user\" : \"trino\" , \"password\" : \"password\" , \"catalog\" : \"default\" , \"schema\" : \"test\" } ; CREATE DATABASE trino_datasource WITH ENGINE = 'trino' , PARAMETERS = { \"host\" : \"127.0.0.1\" , \"port\" : 443 , \"auth\" : \"basic\" , \"http_scheme\" : \"https\" , \"user\" : \"trino\" , \"password\" : \"password\" , \"catalog\" : \"default\" , \"schema\" : \"test\" , \"with\" : \"with (transactional = true)\" } ;","title":"Trino"},{"location":"sql/create/databases/#vertica","text":"Template Example CREATE DATABASE vertica_datasource --- display name for the database WITH ENGINE = 'vertica' , --- name of the MindsDB handler PARAMETERS = { \"host\" : \" \" , --- host name or IP address \"port\" : , --- port used to make TCP/IP connection \"database\" : \" \" , --- database name \"user\" : \" \" , --- database user \"password\" : \" \" , --- database password \"schema_name\" : \" \" --- database schema name } ; CREATE DATABASE vertica_datasource WITH ENGINE = 'vertica' , PARAMETERS = { \"host\" : \"127.0.0.1\" , \"port\" : 5433 , \"database\" : \"VMart\" , \"user\" : \"vertica\" , \"password\" : \"password\" , \"schema_name\" : \"public\" } ;","title":"Vertica"},{"location":"sql/create/databases/#yugabyte","text":"Template Example CREATE DATABASE display_name --- display name for the database WITH ENGINE = 'yugabyte' , --- name of the MindsDB handler PARAMETERS = { \"user\" : \" \" , --- database user \"password\" : \" \" , --- database password \"host\" : \" \" , --- host name or IP address \"port\" : , --- port used to make TCP/IP connection \"database\" : \" \" --- database name } ; CREATE DATABASE display_name WITH ENGINE = 'yugabyte' , PARAMETERS = { \"user\" : \"yugabyte\" , \"password\" : \"password\" , \"host\" : \"127.0.0.1\" , \"port\" : 5433 , \"database\" : \"yugabyte_db\" } ; \"host\": \" .hana.trial-us10.hanacloud.ondemand.com\", \"port\": \"443\", \"schema\": \"MINDSDB\", \"encrypt\": true }; ```","title":"Yugabyte"},{"location":"sql/create/databases/#opengauss","text":"Template Example CREATE DATABASE opengauss_datasource --- display name for the database WITH ENGINE = 'opengauss' , --- name of the MindsDB handler PARAMETERS = { \"host\" : \" \" , --- host name or IP address \"port\" : , --- port used to make TCP/IP connection \"database\" : \" \" , --- database name \"user\" : \" \" , --- database user \"password\" : \" \" , --- database password } ; CREATE DATABASE opengauss_datasource WITH ENGINE = 'opengauss' , PARAMETERS = { \"host\" : \"127.0.0.1\" , \"port\" : 5432 , \"database\" : \"opengauss\" , \"user\" : \"mindsdb\" , \"password\" : \"password\" } ;","title":"openGauss"},{"location":"sql/create/file/","text":"Upload a File to MindsDB \u00b6 Follow the steps below to upload a file to MindsDB. Log in to your MindsDB Cloud account to open the MindsDB Editor. Navigate to Add data section by clicking the Add data button located in the top right corner. Choose the Files tab. Choose the Import File option. Upload a file (here it is house_sales.csv ), name a table used to store the file data (here it is house_sales ), and click the Save and Continue button. What's Next? \u00b6 Now, you are ready to create a predictor from a file. Make sure to check out this guide on how to do that.","title":"Upload a File to MindsDB"},{"location":"sql/create/file/#upload-a-file-to-mindsdb","text":"Follow the steps below to upload a file to MindsDB. Log in to your MindsDB Cloud account to open the MindsDB Editor. Navigate to Add data section by clicking the Add data button located in the top right corner. Choose the Files tab. Choose the Import File option. Upload a file (here it is house_sales.csv ), name a table used to store the file data (here it is house_sales ), and click the Save and Continue button.","title":"Upload a File to MindsDB"},{"location":"sql/create/file/#whats-next","text":"Now, you are ready to create a predictor from a file. Make sure to check out this guide on how to do that.","title":"What's Next?"},{"location":"sql/create/ml_engine/","text":"CREATE ML_ENGINE Statement \u00b6 You can create machine learning (ML) engines based on the ML handlers available in MindsDB. And if you can't find the ML handler of your interest, you can always contribute by building a new ML handler . Description \u00b6 The CREATE ML_ENGINE command creates an ML engine that uses one of the available ML handlers. Syntax \u00b6 Before creating an ML engine, make sure that the ML handler of your interest is available by querying for the ML handlers. SELECT * FROM information_schema . handlers ; -- or SHOW HANDLERS ; Can't Find an ML Handler? If you can't find the ML handler of your interest, you can contribute by building a new ML handler . If you find the ML handler of your interest, then you can create an ML engine using this command: CREATE ML_ENGINE ml_engine_name FROM handler_name [ USING argument_key = argument_value ]; Please replace ml_engine_name , handler_name , and optionally, argument_key and argument_value with the real values. To verify that your ML engine was successfully created, run the command below: SELECT * FROM information_schema . ml_engines ; -- or SHOW ML_ENGINES ; If you want to drop an ML engine, run the command below: DROP ML_ENGINE ml_engine_name ; Example \u00b6 Let's check what ML handlers are currently available: SHOW HANDLERS ; On execution, we get: + ------------------+------------+---------------------------------+-------+------------------------------------------------------------------------------+---------------+----------------------+------+ | NAME | TITLE | DESCRIPTION | VERSION | CONNECTION_ARGS | IMPORT_SUCCESS | IMPORT_ERROR | FIELD8 | + ------------------+------------+---------------------------------+-------+------------------------------------------------------------------------------+---------------+----------------------+------+ | merlion | Merlion | MindsDB handler for Merlion | 0 . 0 . 1 | [ NULL ] | true | [ NULL ] | | | byom | BYOM | MindsDB handler for BYOM | 0 . 0 . 1 | { 'model_code' : { 'type' : 'path' , 'description' : 'The path name to model code' }} | true | [ NULL ] | | | ludwig | Ludwig | MindsDB handler for Ludwig AutoML | 0 . 0 . 2 | [ NULL ] | false | No module named 'dask' | | | lightwood | Lightwood | [ NULL ] | 1 . 0 . 0 | [ NULL ] | true | [ NULL ] | | | huggingface | Hugging Face | MindsDB handler for Higging Face | 0 . 0 . 1 | [ NULL ] | true | [ NULL ] | | + ------------------+------------+---------------------------------+-------+------------------------------------------------------------------------------+---------------+----------------------+------+ Here we create an ML engine using the Lightwood handler. CREATE ML_ENGINE my_lightwood_engine FROM lightwood ; On execution, we get: Query successfully completed Now let's verify that our ML engine exists. SHOW ML_ENGINES ; On execution, we get: + -------------------+-----------+----------------+ | NAME | HANDLER | CONNECTION_DATA | + -------------------+-----------+----------------+ | huggingface | huggingface | { 'password' : '' } | | lightwood | lightwood | { 'password' : '' } | | my_lightwood_engine | lightwood | { 'password' : '' } | + -------------------+-----------+----------------+ Please note that we haven't used any arguments while creating the ML engine. The USING clause is optional, as it depends on the ML handler whether it requires/allows some arguments or not. After creating your ML engine, you can create a model like this: CREATE MODEL my_model FROM integration_name ( SELECT * FROM table_name ) USING engine = 'my_lightwood_engine' ; The USING clause specifies the ML engine to be used for creating a new model.","title":"ML_ENGINE"},{"location":"sql/create/ml_engine/#create-ml_engine-statement","text":"You can create machine learning (ML) engines based on the ML handlers available in MindsDB. And if you can't find the ML handler of your interest, you can always contribute by building a new ML handler .","title":"CREATE ML_ENGINE Statement"},{"location":"sql/create/ml_engine/#description","text":"The CREATE ML_ENGINE command creates an ML engine that uses one of the available ML handlers.","title":"Description"},{"location":"sql/create/ml_engine/#syntax","text":"Before creating an ML engine, make sure that the ML handler of your interest is available by querying for the ML handlers. SELECT * FROM information_schema . handlers ; -- or SHOW HANDLERS ; Can't Find an ML Handler? If you can't find the ML handler of your interest, you can contribute by building a new ML handler . If you find the ML handler of your interest, then you can create an ML engine using this command: CREATE ML_ENGINE ml_engine_name FROM handler_name [ USING argument_key = argument_value ]; Please replace ml_engine_name , handler_name , and optionally, argument_key and argument_value with the real values. To verify that your ML engine was successfully created, run the command below: SELECT * FROM information_schema . ml_engines ; -- or SHOW ML_ENGINES ; If you want to drop an ML engine, run the command below: DROP ML_ENGINE ml_engine_name ;","title":"Syntax"},{"location":"sql/create/ml_engine/#example","text":"Let's check what ML handlers are currently available: SHOW HANDLERS ; On execution, we get: + ------------------+------------+---------------------------------+-------+------------------------------------------------------------------------------+---------------+----------------------+------+ | NAME | TITLE | DESCRIPTION | VERSION | CONNECTION_ARGS | IMPORT_SUCCESS | IMPORT_ERROR | FIELD8 | + ------------------+------------+---------------------------------+-------+------------------------------------------------------------------------------+---------------+----------------------+------+ | merlion | Merlion | MindsDB handler for Merlion | 0 . 0 . 1 | [ NULL ] | true | [ NULL ] | | | byom | BYOM | MindsDB handler for BYOM | 0 . 0 . 1 | { 'model_code' : { 'type' : 'path' , 'description' : 'The path name to model code' }} | true | [ NULL ] | | | ludwig | Ludwig | MindsDB handler for Ludwig AutoML | 0 . 0 . 2 | [ NULL ] | false | No module named 'dask' | | | lightwood | Lightwood | [ NULL ] | 1 . 0 . 0 | [ NULL ] | true | [ NULL ] | | | huggingface | Hugging Face | MindsDB handler for Higging Face | 0 . 0 . 1 | [ NULL ] | true | [ NULL ] | | + ------------------+------------+---------------------------------+-------+------------------------------------------------------------------------------+---------------+----------------------+------+ Here we create an ML engine using the Lightwood handler. CREATE ML_ENGINE my_lightwood_engine FROM lightwood ; On execution, we get: Query successfully completed Now let's verify that our ML engine exists. SHOW ML_ENGINES ; On execution, we get: + -------------------+-----------+----------------+ | NAME | HANDLER | CONNECTION_DATA | + -------------------+-----------+----------------+ | huggingface | huggingface | { 'password' : '' } | | lightwood | lightwood | { 'password' : '' } | | my_lightwood_engine | lightwood | { 'password' : '' } | + -------------------+-----------+----------------+ Please note that we haven't used any arguments while creating the ML engine. The USING clause is optional, as it depends on the ML handler whether it requires/allows some arguments or not. After creating your ML engine, you can create a model like this: CREATE MODEL my_model FROM integration_name ( SELECT * FROM table_name ) USING engine = 'my_lightwood_engine' ; The USING clause specifies the ML engine to be used for creating a new model.","title":"Example"},{"location":"sql/create/predictor/","text":"CREATE MODEL Statement \u00b6 Description \u00b6 The CREATE MODEL statement creates and trains a new ML model. Syntax \u00b6 Here is the syntax: CREATE MODEL mindsdb .[ predictor_name ] FROM [ integration_name ] ( SELECT [ column_name , ...] FROM [ table_name ]) PREDICT [ target_column ] [ USING [ parameter_key ] = [ 'parameter_value' ]]; On execution, we get: Query OK , 0 rows affected ( x . xxx sec ) Where: Expressions Description [predictor_name] Name of the model to be created. [integration_name] Name of the integration created using the CREATE DATABASE statement or file upload . (SELECT [column_name, ...] FROM [table_name]) SELECT statement for selecting data to be used for training and validation. PREDICT [target_column] target_column is the column to be predicted. Checking Model Status After you run the CREATE MODEL statement, you can check the status of the training process by querying the mindsdb . models table. SELECT * FROM mindsdb . models WHERE name = '[predictor_name]' ; On execution, we get: + ------------------------+-----------------------------------+----------------------------------------+-----------------------+-------------+---------------+-----+-----------------+----------------+ | name | status | accuracy | predict | update_status | mindsdb_version | error | select_data_query | training_options | + ------------------------+-----------------------------------+----------------------------------------+-----------------------+-------------+---------------+-----+-----------------+----------------+ | predictor_name | generating or training or complete | number depending on the accuracy metric | column_to_be_predicted | up_to_date | 22 . 7 . 5 . 0 | | | | + ------------------------+-----------------------------------+----------------------------------------+-----------------------+-------------+---------------+-----+-----------------+----------------+ Example \u00b6 This example shows how to create and train a machine learning model called home_rentals_model and predict the rental prices for real estate properties inside the dataset. CREATE MODEL mindsdb . home_rentals_model FROM db_integration ( SELECT * FROM house_rentals_data ) PREDICT rental_price ; On execution, we get: Query OK , 0 rows affected ( x . xxx sec ) To check the predictor status, query the mindsdb . models table. SELECT * FROM mindsdb . models WHERE name = 'home_rentals_model' ; On execution, we get: + --------------------+----------+--------------------+--------------+---------------+-----------------+-------+-------------------+------------------+ | name | status | accuracy | predict | update_status | mindsdb_version | error | select_data_query | training_options | + --------------------+----------+--------------------+--------------+---------------+-----------------+-------+-------------------+------------------+ | home_rentals_model | complete | 0 . 9991920992432087 | rental_price | up_to_date | 22 . 5 . 1 . 0 | NULL | | | + --------------------+----------+--------------------+--------------+---------------+-----------------+-------+-------------------+------------------+ CREATE MODEL with the USING Statement \u00b6 Description \u00b6 In MindsDB, the underlying AutoML models are based on the Lightwood engine by default. This library generates models automatically based on the data and declarative problem definition. But the default configuration can be overridden using the USING statement that provides an option to configure specific parameters of the training process. In the upcoming version of MindsDB, it will be possible to choose another ML framework. Please note that the Lightwood engine is used by default. Syntax \u00b6 Here is the syntax: CREATE MODEL mindsdb .[ predictor_name ] FROM [ integration_name ] ( SELECT [ column_name , ...] FROM [ table_name ]) PREDICT [ target_column ] USING [ parameter_key ] = [ 'parameter_value' ]; On execution, we get: Query OK , 0 rows affected ( x . xxx sec ) encoders Key \u00b6 It grants access to configure how each column is encoded. By default, the AutoML engine tries to get the best match for the data. ... USING encoders .[ column_name ]. module = 'value' ; To learn more about encoders and their options, visit the Lightwood documentation page on encoders . model Key \u00b6 It allows you to specify the type of machine learning algorithm to learn from the encoder data. ... USING model . args = { \"key\" : value } ; Module options: Module Description BaseMixer Base class for all mixers. LightGBM This mixer configures and uses LightGBM for regression or classification tasks depending on the problem definition. LightGBMArray This mixer consists of several LightGBM mixers in regression mode aimed at time series forecasting tasks. NHitsMixer Wrapper around an MQN-HITS deep learning model. Neural The Neural mixer trains a fully connected dense network from concatenated encoded outputs of each feature in the dataset to predict the encoded output. ProphetMixer This mixer is a wrapper around the popular time series library sktime. Regression The Regression mixer inherits from scikit-learn\u2019s Ridge class . SkTime This mixer is a wrapper around the popular time series library sktime. Unit Special mixer that passes along whatever prediction is made by the target encoder without modifications. It is used for single-column predictive scenarios that may involve complex and/or expensive encoders (e.g. free-form text classification with transformers). To learn more about all the model options, visit the Lightwood documentation page . Other Keys Supported by Lightwood in JsonAI \u00b6 The most common use cases of configuring predictors use encoders and model keys explained above. To see all the available keys, check out the Lightwood documentation page on JsonAI . Example \u00b6 Here we use the home_rentals dataset and specify particular encoders for some columns and a LightGBM model . CREATE MODEL mindsdb . home_rentals_model FROM db_integration ( SELECT * FROM home_rentals ) PREDICT rental_price USING encoders . location . module = 'CategoricalAutoEncoder' , encoders . rental_price . module = 'NumericEncoder' , encoders . rental_price . args . positive_domain = 'True' , model . args = { \"submodels\" :[ { \"module\" : \"LightGBM\" , \"args\" : { \"stop_after\" : 12 , \"fit_on_dev\" : true } } ] } ; On execution, we get: Query OK , 0 rows affected ( x . xxx sec ) CREATE MODEL From File \u00b6 To create a predictor from a file, you should first upload a file to MindsDB. Follow this guide to see how to do that. Description \u00b6 This statement is used to create and train a model from a file or a database table. Syntax \u00b6 Here is the syntax: CREATE MODEL mindsdb .[ predictor_name ] FROM files ( SELECT * FROM [ file_name ]) PREDICT target_column ; On execution, we get: Query OK , 0 rows affected ( x . xxx sec ) Where: Name Description [predictor_name] Name of the model to be created. [file_name] Name of the file uploaded via the MindsDB editor. (SELECT * FROM [file_name]) SELECT statement for selecting the data to be used for training and validation. target_column target_column is the column to be predicted. Example \u00b6 Here we uploaded the home_rentals dataset as a file. CREATE MODEL mindsdb . home_rentals_model FROM files ( SELECT * from home_rentals ) PREDICT rental_price ; On execution, we get: Query OK , 0 rows affected ( x . xxx sec ) CREATE MODEL For Time Series Models \u00b6 Description \u00b6 To train a time series model, MindsDB provides additional statements. Syntax \u00b6 Here is the syntax: CREATE MODEL mindsdb .[ predictor_name ] FROM [ integration_name ] ( SELECT [ sequential_column ], [ partition_column ], [ other_column ], [ target_column ] FROM [ table_name ]) PREDICT [ target_column ] ORDER BY [ sequential_column ] GROUP BY [ partition_column ] WINDOW [ int ] HORIZON [ int ]; On execution, we get: Query OK , 0 rows affected ( x . xxx sec ) Where: Expressions Description ORDER BY [sequential_column] The column by which time series is ordered. It can be a date or anything that defines the sequence of events. GROUP BY [partition_column] It is optional. The column by which rows that make a partition are grouped. For example, if you want to forecast the inventory for all items in the store, you can partition the data by product_id , so each distinct product_id has its own time series. WINDOW [int] The number of rows to look back at when making a prediction. It comes after the rows are ordered by the column defined in ORDER BY and split into groups by the column(s) defined in GROUP BY . This could be interpreted as \"Always use the previous 10 rows\". HORIZON [int] It is optional. The number of future predictions (it is 1 by default). Getting a Prediction from a Time Series Model Due to the nature of time series forecasting, you need to use the JOIN statement to get results. Example \u00b6 Here is an example: CREATE MODEL mindsdb . inventory_model FROM db_integration ( SELECT * FROM inventory ) PREDICT units_in_inventory ORDER BY date GROUP BY product_id WINDOW 20 HORIZON 7 ; On execution, we get: Query OK , 0 rows affected ( x . xxx sec ) Now, to get the results, we use the JOIN statement. SELECT im . product_id , im . date , im . units_in_inventory AS predicted_units_in_inventory FROM db_integration . inventory AS i JOIN mindsdb . inventory_model AS im WHERE i . date > LATEST LIMIT 10 ; The data source table ( db_integration.inventory ) and the predictor table ( mindsdb.inventory_model ) are joined to let us fetch the predictions for future dates.","title":"MODEL"},{"location":"sql/create/predictor/#create-model-statement","text":"","title":"CREATE MODEL Statement"},{"location":"sql/create/predictor/#description","text":"The CREATE MODEL statement creates and trains a new ML model.","title":"Description"},{"location":"sql/create/predictor/#syntax","text":"Here is the syntax: CREATE MODEL mindsdb .[ predictor_name ] FROM [ integration_name ] ( SELECT [ column_name , ...] FROM [ table_name ]) PREDICT [ target_column ] [ USING [ parameter_key ] = [ 'parameter_value' ]]; On execution, we get: Query OK , 0 rows affected ( x . xxx sec ) Where: Expressions Description [predictor_name] Name of the model to be created. [integration_name] Name of the integration created using the CREATE DATABASE statement or file upload . (SELECT [column_name, ...] FROM [table_name]) SELECT statement for selecting data to be used for training and validation. PREDICT [target_column] target_column is the column to be predicted. Checking Model Status After you run the CREATE MODEL statement, you can check the status of the training process by querying the mindsdb . models table. SELECT * FROM mindsdb . models WHERE name = '[predictor_name]' ; On execution, we get: + ------------------------+-----------------------------------+----------------------------------------+-----------------------+-------------+---------------+-----+-----------------+----------------+ | name | status | accuracy | predict | update_status | mindsdb_version | error | select_data_query | training_options | + ------------------------+-----------------------------------+----------------------------------------+-----------------------+-------------+---------------+-----+-----------------+----------------+ | predictor_name | generating or training or complete | number depending on the accuracy metric | column_to_be_predicted | up_to_date | 22 . 7 . 5 . 0 | | | | + ------------------------+-----------------------------------+----------------------------------------+-----------------------+-------------+---------------+-----+-----------------+----------------+","title":"Syntax"},{"location":"sql/create/predictor/#example","text":"This example shows how to create and train a machine learning model called home_rentals_model and predict the rental prices for real estate properties inside the dataset. CREATE MODEL mindsdb . home_rentals_model FROM db_integration ( SELECT * FROM house_rentals_data ) PREDICT rental_price ; On execution, we get: Query OK , 0 rows affected ( x . xxx sec ) To check the predictor status, query the mindsdb . models table. SELECT * FROM mindsdb . models WHERE name = 'home_rentals_model' ; On execution, we get: + --------------------+----------+--------------------+--------------+---------------+-----------------+-------+-------------------+------------------+ | name | status | accuracy | predict | update_status | mindsdb_version | error | select_data_query | training_options | + --------------------+----------+--------------------+--------------+---------------+-----------------+-------+-------------------+------------------+ | home_rentals_model | complete | 0 . 9991920992432087 | rental_price | up_to_date | 22 . 5 . 1 . 0 | NULL | | | + --------------------+----------+--------------------+--------------+---------------+-----------------+-------+-------------------+------------------+","title":"Example"},{"location":"sql/create/predictor/#create-model-with-the-using-statement","text":"","title":"CREATE MODEL with the USING Statement"},{"location":"sql/create/predictor/#description_1","text":"In MindsDB, the underlying AutoML models are based on the Lightwood engine by default. This library generates models automatically based on the data and declarative problem definition. But the default configuration can be overridden using the USING statement that provides an option to configure specific parameters of the training process. In the upcoming version of MindsDB, it will be possible to choose another ML framework. Please note that the Lightwood engine is used by default.","title":"Description"},{"location":"sql/create/predictor/#syntax_1","text":"Here is the syntax: CREATE MODEL mindsdb .[ predictor_name ] FROM [ integration_name ] ( SELECT [ column_name , ...] FROM [ table_name ]) PREDICT [ target_column ] USING [ parameter_key ] = [ 'parameter_value' ]; On execution, we get: Query OK , 0 rows affected ( x . xxx sec )","title":"Syntax"},{"location":"sql/create/predictor/#encoders-key","text":"It grants access to configure how each column is encoded. By default, the AutoML engine tries to get the best match for the data. ... USING encoders .[ column_name ]. module = 'value' ; To learn more about encoders and their options, visit the Lightwood documentation page on encoders .","title":"encoders Key"},{"location":"sql/create/predictor/#model-key","text":"It allows you to specify the type of machine learning algorithm to learn from the encoder data. ... USING model . args = { \"key\" : value } ; Module options: Module Description BaseMixer Base class for all mixers. LightGBM This mixer configures and uses LightGBM for regression or classification tasks depending on the problem definition. LightGBMArray This mixer consists of several LightGBM mixers in regression mode aimed at time series forecasting tasks. NHitsMixer Wrapper around an MQN-HITS deep learning model. Neural The Neural mixer trains a fully connected dense network from concatenated encoded outputs of each feature in the dataset to predict the encoded output. ProphetMixer This mixer is a wrapper around the popular time series library sktime. Regression The Regression mixer inherits from scikit-learn\u2019s Ridge class . SkTime This mixer is a wrapper around the popular time series library sktime. Unit Special mixer that passes along whatever prediction is made by the target encoder without modifications. It is used for single-column predictive scenarios that may involve complex and/or expensive encoders (e.g. free-form text classification with transformers). To learn more about all the model options, visit the Lightwood documentation page .","title":"model Key"},{"location":"sql/create/predictor/#other-keys-supported-by-lightwood-in-jsonai","text":"The most common use cases of configuring predictors use encoders and model keys explained above. To see all the available keys, check out the Lightwood documentation page on JsonAI .","title":"Other Keys Supported by Lightwood in JsonAI"},{"location":"sql/create/predictor/#example_1","text":"Here we use the home_rentals dataset and specify particular encoders for some columns and a LightGBM model . CREATE MODEL mindsdb . home_rentals_model FROM db_integration ( SELECT * FROM home_rentals ) PREDICT rental_price USING encoders . location . module = 'CategoricalAutoEncoder' , encoders . rental_price . module = 'NumericEncoder' , encoders . rental_price . args . positive_domain = 'True' , model . args = { \"submodels\" :[ { \"module\" : \"LightGBM\" , \"args\" : { \"stop_after\" : 12 , \"fit_on_dev\" : true } } ] } ; On execution, we get: Query OK , 0 rows affected ( x . xxx sec )","title":"Example"},{"location":"sql/create/predictor/#create-model-from-file","text":"To create a predictor from a file, you should first upload a file to MindsDB. Follow this guide to see how to do that.","title":"CREATE MODEL From File"},{"location":"sql/create/predictor/#description_2","text":"This statement is used to create and train a model from a file or a database table.","title":"Description"},{"location":"sql/create/predictor/#syntax_2","text":"Here is the syntax: CREATE MODEL mindsdb .[ predictor_name ] FROM files ( SELECT * FROM [ file_name ]) PREDICT target_column ; On execution, we get: Query OK , 0 rows affected ( x . xxx sec ) Where: Name Description [predictor_name] Name of the model to be created. [file_name] Name of the file uploaded via the MindsDB editor. (SELECT * FROM [file_name]) SELECT statement for selecting the data to be used for training and validation. target_column target_column is the column to be predicted.","title":"Syntax"},{"location":"sql/create/predictor/#example_2","text":"Here we uploaded the home_rentals dataset as a file. CREATE MODEL mindsdb . home_rentals_model FROM files ( SELECT * from home_rentals ) PREDICT rental_price ; On execution, we get: Query OK , 0 rows affected ( x . xxx sec )","title":"Example"},{"location":"sql/create/predictor/#create-model-for-time-series-models","text":"","title":"CREATE MODEL For Time Series Models"},{"location":"sql/create/predictor/#description_3","text":"To train a time series model, MindsDB provides additional statements.","title":"Description"},{"location":"sql/create/predictor/#syntax_3","text":"Here is the syntax: CREATE MODEL mindsdb .[ predictor_name ] FROM [ integration_name ] ( SELECT [ sequential_column ], [ partition_column ], [ other_column ], [ target_column ] FROM [ table_name ]) PREDICT [ target_column ] ORDER BY [ sequential_column ] GROUP BY [ partition_column ] WINDOW [ int ] HORIZON [ int ]; On execution, we get: Query OK , 0 rows affected ( x . xxx sec ) Where: Expressions Description ORDER BY [sequential_column] The column by which time series is ordered. It can be a date or anything that defines the sequence of events. GROUP BY [partition_column] It is optional. The column by which rows that make a partition are grouped. For example, if you want to forecast the inventory for all items in the store, you can partition the data by product_id , so each distinct product_id has its own time series. WINDOW [int] The number of rows to look back at when making a prediction. It comes after the rows are ordered by the column defined in ORDER BY and split into groups by the column(s) defined in GROUP BY . This could be interpreted as \"Always use the previous 10 rows\". HORIZON [int] It is optional. The number of future predictions (it is 1 by default). Getting a Prediction from a Time Series Model Due to the nature of time series forecasting, you need to use the JOIN statement to get results.","title":"Syntax"},{"location":"sql/create/predictor/#example_3","text":"Here is an example: CREATE MODEL mindsdb . inventory_model FROM db_integration ( SELECT * FROM inventory ) PREDICT units_in_inventory ORDER BY date GROUP BY product_id WINDOW 20 HORIZON 7 ; On execution, we get: Query OK , 0 rows affected ( x . xxx sec ) Now, to get the results, we use the JOIN statement. SELECT im . product_id , im . date , im . units_in_inventory AS predicted_units_in_inventory FROM db_integration . inventory AS i JOIN mindsdb . inventory_model AS im WHERE i . date > LATEST LIMIT 10 ; The data source table ( db_integration.inventory ) and the predictor table ( mindsdb.inventory_model ) are joined to let us fetch the predictions for future dates.","title":"Example"},{"location":"sql/create/table/","text":"CREATE TABLE Statement \u00b6 Description \u00b6 The CREATE TABLE statement creates a table and fills it with a subselect query output. It is usually used to materialize prediction results as tables. Syntax \u00b6 You can use the usual CREATE TABLE statement: CREATE TABLE [ integration_name ].[ table_name ] ( SELECT ...); Or the CREATE OR REPLACE TABLE statement: CREATE OR REPLACE TABLE [ integration_name ].[ table_name ] ( SELECT ...); Here are the steps followed by the syntax: It executes a subselect query to get the output dataset. In the case of the CREATE OR REPLACE TABLE statement, the [integration_name].[table_name] table is dropped before recreating it. It (re)creates the [integration_name].[table_name] table inside the [ integration_name ] integration. It uses the INSERT INTO statement to insert the output of the ( SELECT ...) query into the [integration_name].[table_name] . On execution, we get: Query OK , 0 rows affected ( x . xxx sec ) Example \u00b6 We want to save the prediction results into the int1 . tbl1 table. Here is the schema structure used throughout this example: int1 \u2514\u2500\u2500 tbl1 mindsdb \u2514\u2500\u2500 predictor_name int2 \u2514\u2500\u2500 tbl2 Where: Name Description int1 Integration where the table that stores prediction results resides. tbl1 Table that stores prediction results. predictor_name Name of the model. int2 Integration where the data source table used in the inner SELECT statement resides. tbl2 Data source table used in the inner SELECT statement. Let's execute the query. CREATE OR REPLACE TABLE int1 . tbl1 ( SELECT * FROM int2 . tbl2 AS ta JOIN mindsdb . predictor_name AS tb WHERE ta . date > '2015-12-31' ); On execution, we get: Query OK , 0 rows affected ( x . xxx sec )","title":"TABLE"},{"location":"sql/create/table/#create-table-statement","text":"","title":"CREATE TABLE Statement"},{"location":"sql/create/table/#description","text":"The CREATE TABLE statement creates a table and fills it with a subselect query output. It is usually used to materialize prediction results as tables.","title":"Description"},{"location":"sql/create/table/#syntax","text":"You can use the usual CREATE TABLE statement: CREATE TABLE [ integration_name ].[ table_name ] ( SELECT ...); Or the CREATE OR REPLACE TABLE statement: CREATE OR REPLACE TABLE [ integration_name ].[ table_name ] ( SELECT ...); Here are the steps followed by the syntax: It executes a subselect query to get the output dataset. In the case of the CREATE OR REPLACE TABLE statement, the [integration_name].[table_name] table is dropped before recreating it. It (re)creates the [integration_name].[table_name] table inside the [ integration_name ] integration. It uses the INSERT INTO statement to insert the output of the ( SELECT ...) query into the [integration_name].[table_name] . On execution, we get: Query OK , 0 rows affected ( x . xxx sec )","title":"Syntax"},{"location":"sql/create/table/#example","text":"We want to save the prediction results into the int1 . tbl1 table. Here is the schema structure used throughout this example: int1 \u2514\u2500\u2500 tbl1 mindsdb \u2514\u2500\u2500 predictor_name int2 \u2514\u2500\u2500 tbl2 Where: Name Description int1 Integration where the table that stores prediction results resides. tbl1 Table that stores prediction results. predictor_name Name of the model. int2 Integration where the data source table used in the inner SELECT statement resides. tbl2 Data source table used in the inner SELECT statement. Let's execute the query. CREATE OR REPLACE TABLE int1 . tbl1 ( SELECT * FROM int2 . tbl2 AS ta JOIN mindsdb . predictor_name AS tb WHERE ta . date > '2015-12-31' ); On execution, we get: Query OK , 0 rows affected ( x . xxx sec )","title":"Example"},{"location":"sql/create/view/","text":"CREATE VIEW Statement \u00b6 Description \u00b6 The CREATE VIEW statement creates a view, which is a saved SELECT statement executed every time we call this view, as opposed to a table that stores its data in columns and rows. In MindsDB, the CREATE VIEW statement is commonly used to create AI Tables . An AI Table is a virtual table created by joining the data source table with the prediction model. Syntax \u00b6 Here is the syntax: CREATE VIEW [ project_name ].[ view_name ] AS ( SELECT a .[ column_name1 ], a .[ column_name2 ], a .[ column_name3 ], p .[ model_column ] AS model_column FROM [ integration_name ].[ table_name ] AS a JOIN mindsdb .[ predictor_name ] AS p ); On execution, we get: Query OK , 0 rows affected ( x . xxx sec ) Where: Name Description [project_name] Name of the project to store the view. [view_name] Name of the view. [column_name1], [column_name2], ... Columns of the data source table that are the input for the model to make predictions. [model_column] Name of the target column to be predicted. [integration_name].[table_name] Data source table name along with the integration where it resides. [predictor_name] Name of the model. Example \u00b6 Below is the query that creates and trains the home_rentals_model model to predict the rental_price value. The inner SELECT statement provides all real estate listing data used to train the model. CREATE MODEL mindsdb . home_rentals_model FROM integration ( SELECT * FROM house_rentals_data ) PREDICT rental_price ; On execution, we get: Query OK , 0 rows affected ( x . xxx sec ) Now, we can JOIN the home_rentals_data table with the home_rentals_model model to make predictions. By creating a view (using the CREATE VIEW statement) that is based on the SELECT statement joining the data and model tables, we create an AI Table. Here, the SELECT statement joins the data source table and the model table. The input data for making predictions consists of the sqft , number_of_bathrooms , and location columns. These are joined with the rental_price column that stores predicted values. CREATE VIEW mindsdb . home_rentals_predictions AS ( SELECT a . sqft , a . number_of_bathrooms , a . location , p . rental_price AS price FROM integration . home_rentals_data AS a JOIN mindsdb . home_rentals_model AS p ); On execution, we get: Query OK , 0 rows affected ( x . xxx sec ) Dataset for Training and Dataset for Joining In this example, we used the same dataset ( integration.home_rentals_data ) for training the model (see the CREATE MODEL statement above) and for joining with the model to make predictions (see the CREATE VIEW statement above). It doesn't happen like that in real-world scenarios. Normally, you use the old data to train the model, and then you join the new data with this model to make predictions. Consider the old_data dataset that stores data from the years 2019-2021 and the new_data dataset that stores data from the year 2022. We train the model with the old_data dataset like this: CREATE MODEL mindsdb . data_model FROM integration ( SELECT * FROM old_data ) PREDICT column ; Now, having the data_model model trained using the old_data dataset, we can join this model with the new_data dataset to make predictions like this: CREATE VIEW mindsdb . data_predictions AS ( SELECT a . column1 , a . column2 , a . column3 , p . column AS predicted_column FROM integration . new_data AS a JOIN mindsdb . data_model AS p ); USING VIEW \u00b6 Examples to use view Complex select on view (it is grouping in this example). It performs on mindsdb side SELECT type , last ( bedrooms ) FROM mindsdb . house_v GROUP BY 1 Creating predictor from view CREATE MODEL house_sales_model FROM mindsdb ( SELECT * FROM house_v ) PREDICT ma ORDER BY saledate GROUP BY bedrooms , type WINDOW 1 HORIZON 4 Using predictor with view SELECT * FROM mindsdb . house_v JOIN mindsdb . house_sales_model WHERE house_v . saledate > latest","title":"VIEW"},{"location":"sql/create/view/#create-view-statement","text":"","title":"CREATE VIEW Statement"},{"location":"sql/create/view/#description","text":"The CREATE VIEW statement creates a view, which is a saved SELECT statement executed every time we call this view, as opposed to a table that stores its data in columns and rows. In MindsDB, the CREATE VIEW statement is commonly used to create AI Tables . An AI Table is a virtual table created by joining the data source table with the prediction model.","title":"Description"},{"location":"sql/create/view/#syntax","text":"Here is the syntax: CREATE VIEW [ project_name ].[ view_name ] AS ( SELECT a .[ column_name1 ], a .[ column_name2 ], a .[ column_name3 ], p .[ model_column ] AS model_column FROM [ integration_name ].[ table_name ] AS a JOIN mindsdb .[ predictor_name ] AS p ); On execution, we get: Query OK , 0 rows affected ( x . xxx sec ) Where: Name Description [project_name] Name of the project to store the view. [view_name] Name of the view. [column_name1], [column_name2], ... Columns of the data source table that are the input for the model to make predictions. [model_column] Name of the target column to be predicted. [integration_name].[table_name] Data source table name along with the integration where it resides. [predictor_name] Name of the model.","title":"Syntax"},{"location":"sql/create/view/#example","text":"Below is the query that creates and trains the home_rentals_model model to predict the rental_price value. The inner SELECT statement provides all real estate listing data used to train the model. CREATE MODEL mindsdb . home_rentals_model FROM integration ( SELECT * FROM house_rentals_data ) PREDICT rental_price ; On execution, we get: Query OK , 0 rows affected ( x . xxx sec ) Now, we can JOIN the home_rentals_data table with the home_rentals_model model to make predictions. By creating a view (using the CREATE VIEW statement) that is based on the SELECT statement joining the data and model tables, we create an AI Table. Here, the SELECT statement joins the data source table and the model table. The input data for making predictions consists of the sqft , number_of_bathrooms , and location columns. These are joined with the rental_price column that stores predicted values. CREATE VIEW mindsdb . home_rentals_predictions AS ( SELECT a . sqft , a . number_of_bathrooms , a . location , p . rental_price AS price FROM integration . home_rentals_data AS a JOIN mindsdb . home_rentals_model AS p ); On execution, we get: Query OK , 0 rows affected ( x . xxx sec ) Dataset for Training and Dataset for Joining In this example, we used the same dataset ( integration.home_rentals_data ) for training the model (see the CREATE MODEL statement above) and for joining with the model to make predictions (see the CREATE VIEW statement above). It doesn't happen like that in real-world scenarios. Normally, you use the old data to train the model, and then you join the new data with this model to make predictions. Consider the old_data dataset that stores data from the years 2019-2021 and the new_data dataset that stores data from the year 2022. We train the model with the old_data dataset like this: CREATE MODEL mindsdb . data_model FROM integration ( SELECT * FROM old_data ) PREDICT column ; Now, having the data_model model trained using the old_data dataset, we can join this model with the new_data dataset to make predictions like this: CREATE VIEW mindsdb . data_predictions AS ( SELECT a . column1 , a . column2 , a . column3 , p . column AS predicted_column FROM integration . new_data AS a JOIN mindsdb . data_model AS p );","title":"Example"},{"location":"sql/create/view/#using-view","text":"Examples to use view Complex select on view (it is grouping in this example). It performs on mindsdb side SELECT type , last ( bedrooms ) FROM mindsdb . house_v GROUP BY 1 Creating predictor from view CREATE MODEL house_sales_model FROM mindsdb ( SELECT * FROM house_v ) PREDICT ma ORDER BY saledate GROUP BY bedrooms , type WINDOW 1 HORIZON 4 Using predictor with view SELECT * FROM mindsdb . house_v JOIN mindsdb . house_sales_model WHERE house_v . saledate > latest","title":"USING VIEW"},{"location":"sql/tutorials/ai-tables/","text":"AI Tables Intro \u00b6 There is an ongoing transformational shift within the modern business world from the \u201cwhat happened and why\u201d based on historical data analysis to the \u201cwhat will we predict can happen and how can we make it happen\u201d based on machine learning predictive modeling. The success of your predictions depends both on the data you have available and the models you train this data on. Data Scientists and Data Engineers need best-in-class tools to prepare the data for feature engineering, the best training models, and the best way of deploying, monitoring, and managing these implementations for optimal prediction confidence. Machine Learning (ML) Lifecycle \u00b6 The ML lifecycle can be represented as a process that consists of the data preparation phase, modeling phase, and deployment phase. The diagram below presents all the steps included in each of the stages. Companies looking to implement machine learning have found their current solutions require substantial amounts of data preparation, cleaning, and labeling, plus hard to find machine learning/AI data scientists to conduct feature engineering; build, train, and optimize models; assemble, verify, and deploy into production; and then monitor in real-time, improve, and refine. Machine learning models require multiple iterations with existing data to train. Additionally, extracting, transforming, and loading (ETL) data from one system to another is complicated, leads to multiple copies of information, and is a compliance and tracking nightmare. A recent study has shown it takes 64% of companies a month, to over a year, to deploy a machine learning model into production\u00b9. Leveraging existing databases and automating the feature engineering, building, training, and optimization of models, assembling them, and deploying them into production is called AutoML and has been gaining traction within enterprises for enabling non-experts to use machine learning models for practical applications. MindsDB brings machine learning to existing SQL databases with a concept called AI Tables. AI Tables integrate the machine learning models as virtual tables inside a database, create predictions, and can be queried with simple SQL statements. Almost instantly, time series, regression, and classification predictions can be done directly in your database. Deep Dive into the AI Tables \u00b6 Let\u2019s consider the following income table that stores the income and debt values. SELECT income , debt FROM income_table ; A simple visualization of the data present in the income table is as follows. Querying the income table to get the debt value for a particular income value results in the following. SELECT income , debt FROM income WHERE income = 80000 ; But what happens when we query the table for income value that is not present? SELECT income , debt FROM income WHERE income = 90000 ; When a table doesn\u2019t have an exact match the query will return a null value. This is where the AI Tables come into play! Let\u2019s create a debt model that allows us to approximate the debt value for any income value. We\u2019ll train this debt model using the income table\u2019s data. CREATE MODEL mindsdb . debt_model FROM income_table PREDICT debt ; MindsDB provides the CREATE MODEL statement. When we execute this statement, the predictive model works in the background, automatically creating a vector representation of the data that can be visualized as follows. Let\u2019s now look for the debt value of some random income value. To get the approximated debt value, we query the debt_model and not the income table. SELECT income , debt FROM debt_model WHERE income = 90120 ;","title":"AI Tables Intro"},{"location":"sql/tutorials/ai-tables/#ai-tables-intro","text":"There is an ongoing transformational shift within the modern business world from the \u201cwhat happened and why\u201d based on historical data analysis to the \u201cwhat will we predict can happen and how can we make it happen\u201d based on machine learning predictive modeling. The success of your predictions depends both on the data you have available and the models you train this data on. Data Scientists and Data Engineers need best-in-class tools to prepare the data for feature engineering, the best training models, and the best way of deploying, monitoring, and managing these implementations for optimal prediction confidence.","title":"AI Tables Intro"},{"location":"sql/tutorials/ai-tables/#machine-learning-ml-lifecycle","text":"The ML lifecycle can be represented as a process that consists of the data preparation phase, modeling phase, and deployment phase. The diagram below presents all the steps included in each of the stages. Companies looking to implement machine learning have found their current solutions require substantial amounts of data preparation, cleaning, and labeling, plus hard to find machine learning/AI data scientists to conduct feature engineering; build, train, and optimize models; assemble, verify, and deploy into production; and then monitor in real-time, improve, and refine. Machine learning models require multiple iterations with existing data to train. Additionally, extracting, transforming, and loading (ETL) data from one system to another is complicated, leads to multiple copies of information, and is a compliance and tracking nightmare. A recent study has shown it takes 64% of companies a month, to over a year, to deploy a machine learning model into production\u00b9. Leveraging existing databases and automating the feature engineering, building, training, and optimization of models, assembling them, and deploying them into production is called AutoML and has been gaining traction within enterprises for enabling non-experts to use machine learning models for practical applications. MindsDB brings machine learning to existing SQL databases with a concept called AI Tables. AI Tables integrate the machine learning models as virtual tables inside a database, create predictions, and can be queried with simple SQL statements. Almost instantly, time series, regression, and classification predictions can be done directly in your database.","title":"Machine Learning (ML) Lifecycle"},{"location":"sql/tutorials/ai-tables/#deep-dive-into-the-ai-tables","text":"Let\u2019s consider the following income table that stores the income and debt values. SELECT income , debt FROM income_table ; A simple visualization of the data present in the income table is as follows. Querying the income table to get the debt value for a particular income value results in the following. SELECT income , debt FROM income WHERE income = 80000 ; But what happens when we query the table for income value that is not present? SELECT income , debt FROM income WHERE income = 90000 ; When a table doesn\u2019t have an exact match the query will return a null value. This is where the AI Tables come into play! Let\u2019s create a debt model that allows us to approximate the debt value for any income value. We\u2019ll train this debt model using the income table\u2019s data. CREATE MODEL mindsdb . debt_model FROM income_table PREDICT debt ; MindsDB provides the CREATE MODEL statement. When we execute this statement, the predictive model works in the background, automatically creating a vector representation of the data that can be visualized as follows. Let\u2019s now look for the debt value of some random income value. To get the approximated debt value, we query the debt_model and not the income table. SELECT income , debt FROM debt_model WHERE income = 90120 ;","title":"Deep Dive into the AI Tables"},{"location":"sql/tutorials/bitcoin-forecasting/","text":"Forecast Bitcoin price using MindsDB \u00b6 Level: Easy Dataset: Coinbase 2017-2018 Bitcoin data Bitcoin is a digital currency that uses blockchain technology, Bitcoin can be sent from user to user on the peer-to-peer Bitcoin network without the need for intermediaries. Note that this is just a task for fun so use it at your own risk. In this tutorial, you will learn how to forecast Bitcoin using MindsDB. And all you need to know is just SQL. Behind the scenes, MindsDB will create the complete machine learning workflow, like determine, normalize & encode the data, train & test the model, etc. But we don\u2019t need to bother with all this complexity. Of course, if you want to, you can tune things manually inside MindsDB with a declarative syntax called JSON-AI, but we will not cover it in this article. DISCLAIMER: Please note that predicting Bitcoin price is just an example for showing MindsDB technology and you are solely responsible for any results you may get in real life, if you use this information for real trading purposes. Please note, that you can also follow this tutorial with other data you have. Pre-requisites \u00b6 First, you need MindsDB installed. If you want to use MindsDB locally, you need to install MindsDB with Docker or Python . However, if you want to use MindsDB without installing it locally, you can use Cloud Mindsdb . In this tutorial, I'm using MindsDB Cloud, because it is easy to set up in just 2 minutes and it has a great free tier. Second, you need a MySQL client to connect to MindsDB MYSQL API. Connect your database \u00b6 You must first connect MindsDB to the database where the record is stored. In the left navigation, click Database, click ADD DATABASE. And you must provide all the necessary parameters to connect to the database. Supported Database - select the database that you want to connect to Integrations Name - add a name to the integration, here I'm using 'mysql' but you can name it as you like Database - the database name Host - database hostname Port - database port Username - database user Password - user's password Then, click on CONNECT. The next step is to use the MySQL client to connect to MindsDB\u2019s MySQL API, train a new model, and make a prediction. Connect to MindsDB\u2019s MySQL API \u00b6 In this tutorial I'm using MySQL command-line client, but you can also follow up with the one that works the best for you, like Dbeaver. The first step is to use the MindsDB Cloud user to connect to the MindsDB MySQL API, using this command: You need to specify the hostname and user name explicitly, as well as a password for connecting. Click enter and you are connected to MindsDB API. If you have an authentication error, please make sure you are providing the email address you have used to create an account on MindsDB Cloud. Data \u00b6 Now, let's show the databases. There are 4 databases, and the MySQL database is the database that I've connected to MindsDB. Let's check the MySQL database. There are 3 tables, and in this tutorial, we will use the Bitcoin table. And let's check what is inside this table. These tables have 5 columns: date, open price, the highest price of the day, lowest price of the day, and close price. The column we want to forecast is close price. Create the model \u00b6 Now, to create the model let's move to MindsDB database. and let's see what's inside. There are 2 tables, predictors, and commands. Predictors contain your predictors record, and commands contain your last commands used. To train a new machine learning model we will need to CREATE MODEL as a new record inside the predictors table, and using this command: CREATE MODEL mindsdb . predictor_name FROM integration_name ( SELECT column_name , column_name2 FROM table_name ) as ds_name PREDICT column_target as column_alias ORDER BY column_orderby WINDOW num_window HORIZON num_horizon USING { \"is_timeseries\" : \"Yes\" } The values that we need to provide are: predictor_name (string) - The name of the model. integration_name (string) - The name of the connection to your database. ds_name (string) - the name of the dataset you want to create, it's optional if you don't specify this value MindsDB will generate by itself. column_target (string) - The feature you want to predict. column_alias - Alias name of the feature you want to predict. column_orderby - The column to order the data, for time series this should be the date/time column. num_window - keyword specifies the number of rows to \"look back\" into when making a prediction after the rows are ordered by the order_by column and split into groups. This could be used to specify something like \"Always use the previous 10 rows\". num_horizon - keyword specifies the number of future predictions. So, use this command to create the models: If there's no error, that means your model is created and training. To see if your model is finished, use this command: SELECT * FROM mindsdb . models WHERE name = predictor_name ; And values that we need to provide are: predictor_name (string) - The name of the model. If the model is finished, it will look like this. The model has been created! and the accuracy is 99%! Create the prediction \u00b6 Now you are in the last step of this tutorial, creating the prediction. To create a prediction you can use this command: SELECT target_variable , target_variable_explain FROM model_table WHERE when_data = { \"column3\" : \"value\" , \"column2\" : \"value\" } ; And you need to set these values: - target_variable - The original value of the target variable. - target_variable_confidence - Model confidence score. - target_variable_explain - JSON object that contains additional information as confidence_lower_bound, confidence_upper_bound, anomaly, truth. - when_data - The data to make the predictions from(WHERE clause params). Finally, we have created a Bitcoin forecasting model using SQL and MindsDB. Yayyy! Conclusions \u00b6 As you can see it is very easy to start making predictions with machine learning even without being a data scientist! Feel free to check this yourself! MindsDB free cloud account is fast to set up and has more than enough to give it a try. Or use the open source version if you want to.","title":"Forecast Bitcoin price using MindsDB"},{"location":"sql/tutorials/bitcoin-forecasting/#forecast-bitcoin-price-using-mindsdb","text":"Level: Easy Dataset: Coinbase 2017-2018 Bitcoin data Bitcoin is a digital currency that uses blockchain technology, Bitcoin can be sent from user to user on the peer-to-peer Bitcoin network without the need for intermediaries. Note that this is just a task for fun so use it at your own risk. In this tutorial, you will learn how to forecast Bitcoin using MindsDB. And all you need to know is just SQL. Behind the scenes, MindsDB will create the complete machine learning workflow, like determine, normalize & encode the data, train & test the model, etc. But we don\u2019t need to bother with all this complexity. Of course, if you want to, you can tune things manually inside MindsDB with a declarative syntax called JSON-AI, but we will not cover it in this article. DISCLAIMER: Please note that predicting Bitcoin price is just an example for showing MindsDB technology and you are solely responsible for any results you may get in real life, if you use this information for real trading purposes. Please note, that you can also follow this tutorial with other data you have.","title":"Forecast Bitcoin price using MindsDB"},{"location":"sql/tutorials/bitcoin-forecasting/#pre-requisites","text":"First, you need MindsDB installed. If you want to use MindsDB locally, you need to install MindsDB with Docker or Python . However, if you want to use MindsDB without installing it locally, you can use Cloud Mindsdb . In this tutorial, I'm using MindsDB Cloud, because it is easy to set up in just 2 minutes and it has a great free tier. Second, you need a MySQL client to connect to MindsDB MYSQL API.","title":"Pre-requisites"},{"location":"sql/tutorials/bitcoin-forecasting/#connect-your-database","text":"You must first connect MindsDB to the database where the record is stored. In the left navigation, click Database, click ADD DATABASE. And you must provide all the necessary parameters to connect to the database. Supported Database - select the database that you want to connect to Integrations Name - add a name to the integration, here I'm using 'mysql' but you can name it as you like Database - the database name Host - database hostname Port - database port Username - database user Password - user's password Then, click on CONNECT. The next step is to use the MySQL client to connect to MindsDB\u2019s MySQL API, train a new model, and make a prediction.","title":"Connect your database"},{"location":"sql/tutorials/bitcoin-forecasting/#connect-to-mindsdbs-mysql-api","text":"In this tutorial I'm using MySQL command-line client, but you can also follow up with the one that works the best for you, like Dbeaver. The first step is to use the MindsDB Cloud user to connect to the MindsDB MySQL API, using this command: You need to specify the hostname and user name explicitly, as well as a password for connecting. Click enter and you are connected to MindsDB API. If you have an authentication error, please make sure you are providing the email address you have used to create an account on MindsDB Cloud.","title":"Connect to MindsDB\u2019s MySQL API"},{"location":"sql/tutorials/bitcoin-forecasting/#data","text":"Now, let's show the databases. There are 4 databases, and the MySQL database is the database that I've connected to MindsDB. Let's check the MySQL database. There are 3 tables, and in this tutorial, we will use the Bitcoin table. And let's check what is inside this table. These tables have 5 columns: date, open price, the highest price of the day, lowest price of the day, and close price. The column we want to forecast is close price.","title":"Data"},{"location":"sql/tutorials/bitcoin-forecasting/#create-the-model","text":"Now, to create the model let's move to MindsDB database. and let's see what's inside. There are 2 tables, predictors, and commands. Predictors contain your predictors record, and commands contain your last commands used. To train a new machine learning model we will need to CREATE MODEL as a new record inside the predictors table, and using this command: CREATE MODEL mindsdb . predictor_name FROM integration_name ( SELECT column_name , column_name2 FROM table_name ) as ds_name PREDICT column_target as column_alias ORDER BY column_orderby WINDOW num_window HORIZON num_horizon USING { \"is_timeseries\" : \"Yes\" } The values that we need to provide are: predictor_name (string) - The name of the model. integration_name (string) - The name of the connection to your database. ds_name (string) - the name of the dataset you want to create, it's optional if you don't specify this value MindsDB will generate by itself. column_target (string) - The feature you want to predict. column_alias - Alias name of the feature you want to predict. column_orderby - The column to order the data, for time series this should be the date/time column. num_window - keyword specifies the number of rows to \"look back\" into when making a prediction after the rows are ordered by the order_by column and split into groups. This could be used to specify something like \"Always use the previous 10 rows\". num_horizon - keyword specifies the number of future predictions. So, use this command to create the models: If there's no error, that means your model is created and training. To see if your model is finished, use this command: SELECT * FROM mindsdb . models WHERE name = predictor_name ; And values that we need to provide are: predictor_name (string) - The name of the model. If the model is finished, it will look like this. The model has been created! and the accuracy is 99%!","title":"Create the model"},{"location":"sql/tutorials/bitcoin-forecasting/#create-the-prediction","text":"Now you are in the last step of this tutorial, creating the prediction. To create a prediction you can use this command: SELECT target_variable , target_variable_explain FROM model_table WHERE when_data = { \"column3\" : \"value\" , \"column2\" : \"value\" } ; And you need to set these values: - target_variable - The original value of the target variable. - target_variable_confidence - Model confidence score. - target_variable_explain - JSON object that contains additional information as confidence_lower_bound, confidence_upper_bound, anomaly, truth. - when_data - The data to make the predictions from(WHERE clause params). Finally, we have created a Bitcoin forecasting model using SQL and MindsDB. Yayyy!","title":"Create the prediction"},{"location":"sql/tutorials/bitcoin-forecasting/#conclusions","text":"As you can see it is very easy to start making predictions with machine learning even without being a data scientist! Feel free to check this yourself! MindsDB free cloud account is fast to set up and has more than enough to give it a try. Or use the open source version if you want to.","title":"Conclusions"},{"location":"sql/tutorials/bodyfat/","text":"Determining Body Fat Percentage \u00b6 Dataset: Body fat prediction Communtiy Author: Contip Machine Learning powered data analysis can be performed quickly and efficiently by MindsDB to enable individuals to make accurate predictions for certain metrics based on a variety of associated values. MindsDB enables you to make predictions automatically using just SQL commands, all the ML workflow is automated, and abstracted as virtual \u201cAI tables\u201d in your database so you may start getting insights from forecasts right away. In this tutorial, we'll be using MindsDB and a MySQL database to predict body fat percentage based on several body part measurement criteria. Pre-requisites \u00b6 Before you start make sure that you've: Visted Getting Started Guide Visited Getting Started with Cloud Downloaded the dataset. The dataset being used for this tutorial. Get it from Kaggle . Data Overview \u00b6 For this tutorial, we'll be using the Body Fat Prediction dataset available at Kaggle . Each row represents one person and we'll train an ML model to help us predict an individual's body fat percentage using MindsDB. Below is a short description of each feature of the data: Density: Individual's body density as determined by underwater weighing (float) BodyFat: The individual's determined body fat percentage (float). This is what we want to predict Age: Age of the individual (int) Weight: Weight of the individual in pounds (float) Height: Height of the individual in inches (float) Neck: Circumference of the individual's neck in cm (float) Chest: Circumference of the individual's chest in cm (float) Abdomen: Circumference of the individual's abdomen in cm (float) Hip: Circumference of the individual's hips in cm (float) Thigh: Circumference of the individual's thigh in cm (float) Knee: Circumference of the individual's knee in cm (float) Ankle: Circumference of the individual's ankle in cm (float) Biceps: Circumference of the individual's biceps in cm (float) Forearm: Circumference of the individual's forearm in cm (float) Wrist: Circumference of the individual's wrist in cm (float) Add data to MindsDB GUI \u00b6 MindsDB has a functionality to uploud your data file directly via the GUI where you can immediately query the data and create a machine learning model. The following is the steps to upload your data directly to MindsDB: Access the MindsDB GUI via cloud or local via the URL 127.0.0.1:47334/. Select the button Add data or select the plug icon on the left side bar. The page will navigate to 'Select your data source'. Select the option 'Files'. Select the tab under 'Import a file'. Please note the dataset files should not exceed the maximum size limit which is 10MB. Provide a name for the data file which will be saved as a table. Once you have successfully uploaded the file, you can query the data from the files table to ensure the information pulls through. Run the following syntax: SELECT * FROM files . bodyfat LIMIT 10 ; Once you have confirmed the file has successfully uploaded and the data can be retrieved, we can move on to creating a predictor. Create and train a machine learning model. \u00b6 With the CREATE MODEL statement, we can create a machine learning model: CREATE MODEL mindsdb . predictor_name FROM files ( SELECT column_name , column_name2 FROM file_name ) PREDICT column_name ; The required values that we need to provide are: \u200b - predictor_name (string): The name of the model - integration_name (string): The name of the connection to your database. - column_name (string): The feature you want to predict. For our case, we'll enter the following syntax: CREATE MODEL bodyfat_predictor FROM files ( SELECT * FROM bodyfat ) PREDICT Bodyfat ; Select the Run button or select Shift+Enter to run the syntax. Once is is successful you will receive a message in the console 'Query successfully completed'. You should see output similar to the following: At this point, the predictor will immediately begin training. Check the status of the training by entering the command: SELECT * FROM mindsdb . models WHERE name = 'bodyfat_predictor' ; When complete, you should see output similar to the following: As you can see, the predictor training has been completed with an accuracy of approximately 99%. At this point, you have successfully trained an ML model for our Body Fat Prediction dataset! Using SQL Commands to Make Predictions \u00b6 Now, we can query the model and make predictions based on our input data by using SQL statements. Let's imagine an individual aged 25, with a body density of 1.08, a weight of 170lb, a height of 70in, a neck circumference of 38.1cm, a chest circumference of 103.5cm, an abdomen circumference of 85.4cm, a hip circumference of 102.2cm, a thigh circumference of 63.0cm, a knee circumference of 39.4cm, an ankle circumference of 22.8cm, a biceps circumference of 33.3cm, a forearm circumference of 28.7cm, and a wrist circumference of 18.3cm. We can predict this person's body fat percentage by issuing the following command: SELECT BodyFat , BodyFat_confidence , BodyFat_explain FROM mindsdb . bodyfat_predictor WHERE Density = 1 . 08 AND Age = 25 AND Weight = 170 AND Height = 70 AND Neck = 38 . 1 AND Chest = 103 . 5 AND Abdomen = 85 . 4 AND Hip = 102 . 2 AND Thigh = 63 . 0 AND Knee = 39 . 4 AND Ankle = 22 . 8 AND Biceps = 33 . 3 AND Forearm = 28 . 7 AND Wrist = 18 . 3 ; This should return output similar to: As you can see, with around 99% confidence, MindsDB predicted the body fat percentage for this individual at 8.97%. You can at this point feel free to alter the prospective individual's bodypart measurement parameters and make additional prediction queries if you'd like. Making Batch Predictions using the JOIN syntax \u00b6 The above example showed how to make predictions for a single individual's bodyfat, but what if you had a table of bodypart measurements for a number of individuals, and wanted to make predictions for them all? This is possible using the JOIN command , which allows for the combining of rows from a database table and the prediction model table on a related column. The basic syntax to use the JOIN syntax is: SELECT t . column_name1 , t . column_name2 FROM integration_name . table AS t JOIN mindsdb . predictor_name AS p WHERE t . column_name IN ( value1 , value2 , ...); For our purposes, we'll re-use the original data set, taking the Age, Density, Weight, Height, Neck circumference, Chest circumference, Abdomen circumference, and Hip circumference fields. We'll also include the original BodyFat percentage to compare our predicted values against the originals. Execute the following command: SELECT t . Age , t . Density , t . Weight , t . Height , t . Neck , t . Chest , t . Abdomen , t . Hip , t . BodyFat , p . BodyFat AS predicted_BodyFat FROM bodyfat_integration . bodyfat AS t JOIN mindsdb . bodyfat_predictor AS p LIMIT 5 ; This should return an output table similar to the following: As you can see, a prediction has been generated for each row in the input table. Additionally, our predicted bodyfat percentages align closely with the original values! Note that even though we chose only to display the Age, Density, Weight, Height, Neck, Chest, Abdomen, and Hip measurements in this example, the predicted_BodyFat field was determined by taking into consideration all of the data fields in the original bodyfat table (as this table was JOINed with the bodyfat_predictor table, from which we selected the specified fields). In order to make predictions based ONLY on the specified fields, we would have to create a new table containing only those fields, and JOIN that with the bodyfat_predictor table! You are now done with the tutorial! \ud83c\udf89 Please feel free to try it yourself. Sign up for a free MindsDB account to get up and running in 5 minutes, and if you need any help, feel free to ask in Slack or Github . For more tutorials like this check out MindsDB documentation .","title":"Body Fat Prediction"},{"location":"sql/tutorials/bodyfat/#determining-body-fat-percentage","text":"Dataset: Body fat prediction Communtiy Author: Contip Machine Learning powered data analysis can be performed quickly and efficiently by MindsDB to enable individuals to make accurate predictions for certain metrics based on a variety of associated values. MindsDB enables you to make predictions automatically using just SQL commands, all the ML workflow is automated, and abstracted as virtual \u201cAI tables\u201d in your database so you may start getting insights from forecasts right away. In this tutorial, we'll be using MindsDB and a MySQL database to predict body fat percentage based on several body part measurement criteria.","title":"Determining Body Fat Percentage"},{"location":"sql/tutorials/bodyfat/#pre-requisites","text":"Before you start make sure that you've: Visted Getting Started Guide Visited Getting Started with Cloud Downloaded the dataset. The dataset being used for this tutorial. Get it from Kaggle .","title":"Pre-requisites"},{"location":"sql/tutorials/bodyfat/#data-overview","text":"For this tutorial, we'll be using the Body Fat Prediction dataset available at Kaggle . Each row represents one person and we'll train an ML model to help us predict an individual's body fat percentage using MindsDB. Below is a short description of each feature of the data: Density: Individual's body density as determined by underwater weighing (float) BodyFat: The individual's determined body fat percentage (float). This is what we want to predict Age: Age of the individual (int) Weight: Weight of the individual in pounds (float) Height: Height of the individual in inches (float) Neck: Circumference of the individual's neck in cm (float) Chest: Circumference of the individual's chest in cm (float) Abdomen: Circumference of the individual's abdomen in cm (float) Hip: Circumference of the individual's hips in cm (float) Thigh: Circumference of the individual's thigh in cm (float) Knee: Circumference of the individual's knee in cm (float) Ankle: Circumference of the individual's ankle in cm (float) Biceps: Circumference of the individual's biceps in cm (float) Forearm: Circumference of the individual's forearm in cm (float) Wrist: Circumference of the individual's wrist in cm (float)","title":"Data Overview"},{"location":"sql/tutorials/bodyfat/#add-data-to-mindsdb-gui","text":"MindsDB has a functionality to uploud your data file directly via the GUI where you can immediately query the data and create a machine learning model. The following is the steps to upload your data directly to MindsDB: Access the MindsDB GUI via cloud or local via the URL 127.0.0.1:47334/. Select the button Add data or select the plug icon on the left side bar. The page will navigate to 'Select your data source'. Select the option 'Files'. Select the tab under 'Import a file'. Please note the dataset files should not exceed the maximum size limit which is 10MB. Provide a name for the data file which will be saved as a table. Once you have successfully uploaded the file, you can query the data from the files table to ensure the information pulls through. Run the following syntax: SELECT * FROM files . bodyfat LIMIT 10 ; Once you have confirmed the file has successfully uploaded and the data can be retrieved, we can move on to creating a predictor.","title":"Add data to MindsDB GUI"},{"location":"sql/tutorials/bodyfat/#create-and-train-a-machine-learning-model","text":"With the CREATE MODEL statement, we can create a machine learning model: CREATE MODEL mindsdb . predictor_name FROM files ( SELECT column_name , column_name2 FROM file_name ) PREDICT column_name ; The required values that we need to provide are: \u200b - predictor_name (string): The name of the model - integration_name (string): The name of the connection to your database. - column_name (string): The feature you want to predict. For our case, we'll enter the following syntax: CREATE MODEL bodyfat_predictor FROM files ( SELECT * FROM bodyfat ) PREDICT Bodyfat ; Select the Run button or select Shift+Enter to run the syntax. Once is is successful you will receive a message in the console 'Query successfully completed'. You should see output similar to the following: At this point, the predictor will immediately begin training. Check the status of the training by entering the command: SELECT * FROM mindsdb . models WHERE name = 'bodyfat_predictor' ; When complete, you should see output similar to the following: As you can see, the predictor training has been completed with an accuracy of approximately 99%. At this point, you have successfully trained an ML model for our Body Fat Prediction dataset!","title":"Create and train a machine learning model."},{"location":"sql/tutorials/bodyfat/#using-sql-commands-to-make-predictions","text":"Now, we can query the model and make predictions based on our input data by using SQL statements. Let's imagine an individual aged 25, with a body density of 1.08, a weight of 170lb, a height of 70in, a neck circumference of 38.1cm, a chest circumference of 103.5cm, an abdomen circumference of 85.4cm, a hip circumference of 102.2cm, a thigh circumference of 63.0cm, a knee circumference of 39.4cm, an ankle circumference of 22.8cm, a biceps circumference of 33.3cm, a forearm circumference of 28.7cm, and a wrist circumference of 18.3cm. We can predict this person's body fat percentage by issuing the following command: SELECT BodyFat , BodyFat_confidence , BodyFat_explain FROM mindsdb . bodyfat_predictor WHERE Density = 1 . 08 AND Age = 25 AND Weight = 170 AND Height = 70 AND Neck = 38 . 1 AND Chest = 103 . 5 AND Abdomen = 85 . 4 AND Hip = 102 . 2 AND Thigh = 63 . 0 AND Knee = 39 . 4 AND Ankle = 22 . 8 AND Biceps = 33 . 3 AND Forearm = 28 . 7 AND Wrist = 18 . 3 ; This should return output similar to: As you can see, with around 99% confidence, MindsDB predicted the body fat percentage for this individual at 8.97%. You can at this point feel free to alter the prospective individual's bodypart measurement parameters and make additional prediction queries if you'd like.","title":"Using SQL Commands to Make Predictions"},{"location":"sql/tutorials/bodyfat/#making-batch-predictions-using-the-join-syntax","text":"The above example showed how to make predictions for a single individual's bodyfat, but what if you had a table of bodypart measurements for a number of individuals, and wanted to make predictions for them all? This is possible using the JOIN command , which allows for the combining of rows from a database table and the prediction model table on a related column. The basic syntax to use the JOIN syntax is: SELECT t . column_name1 , t . column_name2 FROM integration_name . table AS t JOIN mindsdb . predictor_name AS p WHERE t . column_name IN ( value1 , value2 , ...); For our purposes, we'll re-use the original data set, taking the Age, Density, Weight, Height, Neck circumference, Chest circumference, Abdomen circumference, and Hip circumference fields. We'll also include the original BodyFat percentage to compare our predicted values against the originals. Execute the following command: SELECT t . Age , t . Density , t . Weight , t . Height , t . Neck , t . Chest , t . Abdomen , t . Hip , t . BodyFat , p . BodyFat AS predicted_BodyFat FROM bodyfat_integration . bodyfat AS t JOIN mindsdb . bodyfat_predictor AS p LIMIT 5 ; This should return an output table similar to the following: As you can see, a prediction has been generated for each row in the input table. Additionally, our predicted bodyfat percentages align closely with the original values! Note that even though we chose only to display the Age, Density, Weight, Height, Neck, Chest, Abdomen, and Hip measurements in this example, the predicted_BodyFat field was determined by taking into consideration all of the data fields in the original bodyfat table (as this table was JOINed with the bodyfat_predictor table, from which we selected the specified fields). In order to make predictions based ONLY on the specified fields, we would have to create a new table containing only those fields, and JOIN that with the bodyfat_predictor table! You are now done with the tutorial! \ud83c\udf89 Please feel free to try it yourself. Sign up for a free MindsDB account to get up and running in 5 minutes, and if you need any help, feel free to ask in Slack or Github . For more tutorials like this check out MindsDB documentation .","title":"Making Batch Predictions using the JOIN syntax"},{"location":"sql/tutorials/byom/","text":"Bring Your Own Model \u00b6 Introduction \u00b6 MindsDB allows you to integrate your own machine learning models into it. In order to do this your model will require some sort of API wrapper, for now we have two API specifications we support: MLflow and Ray Serve . The former supports importing already trained models and predicting with them from mindsdb. The later supports both training and predicting with external models. In order to use custom models there are three mandatory arguments one must past inside the USING statement: - url.predict , this is the url to call for getting predictions from your model - format , this can be either mlflow or ray_serve - dtype_dict , this is a JSON specifying all columns expected by their models, and their respective data types. For now, the mapping supports data types used by lightwood , our AutoML library. There's an additional optional argument if you want to train the model via MindsDB (only for Ray Serve): - url.train , which is the endpoint that will be called to train your model 1. Ray Serve \u00b6 1.1 Simple example - Logistic regression \u00b6 Ray serve is a simple high-throughput service that can wrap over your own ml models. In this example, we will train and predict with an external scikit-learn model. First, let's look at the actual model wrapped inside a class that complies with the above requirements: import ray from fastapi import Request , FastAPI from ray import serve import time import pandas as pd import json from sklearn.linear_model import LogisticRegression app = FastAPI () ray . init () serve . start ( detached = True ) async def parse_req ( request : Request ): data = await request . json () target = data . get ( 'target' , None ) di = json . loads ( data [ 'df' ]) df = pd . DataFrame ( di ) return df , target @serve . deployment ( route_prefix = \"/my_model\" ) @serve . ingress ( app ) class MyModel : @app . post ( \"/train\" ) async def train ( self , request : Request ): df , target = await parse_req ( request ) feature_cols = list ( set ( list ( df . columns )) - set ([ target ])) self . feature_cols = feature_cols X = df . loc [:, self . feature_cols ] Y = list ( df [ target ]) self . model = LogisticRegression () self . model . fit ( X , Y ) return { 'status' : 'ok' } @app . post ( \"/predict\" ) async def predict ( self , request : Request ): df , _ = await parse_req ( request ) X = df . loc [:, self . feature_cols ] predictions = self . model . predict ( X ) pred_dict = { 'prediction' : [ float ( x ) for x in predictions ]} return pred_dict MyModel . deploy () while True : time . sleep ( 1 ) The important bits here are having train and predict endpoints. The train endpoint accept two parameters in the JSON sent via POST: - df -- a serialized dictionary that can be converted into a pandas dataframe - target -- the name of the target column The predict endpoint needs only one parameter: - df -- a serialized dictionary that can be converted into a pandas dataframe The training endpoints must return a JSON that contains the keys status set to ok . The predict endpoint must return a dictionary containing the prediction key, storing the predictions. Additional keys can be returned for confidence and confidence intervals. Once you start this RayServe-wrapped model you can train it using a query like this one: CREATE MODEL mindsdb . byom_ray_serve FROM mydb ( SELECT number_of_rooms , initial_price , rental_price FROM test_data . home_rentals ) PREDICT number_of_rooms USING url . train = 'http://127.0.0.1:8000/my_model/train' , url . predict = 'http://127.0.0.1:8000/my_model/predict' , dtype_dict = { \"number_of_rooms\" : \"categorical\" , \"initial_price\" : \"integer\" , \"rental_price\" : \"integer\" } , format = 'ray_server' ; And you can query predictions as usual, either by conditioning on a subset of input colums: SELECT * FROM byom_ray_serve WHERE initial_price = 3000 AND rental_price = 3000 ; Or by JOINING to do batch predicions: SELECT tb . number_of_rooms , t . rental_price FROM mydb . test_data . home_rentals AS t JOIN mindsdb . byom_ray_serve AS tb WHERE t . rental_price > 5300 ; Please note that, if your model is behind a reverse proxy (e.g. nginx) you might have to increase the maximum limit for POST requests in order to receive the training data. MindsDB itself can send as much as you'd like and has been stress-tested with over a billion rows. 1.2. Example - Keras NLP model \u00b6 For this example, we will consider a natural language processing (NLP) task where we want to train a neural network with Keras to detect if a tweet is related to a natural disaster (fires, earthquakes, etc.). Please download this dataset to follow the example. The code for the model here is a bit more complex than in section 1.1, but the same rules apply: we create a Ray Server based service that wraps around a Kaggle NLP Model which can be trained and then used for predictions: import re import time import json import string import requests from collections import Counter , defaultdict \u200b import ray from ray import serve \u200b import gensim import numpy as np import pandas as pd from tqdm import tqdm from nltk.util import ngrams from nltk.corpus import stopwords from nltk.tokenize import word_tokenize from fastapi import Request , FastAPI from sklearn.model_selection import train_test_split from sklearn.feature_extraction.text import CountVectorizer \u200b from tensorflow.keras.preprocessing.text import Tokenizer from tensorflow.keras.preprocessing.sequence import pad_sequences from tensorflow.keras.models import Sequential from tensorflow.keras.layers import Embedding , LSTM , Dense , SpatialDropout1D from tensorflow.keras.initializers import Constant from tensorflow.keras.optimizers import Adam \u200b app = FastAPI () stop = set ( stopwords . words ( 'english' )) \u200b \u200b async def parse_req ( request : Request ): data = await request . json () target = data . get ( 'target' , None ) di = json . loads ( data [ 'df' ]) df = pd . DataFrame ( di ) return df , target \u200b \u200b @serve . deployment ( route_prefix = \"/nlp_kaggle_model\" ) @serve . ingress ( app ) class Model : MAX_LEN = 100 GLOVE_DIM = 50 EPOCHS = 10 \u200b def __init__ ( self ): self . model = None \u200b @app . post ( \"/train\" ) async def train ( self , request : Request ): df , target = await parse_req ( request ) \u200b target_arr = df . pop ( target ) . values df = self . preprocess_df ( df ) train_corpus = self . create_corpus ( df ) \u200b self . embedding_dict = {} with open ( './glove.6B.50d.txt' , 'r' ) as f : for line in f : values = line . split () word = values [ 0 ] vectors = np . asarray ( values [ 1 :], 'float32' ) self . embedding_dict [ word ] = vectors f . close () \u200b self . tokenizer_obj = Tokenizer () self . tokenizer_obj . fit_on_texts ( train_corpus ) \u200b sequences = self . tokenizer_obj . texts_to_sequences ( train_corpus ) tweet_pad = pad_sequences ( sequences , maxlen = self . __class__ . MAX_LEN , truncating = 'post' , padding = 'post' ) df = tweet_pad [: df . shape [ 0 ]] \u200b word_index = self . tokenizer_obj . word_index num_words = len ( word_index ) + 1 embedding_matrix = np . zeros (( num_words , self . __class__ . GLOVE_DIM )) \u200b for word , i in tqdm ( word_index . items ()): if i > num_words : continue \u200b emb_vec = self . embedding_dict . get ( word ) if emb_vec is not None : embedding_matrix [ i ] = emb_vec \u200b self . model = Sequential () embedding = Embedding ( num_words , self . __class__ . GLOVE_DIM , embeddings_initializer = Constant ( embedding_matrix ), input_length = self . __class__ . MAX_LEN , trainable = False ) self . model . add ( embedding ) self . model . add ( SpatialDropout1D ( 0.2 )) self . model . add ( LSTM ( 64 , dropout = 0.2 , recurrent_dropout = 0.2 )) self . model . add ( Dense ( 1 , activation = 'sigmoid' )) \u200b optimzer = Adam ( learning_rate = 1e-5 ) self . model . compile ( loss = 'binary_crossentropy' , optimizer = optimzer , metrics = [ 'accuracy' ]) \u200b X_train , X_test , y_train , y_test = train_test_split ( df , target_arr , test_size = 0.15 ) self . model . fit ( X_train , y_train , batch_size = 4 , epochs = self . __class__ . EPOCHS , validation_data = ( X_test , y_test ), verbose = 2 ) \u200b return { 'status' : 'ok' } \u200b @app . post ( \"/predict\" ) async def predict ( self , request : Request ): df , _ = await parse_req ( request ) \u200b df = self . preprocess_df ( df ) test_corpus = self . create_corpus ( df ) \u200b sequences = self . tokenizer_obj . texts_to_sequences ( test_corpus ) tweet_pad = pad_sequences ( sequences , maxlen = self . __class__ . MAX_LEN , truncating = 'post' , padding = 'post' ) df = tweet_pad [: df . shape [ 0 ]] \u200b y_pre = self . model . predict ( df ) y_pre = np . round ( y_pre ) . astype ( int ) . flatten () . tolist () sub = pd . DataFrame ({ 'target' : y_pre }) \u200b pred_dict = { 'prediction' : [ float ( x ) for x in sub [ 'target' ] . values ]} return pred_dict \u200b def preprocess_df ( self , df ): df = df [[ 'text' ]] df [ 'text' ] = df [ 'text' ] . apply ( lambda x : self . remove_URL ( x )) df [ 'text' ] = df [ 'text' ] . apply ( lambda x : self . remove_html ( x )) df [ 'text' ] = df [ 'text' ] . apply ( lambda x : self . remove_emoji ( x )) df [ 'text' ] = df [ 'text' ] . apply ( lambda x : self . remove_punct ( x )) return df \u200b def remove_URL ( self , text ): url = re . compile ( r 'https?://\\S+|www\\.\\S+' ) return url . sub ( r '' , text ) \u200b def remove_html ( self , text ): html = re . compile ( r '<.*?>' ) return html . sub ( r '' , text ) \u200b def remove_punct ( self , text ): table = str . maketrans ( '' , '' , string . punctuation ) return text . translate ( table ) \u200b def remove_emoji ( self , text ): emoji_pattern = re . compile ( \"[\" u \" \\U0001F600 - \\U0001F64F \" # emoticons u \" \\U0001F300 - \\U0001F5FF \" # symbols & pictographs u \" \\U0001F680 - \\U0001F6FF \" # transport & map symbols u \" \\U0001F1E0 - \\U0001F1FF \" # flags (iOS) u \" \\U00002702 - \\U000027B0 \" u \" \\U000024C2 - \\U0001F251 \" \"]+\" , flags = re . UNICODE ) return emoji_pattern . sub ( r '' , text ) \u200b def create_corpus ( self , df ): corpus = [] for tweet in tqdm ( df [ 'text' ]): words = [ word . lower () for word in word_tokenize ( tweet ) if (( word . isalpha () == 1 ) & ( word not in stop ))] corpus . append ( words ) return corpus \u200b \u200b if __name__ == '__main__' : \u200b ray . init () serve . start ( detached = True ) \u200b Model . deploy () \u200b while True : time . sleep ( 1 ) We need access to the training data, so we'll create a table called nlp_kaggle_train to load the dataset that the original model uses. And ingest it into a table with the following schema: id INT , keyword VARCHAR ( 255 ), location VARCHAR ( 255 ), text VARCHAR ( 5000 ), target INT Note: specifics of the schema and how to ingest the csv will vary depending on your database. Next, we can register and train the above custom model using the following query: CREATE MODEL mindsdb . byom_ray_serve_nlp FROM maria ( SELECT text , target FROM test . nlp_kaggle_train ) PREDICT target USING url . train = 'http://127.0.0.1:8000/nlp_kaggle_model/train' , url . predict = 'http://127.0.0.1:8000/nlp_kaggle_model/predict' , dtype_dict = { \"text\" : \"rich_text\" , \"target\" : \"integer\" } , format = 'ray_server' ; Training will take a while given that this model is a neural network rather than a simple logistic regression. You can check its status with the query SELECT * FROM mindsdb.models WHERE name = 'byom_ray_serve_nlp'; , much like you'd do with a \"normal\" MindsDB predictor. Once the predictor's status becomes trained we can query it for predictions as usual: SELECT * FROM mindsdb . byom_ray_serve_nlp WHERE text = 'The tsunami is coming, seek high ground' ; Which would, hopefully, output 1 . Alternatively, we can try out this tweet to expect 0 as an output: SELECT * FROM mindsdb . byom_ray_serve_nlp WHERE text = 'This is lovely dear friend' ; If your results do not match this example, it could help to train the model for a longer amount of epochs. 2. MLFlow \u00b6 2.1 Simple example - Logistic Regression \u00b6 MLFlow is a tool that you can use to train and serve models, among other features like organizing experiments, tracking metrics, etc. Given there is no way to train an MLflow-wrapped model using its API, you will have to train your models outside of MindsDB by pulling your data manually (i.e. with a script), ideally using a MLflow run or experiment. The first step would be to create a script where you train a model and save it using one of the saving methods that MLflow exposes. For this example, we will use the model in this simple tutorial where the method is mlflow.sklearn.log_model ( here ), given that the model is built with scikit-learn. Once trained, you need to make sure the model is served and listening for input in a URL of your choice (note, this can mean your model can run on a different machine than the one executing MindsDB). Let's assume this URL to be http://localhost:5000/invocations for now. This means you would execute the following command in your terminal, from the directory where the model was stored: mlflow models serve --model-uri runs:/<run-id>/model With <run-id> given in the output of the command python train.py used for actually training the model. Next, we're going to bring this model into MindsDB: CREATE MODEL mindsdb . byom_mlflow PREDICT ` 1 ` -- `1` is the target column name USING url . predict = 'http://localhost:5000/invocations' , format = 'mlflow' , data_dtype = { \"0\" : \"integer\" , \"1\" : \"integer\" } We can now run predictions as usual, by using the WHERE statement or joining on a data table with an appropriate schema: SELECT ` 1 ` FROM byom_mlflow WHERE ` 0 `= 2 ; 2.2. Advanced example - Keras NLP model \u00b6 Same use case as in section 1.2, be sure to download the dataset to reproduce the steps here. In this case, we will take a look at the best practices when your model needs custom data preprocessing code (which, realistically, will be fairly common). The key difference is that we now need to use the mlflow.pyfunc module to both 1) save the model using mlflow.pyfunc.save_model and 2) subclass mlflow.pyfunc.PythonModel to wrap the model in an MLflow-compatible way that will enable our custom inference logic to be called. Saving the model \u00b6 In the same script where you train the model (which you can find in the final section of 2.2) there should be a call at the end where you actually use mlflow to save every produced artifact: mlflow . pyfunc . save_model ( path = \"nlp_kaggle\" , python_model = Model (), conda_env = conda_env , artifacts = artifacts ) Here, artifacts will be a dictionary with all expected produced outputs when running the training phase. In this case, we want both a model and a tokenizer to preprocess the input text. On the other hand, conda_env specifies the environment under which your model should be executed once served in a self-contained conda environment, so it should include all required packages and dependencies. For this example, they look like this: # these will be accessible inside the Model() wrapper artifacts = { 'model' : model_path , 'tokenizer_path' : tokenizer_path , } # specs for environment that will be created when serving the model conda_env = { 'name' : 'nlp_keras_env' , 'channels' : [ 'defaults' ], 'dependencies' : [ 'python=3.8' , 'pip' , { 'pip' : [ 'mlflow' , 'tensorflow' , 'cloudpickle' , 'nltk' , 'pandas' , 'numpy' , 'scikit-learn' , 'tqdm' , ], }, ], } Finally, to actually store the model you need to provide the wrapper class that will 1) load all produced artifacts into an accessible \"context\" and 2) implement all required inference logic: class Model ( mlflow . pyfunc . PythonModel ): def load_context ( self , context ): # we use paths in the context to load everything self . model_path = context . artifacts [ 'model' ] self . model = load_model ( self . model_path ) with open ( context . artifacts [ 'tokenizer_path' ], 'rb' ) as f : self . tokenizer = pickle . load ( f ) def predict ( self , context , model_input ): # preprocess input, tokenize, pad, and call the model df = preprocess_df ( model_input ) corpus = create_corpus ( df ) sequences = self . tokenizer . texts_to_sequences ( corpus ) tweet_pad = pad_sequences ( sequences , maxlen = MAX_LEN , truncating = 'post' , padding = 'post' ) df = tweet_pad [: df . shape [ 0 ]] y_pre = self . model . predict ( df ) y_pre = np . round ( y_pre ) . astype ( int ) . flatten () . tolist () return list ( y_pre ) As you can see, here we are loading multiple artifacts and using them to guarantee the input data will be in the same format that was used when training. Ideally, you would abstract this even further into a single preprocess method that is called both at training time and inference time. Finally, serving is simple. Go to the directory where you called the above script, and execute mlflow models serve --model-uri ./nlp_kaggle . At this point, the rest is essentially the same as in the previous example. You can link the MLflow model with these SQL statements: CREATE MODEL mindsdb . byom_mlflow_nlp PREDICT ` target ` USING url . predict = 'http://localhost:5000/invocations' , format = 'mlflow' , dtype_dict = { \"text\" : \"rich text\" , \"target\" : \"binary\" } ; To get predictions, you can directly pass input data using the WHERE clause: SELECT target FROM mindsdb . byom_mlflow_nlp WHERE text = 'The tsunami is coming, seek high ground' ; Or you can JOIN with a data table. For this, you should ensure the table actually exists and that the database it belongs to has been connected to your MindsDB instance. For more details, refer to the same steps in the Ray Serve example (section 1.2). SELECT ta . text , tb . target as predicted FROM db_byom . test . nlp_kaggle_test as ta JOIN mindsdb . byom_mlflow_nlp as tb ; Full Script \u00b6 Finally, for reference, here's the full script that trains and saves the model. The model is exactly the same as in section 1.2, so it may seem familiar. import re import pickle import string import mlflow.pyfunc import nltk import tqdm import sklearn import tensorflow import cloudpickle import numpy as np import pandas as pd from nltk.corpus import stopwords from nltk.tokenize import word_tokenize from sklearn.model_selection import train_test_split from tensorflow.keras.initializers import Constant from tensorflow.keras.layers import Embedding , LSTM , Dense , SpatialDropout1D from tensorflow.keras.models import Sequential from tensorflow.keras.optimizers import Adam from tensorflow.keras.preprocessing.sequence import pad_sequences from tensorflow.keras.preprocessing.text import Tokenizer from tensorflow.keras.models import load_model stop = set ( stopwords . words ( 'english' )) MAX_LEN = 100 GLOVE_DIM = 50 EPOCHS = 10 def preprocess_df ( df ): df = df [[ 'text' ]] funcs = [ remove_URL , remove_html , remove_emoji , remove_punct ] for fn in funcs : df [ 'text' ] = df [ 'text' ] . apply ( lambda x : fn ( x )) return df def remove_URL ( text ): url = re . compile ( r 'https?://\\S+|www\\.\\S+' ) return url . sub ( r '' , text ) def remove_html ( text ): html = re . compile ( r '<.*?>' ) return html . sub ( r '' , text ) def remove_punct ( text ): table = str . maketrans ( '' , '' , string . punctuation ) return text . translate ( table ) def remove_emoji ( text ): emoji_pattern = re . compile ( \"[\" u \" \\U0001F600 - \\U0001F64F \" # emoticons u \" \\U0001F300 - \\U0001F5FF \" # symbols & pictographs u \" \\U0001F680 - \\U0001F6FF \" # transport & map symbols u \" \\U0001F1E0 - \\U0001F1FF \" # flags (iOS) u \" \\U00002702 - \\U000027B0 \" u \" \\U000024C2 - \\U0001F251 \" \"]+\" , flags = re . UNICODE ) return emoji_pattern . sub ( r '' , text ) def create_corpus ( df ): corpus = [] for tweet in tqdm . tqdm ( df [ 'text' ]): words = [ word . lower () for word in word_tokenize ( tweet ) if (( word . isalpha () == 1 ) & ( word not in stop ))] corpus . append ( words ) return corpus class Model ( mlflow . pyfunc . PythonModel ): def load_context ( self , context ): self . model_path = context . artifacts [ 'model' ] with open ( context . artifacts [ 'tokenizer_path' ], 'rb' ) as f : self . tokenizer = pickle . load ( f ) self . model = load_model ( self . model_path ) def predict ( self , context , model_input ): df = preprocess_df ( model_input ) corpus = create_corpus ( df ) sequences = self . tokenizer . texts_to_sequences ( corpus ) tweet_pad = pad_sequences ( sequences , maxlen = MAX_LEN , truncating = 'post' , padding = 'post' ) df = tweet_pad [: df . shape [ 0 ]] y_pre = self . model . predict ( df ) y_pre = np . round ( y_pre ) . astype ( int ) . flatten () . tolist () return list ( y_pre ) if __name__ == '__main__' : train_model = True model_path = './' tokenizer_path = './tokenizer.pkl' run_name = 'test_run' mlflow_pyfunc_model_path = \"nlp_kaggle\" mlflow . set_tracking_uri ( \"sqlite:///mlflow.db\" ) if train_model : # preprocess data df = pd . read_csv ( './train.csv' ) target = df [[ 'target' ]] target_arr = target . values df = preprocess_df ( df ) train_corpus = create_corpus ( df ) # load embeddings embedding_dict = {} with open ( './glove.6B.50d.txt' , 'r' ) as f : for line in f : values = line . split () word = values [ 0 ] vectors = np . asarray ( values [ 1 :], 'float32' ) embedding_dict [ word ] = vectors f . close () # generate and save tokenizer tokenizer_obj = Tokenizer () tokenizer_obj . fit_on_texts ( train_corpus ) with open ( tokenizer_path , 'wb' ) as f : pickle . dump ( tokenizer_obj , f ) # tokenize and pad sequences = tokenizer_obj . texts_to_sequences ( train_corpus ) tweet_pad = pad_sequences ( sequences , maxlen = MAX_LEN , truncating = 'post' , padding = 'post' ) df = tweet_pad [: df . shape [ 0 ]] word_index = tokenizer_obj . word_index num_words = len ( word_index ) + 1 embedding_matrix = np . zeros (( num_words , GLOVE_DIM )) # fill embedding matrix for word , i in tqdm . tqdm ( word_index . items ()): if i > num_words : continue emb_vec = embedding_dict . get ( word ) if emb_vec is not None : embedding_matrix [ i ] = emb_vec X_train , X_test , y_train , y_test = train_test_split ( df , target_arr , test_size = 0.15 ) # generate model model = Sequential () embedding = Embedding ( num_words , GLOVE_DIM , embeddings_initializer = Constant ( embedding_matrix ), input_length = MAX_LEN , trainable = False ) model . add ( embedding ) model . add ( SpatialDropout1D ( 0.2 )) model . add ( LSTM ( 64 , dropout = 0.2 , recurrent_dropout = 0.2 )) model . add ( Dense ( 1 , activation = 'sigmoid' )) optimzer = Adam ( learning_rate = 1e-5 ) model . compile ( loss = 'binary_crossentropy' , optimizer = optimzer , metrics = [ 'accuracy' ]) # train and save model . fit ( X_train , y_train , batch_size = 4 , epochs = EPOCHS , validation_data = ( X_test , y_test ), verbose = 2 ) model . save ( model_path ) # save in mlflow format artifacts = { 'model' : model_path , 'tokenizer_path' : tokenizer_path , } conda_env = { 'channels' : [ 'defaults' ], 'dependencies' : [ 'python=3.8' , 'pip' , { 'pip' : [ 'mlflow' , 'tensorflow== {} ' . format ( tensorflow . __version__ ), 'cloudpickle== {} ' . format ( cloudpickle . __version__ ), 'nltk== {} ' . format ( nltk . __version__ ), 'pandas== {} ' . format ( pd . __version__ ), 'numpy== {} ' . format ( np . __version__ ), 'scikit-learn== {} ' . format ( sklearn . __version__ ), 'tqdm== {} ' . format ( tqdm . __version__ ) ], }, ], 'name' : 'nlp_keras_env' } # Save and register the MLflow Model with mlflow . start_run ( run_name = run_name ) as run : mlflow . pyfunc . save_model ( path = mlflow_pyfunc_model_path , python_model = Model (), conda_env = conda_env , artifacts = artifacts ) result = mlflow . register_model ( f \"runs:/ { run . info . run_id } / { mlflow_pyfunc_model_path } \" , f \" { mlflow_pyfunc_model_path } \" )","title":"Bring Your Own Model"},{"location":"sql/tutorials/byom/#bring-your-own-model","text":"","title":"Bring Your Own Model"},{"location":"sql/tutorials/byom/#introduction","text":"MindsDB allows you to integrate your own machine learning models into it. In order to do this your model will require some sort of API wrapper, for now we have two API specifications we support: MLflow and Ray Serve . The former supports importing already trained models and predicting with them from mindsdb. The later supports both training and predicting with external models. In order to use custom models there are three mandatory arguments one must past inside the USING statement: - url.predict , this is the url to call for getting predictions from your model - format , this can be either mlflow or ray_serve - dtype_dict , this is a JSON specifying all columns expected by their models, and their respective data types. For now, the mapping supports data types used by lightwood , our AutoML library. There's an additional optional argument if you want to train the model via MindsDB (only for Ray Serve): - url.train , which is the endpoint that will be called to train your model","title":"Introduction"},{"location":"sql/tutorials/byom/#1-ray-serve","text":"","title":"1. Ray Serve"},{"location":"sql/tutorials/byom/#11-simple-example-logistic-regression","text":"Ray serve is a simple high-throughput service that can wrap over your own ml models. In this example, we will train and predict with an external scikit-learn model. First, let's look at the actual model wrapped inside a class that complies with the above requirements: import ray from fastapi import Request , FastAPI from ray import serve import time import pandas as pd import json from sklearn.linear_model import LogisticRegression app = FastAPI () ray . init () serve . start ( detached = True ) async def parse_req ( request : Request ): data = await request . json () target = data . get ( 'target' , None ) di = json . loads ( data [ 'df' ]) df = pd . DataFrame ( di ) return df , target @serve . deployment ( route_prefix = \"/my_model\" ) @serve . ingress ( app ) class MyModel : @app . post ( \"/train\" ) async def train ( self , request : Request ): df , target = await parse_req ( request ) feature_cols = list ( set ( list ( df . columns )) - set ([ target ])) self . feature_cols = feature_cols X = df . loc [:, self . feature_cols ] Y = list ( df [ target ]) self . model = LogisticRegression () self . model . fit ( X , Y ) return { 'status' : 'ok' } @app . post ( \"/predict\" ) async def predict ( self , request : Request ): df , _ = await parse_req ( request ) X = df . loc [:, self . feature_cols ] predictions = self . model . predict ( X ) pred_dict = { 'prediction' : [ float ( x ) for x in predictions ]} return pred_dict MyModel . deploy () while True : time . sleep ( 1 ) The important bits here are having train and predict endpoints. The train endpoint accept two parameters in the JSON sent via POST: - df -- a serialized dictionary that can be converted into a pandas dataframe - target -- the name of the target column The predict endpoint needs only one parameter: - df -- a serialized dictionary that can be converted into a pandas dataframe The training endpoints must return a JSON that contains the keys status set to ok . The predict endpoint must return a dictionary containing the prediction key, storing the predictions. Additional keys can be returned for confidence and confidence intervals. Once you start this RayServe-wrapped model you can train it using a query like this one: CREATE MODEL mindsdb . byom_ray_serve FROM mydb ( SELECT number_of_rooms , initial_price , rental_price FROM test_data . home_rentals ) PREDICT number_of_rooms USING url . train = 'http://127.0.0.1:8000/my_model/train' , url . predict = 'http://127.0.0.1:8000/my_model/predict' , dtype_dict = { \"number_of_rooms\" : \"categorical\" , \"initial_price\" : \"integer\" , \"rental_price\" : \"integer\" } , format = 'ray_server' ; And you can query predictions as usual, either by conditioning on a subset of input colums: SELECT * FROM byom_ray_serve WHERE initial_price = 3000 AND rental_price = 3000 ; Or by JOINING to do batch predicions: SELECT tb . number_of_rooms , t . rental_price FROM mydb . test_data . home_rentals AS t JOIN mindsdb . byom_ray_serve AS tb WHERE t . rental_price > 5300 ; Please note that, if your model is behind a reverse proxy (e.g. nginx) you might have to increase the maximum limit for POST requests in order to receive the training data. MindsDB itself can send as much as you'd like and has been stress-tested with over a billion rows.","title":"1.1 Simple example - Logistic regression"},{"location":"sql/tutorials/byom/#12-example-keras-nlp-model","text":"For this example, we will consider a natural language processing (NLP) task where we want to train a neural network with Keras to detect if a tweet is related to a natural disaster (fires, earthquakes, etc.). Please download this dataset to follow the example. The code for the model here is a bit more complex than in section 1.1, but the same rules apply: we create a Ray Server based service that wraps around a Kaggle NLP Model which can be trained and then used for predictions: import re import time import json import string import requests from collections import Counter , defaultdict \u200b import ray from ray import serve \u200b import gensim import numpy as np import pandas as pd from tqdm import tqdm from nltk.util import ngrams from nltk.corpus import stopwords from nltk.tokenize import word_tokenize from fastapi import Request , FastAPI from sklearn.model_selection import train_test_split from sklearn.feature_extraction.text import CountVectorizer \u200b from tensorflow.keras.preprocessing.text import Tokenizer from tensorflow.keras.preprocessing.sequence import pad_sequences from tensorflow.keras.models import Sequential from tensorflow.keras.layers import Embedding , LSTM , Dense , SpatialDropout1D from tensorflow.keras.initializers import Constant from tensorflow.keras.optimizers import Adam \u200b app = FastAPI () stop = set ( stopwords . words ( 'english' )) \u200b \u200b async def parse_req ( request : Request ): data = await request . json () target = data . get ( 'target' , None ) di = json . loads ( data [ 'df' ]) df = pd . DataFrame ( di ) return df , target \u200b \u200b @serve . deployment ( route_prefix = \"/nlp_kaggle_model\" ) @serve . ingress ( app ) class Model : MAX_LEN = 100 GLOVE_DIM = 50 EPOCHS = 10 \u200b def __init__ ( self ): self . model = None \u200b @app . post ( \"/train\" ) async def train ( self , request : Request ): df , target = await parse_req ( request ) \u200b target_arr = df . pop ( target ) . values df = self . preprocess_df ( df ) train_corpus = self . create_corpus ( df ) \u200b self . embedding_dict = {} with open ( './glove.6B.50d.txt' , 'r' ) as f : for line in f : values = line . split () word = values [ 0 ] vectors = np . asarray ( values [ 1 :], 'float32' ) self . embedding_dict [ word ] = vectors f . close () \u200b self . tokenizer_obj = Tokenizer () self . tokenizer_obj . fit_on_texts ( train_corpus ) \u200b sequences = self . tokenizer_obj . texts_to_sequences ( train_corpus ) tweet_pad = pad_sequences ( sequences , maxlen = self . __class__ . MAX_LEN , truncating = 'post' , padding = 'post' ) df = tweet_pad [: df . shape [ 0 ]] \u200b word_index = self . tokenizer_obj . word_index num_words = len ( word_index ) + 1 embedding_matrix = np . zeros (( num_words , self . __class__ . GLOVE_DIM )) \u200b for word , i in tqdm ( word_index . items ()): if i > num_words : continue \u200b emb_vec = self . embedding_dict . get ( word ) if emb_vec is not None : embedding_matrix [ i ] = emb_vec \u200b self . model = Sequential () embedding = Embedding ( num_words , self . __class__ . GLOVE_DIM , embeddings_initializer = Constant ( embedding_matrix ), input_length = self . __class__ . MAX_LEN , trainable = False ) self . model . add ( embedding ) self . model . add ( SpatialDropout1D ( 0.2 )) self . model . add ( LSTM ( 64 , dropout = 0.2 , recurrent_dropout = 0.2 )) self . model . add ( Dense ( 1 , activation = 'sigmoid' )) \u200b optimzer = Adam ( learning_rate = 1e-5 ) self . model . compile ( loss = 'binary_crossentropy' , optimizer = optimzer , metrics = [ 'accuracy' ]) \u200b X_train , X_test , y_train , y_test = train_test_split ( df , target_arr , test_size = 0.15 ) self . model . fit ( X_train , y_train , batch_size = 4 , epochs = self . __class__ . EPOCHS , validation_data = ( X_test , y_test ), verbose = 2 ) \u200b return { 'status' : 'ok' } \u200b @app . post ( \"/predict\" ) async def predict ( self , request : Request ): df , _ = await parse_req ( request ) \u200b df = self . preprocess_df ( df ) test_corpus = self . create_corpus ( df ) \u200b sequences = self . tokenizer_obj . texts_to_sequences ( test_corpus ) tweet_pad = pad_sequences ( sequences , maxlen = self . __class__ . MAX_LEN , truncating = 'post' , padding = 'post' ) df = tweet_pad [: df . shape [ 0 ]] \u200b y_pre = self . model . predict ( df ) y_pre = np . round ( y_pre ) . astype ( int ) . flatten () . tolist () sub = pd . DataFrame ({ 'target' : y_pre }) \u200b pred_dict = { 'prediction' : [ float ( x ) for x in sub [ 'target' ] . values ]} return pred_dict \u200b def preprocess_df ( self , df ): df = df [[ 'text' ]] df [ 'text' ] = df [ 'text' ] . apply ( lambda x : self . remove_URL ( x )) df [ 'text' ] = df [ 'text' ] . apply ( lambda x : self . remove_html ( x )) df [ 'text' ] = df [ 'text' ] . apply ( lambda x : self . remove_emoji ( x )) df [ 'text' ] = df [ 'text' ] . apply ( lambda x : self . remove_punct ( x )) return df \u200b def remove_URL ( self , text ): url = re . compile ( r 'https?://\\S+|www\\.\\S+' ) return url . sub ( r '' , text ) \u200b def remove_html ( self , text ): html = re . compile ( r '<.*?>' ) return html . sub ( r '' , text ) \u200b def remove_punct ( self , text ): table = str . maketrans ( '' , '' , string . punctuation ) return text . translate ( table ) \u200b def remove_emoji ( self , text ): emoji_pattern = re . compile ( \"[\" u \" \\U0001F600 - \\U0001F64F \" # emoticons u \" \\U0001F300 - \\U0001F5FF \" # symbols & pictographs u \" \\U0001F680 - \\U0001F6FF \" # transport & map symbols u \" \\U0001F1E0 - \\U0001F1FF \" # flags (iOS) u \" \\U00002702 - \\U000027B0 \" u \" \\U000024C2 - \\U0001F251 \" \"]+\" , flags = re . UNICODE ) return emoji_pattern . sub ( r '' , text ) \u200b def create_corpus ( self , df ): corpus = [] for tweet in tqdm ( df [ 'text' ]): words = [ word . lower () for word in word_tokenize ( tweet ) if (( word . isalpha () == 1 ) & ( word not in stop ))] corpus . append ( words ) return corpus \u200b \u200b if __name__ == '__main__' : \u200b ray . init () serve . start ( detached = True ) \u200b Model . deploy () \u200b while True : time . sleep ( 1 ) We need access to the training data, so we'll create a table called nlp_kaggle_train to load the dataset that the original model uses. And ingest it into a table with the following schema: id INT , keyword VARCHAR ( 255 ), location VARCHAR ( 255 ), text VARCHAR ( 5000 ), target INT Note: specifics of the schema and how to ingest the csv will vary depending on your database. Next, we can register and train the above custom model using the following query: CREATE MODEL mindsdb . byom_ray_serve_nlp FROM maria ( SELECT text , target FROM test . nlp_kaggle_train ) PREDICT target USING url . train = 'http://127.0.0.1:8000/nlp_kaggle_model/train' , url . predict = 'http://127.0.0.1:8000/nlp_kaggle_model/predict' , dtype_dict = { \"text\" : \"rich_text\" , \"target\" : \"integer\" } , format = 'ray_server' ; Training will take a while given that this model is a neural network rather than a simple logistic regression. You can check its status with the query SELECT * FROM mindsdb.models WHERE name = 'byom_ray_serve_nlp'; , much like you'd do with a \"normal\" MindsDB predictor. Once the predictor's status becomes trained we can query it for predictions as usual: SELECT * FROM mindsdb . byom_ray_serve_nlp WHERE text = 'The tsunami is coming, seek high ground' ; Which would, hopefully, output 1 . Alternatively, we can try out this tweet to expect 0 as an output: SELECT * FROM mindsdb . byom_ray_serve_nlp WHERE text = 'This is lovely dear friend' ; If your results do not match this example, it could help to train the model for a longer amount of epochs.","title":"1.2. Example - Keras NLP model"},{"location":"sql/tutorials/byom/#2-mlflow","text":"","title":"2. MLFlow"},{"location":"sql/tutorials/byom/#21-simple-example-logistic-regression","text":"MLFlow is a tool that you can use to train and serve models, among other features like organizing experiments, tracking metrics, etc. Given there is no way to train an MLflow-wrapped model using its API, you will have to train your models outside of MindsDB by pulling your data manually (i.e. with a script), ideally using a MLflow run or experiment. The first step would be to create a script where you train a model and save it using one of the saving methods that MLflow exposes. For this example, we will use the model in this simple tutorial where the method is mlflow.sklearn.log_model ( here ), given that the model is built with scikit-learn. Once trained, you need to make sure the model is served and listening for input in a URL of your choice (note, this can mean your model can run on a different machine than the one executing MindsDB). Let's assume this URL to be http://localhost:5000/invocations for now. This means you would execute the following command in your terminal, from the directory where the model was stored: mlflow models serve --model-uri runs:/<run-id>/model With <run-id> given in the output of the command python train.py used for actually training the model. Next, we're going to bring this model into MindsDB: CREATE MODEL mindsdb . byom_mlflow PREDICT ` 1 ` -- `1` is the target column name USING url . predict = 'http://localhost:5000/invocations' , format = 'mlflow' , data_dtype = { \"0\" : \"integer\" , \"1\" : \"integer\" } We can now run predictions as usual, by using the WHERE statement or joining on a data table with an appropriate schema: SELECT ` 1 ` FROM byom_mlflow WHERE ` 0 `= 2 ;","title":"2.1 Simple example - Logistic Regression"},{"location":"sql/tutorials/byom/#22-advanced-example-keras-nlp-model","text":"Same use case as in section 1.2, be sure to download the dataset to reproduce the steps here. In this case, we will take a look at the best practices when your model needs custom data preprocessing code (which, realistically, will be fairly common). The key difference is that we now need to use the mlflow.pyfunc module to both 1) save the model using mlflow.pyfunc.save_model and 2) subclass mlflow.pyfunc.PythonModel to wrap the model in an MLflow-compatible way that will enable our custom inference logic to be called.","title":"2.2. Advanced example - Keras NLP model"},{"location":"sql/tutorials/byom/#saving-the-model","text":"In the same script where you train the model (which you can find in the final section of 2.2) there should be a call at the end where you actually use mlflow to save every produced artifact: mlflow . pyfunc . save_model ( path = \"nlp_kaggle\" , python_model = Model (), conda_env = conda_env , artifacts = artifacts ) Here, artifacts will be a dictionary with all expected produced outputs when running the training phase. In this case, we want both a model and a tokenizer to preprocess the input text. On the other hand, conda_env specifies the environment under which your model should be executed once served in a self-contained conda environment, so it should include all required packages and dependencies. For this example, they look like this: # these will be accessible inside the Model() wrapper artifacts = { 'model' : model_path , 'tokenizer_path' : tokenizer_path , } # specs for environment that will be created when serving the model conda_env = { 'name' : 'nlp_keras_env' , 'channels' : [ 'defaults' ], 'dependencies' : [ 'python=3.8' , 'pip' , { 'pip' : [ 'mlflow' , 'tensorflow' , 'cloudpickle' , 'nltk' , 'pandas' , 'numpy' , 'scikit-learn' , 'tqdm' , ], }, ], } Finally, to actually store the model you need to provide the wrapper class that will 1) load all produced artifacts into an accessible \"context\" and 2) implement all required inference logic: class Model ( mlflow . pyfunc . PythonModel ): def load_context ( self , context ): # we use paths in the context to load everything self . model_path = context . artifacts [ 'model' ] self . model = load_model ( self . model_path ) with open ( context . artifacts [ 'tokenizer_path' ], 'rb' ) as f : self . tokenizer = pickle . load ( f ) def predict ( self , context , model_input ): # preprocess input, tokenize, pad, and call the model df = preprocess_df ( model_input ) corpus = create_corpus ( df ) sequences = self . tokenizer . texts_to_sequences ( corpus ) tweet_pad = pad_sequences ( sequences , maxlen = MAX_LEN , truncating = 'post' , padding = 'post' ) df = tweet_pad [: df . shape [ 0 ]] y_pre = self . model . predict ( df ) y_pre = np . round ( y_pre ) . astype ( int ) . flatten () . tolist () return list ( y_pre ) As you can see, here we are loading multiple artifacts and using them to guarantee the input data will be in the same format that was used when training. Ideally, you would abstract this even further into a single preprocess method that is called both at training time and inference time. Finally, serving is simple. Go to the directory where you called the above script, and execute mlflow models serve --model-uri ./nlp_kaggle . At this point, the rest is essentially the same as in the previous example. You can link the MLflow model with these SQL statements: CREATE MODEL mindsdb . byom_mlflow_nlp PREDICT ` target ` USING url . predict = 'http://localhost:5000/invocations' , format = 'mlflow' , dtype_dict = { \"text\" : \"rich text\" , \"target\" : \"binary\" } ; To get predictions, you can directly pass input data using the WHERE clause: SELECT target FROM mindsdb . byom_mlflow_nlp WHERE text = 'The tsunami is coming, seek high ground' ; Or you can JOIN with a data table. For this, you should ensure the table actually exists and that the database it belongs to has been connected to your MindsDB instance. For more details, refer to the same steps in the Ray Serve example (section 1.2). SELECT ta . text , tb . target as predicted FROM db_byom . test . nlp_kaggle_test as ta JOIN mindsdb . byom_mlflow_nlp as tb ;","title":"Saving the model"},{"location":"sql/tutorials/byom/#full-script","text":"Finally, for reference, here's the full script that trains and saves the model. The model is exactly the same as in section 1.2, so it may seem familiar. import re import pickle import string import mlflow.pyfunc import nltk import tqdm import sklearn import tensorflow import cloudpickle import numpy as np import pandas as pd from nltk.corpus import stopwords from nltk.tokenize import word_tokenize from sklearn.model_selection import train_test_split from tensorflow.keras.initializers import Constant from tensorflow.keras.layers import Embedding , LSTM , Dense , SpatialDropout1D from tensorflow.keras.models import Sequential from tensorflow.keras.optimizers import Adam from tensorflow.keras.preprocessing.sequence import pad_sequences from tensorflow.keras.preprocessing.text import Tokenizer from tensorflow.keras.models import load_model stop = set ( stopwords . words ( 'english' )) MAX_LEN = 100 GLOVE_DIM = 50 EPOCHS = 10 def preprocess_df ( df ): df = df [[ 'text' ]] funcs = [ remove_URL , remove_html , remove_emoji , remove_punct ] for fn in funcs : df [ 'text' ] = df [ 'text' ] . apply ( lambda x : fn ( x )) return df def remove_URL ( text ): url = re . compile ( r 'https?://\\S+|www\\.\\S+' ) return url . sub ( r '' , text ) def remove_html ( text ): html = re . compile ( r '<.*?>' ) return html . sub ( r '' , text ) def remove_punct ( text ): table = str . maketrans ( '' , '' , string . punctuation ) return text . translate ( table ) def remove_emoji ( text ): emoji_pattern = re . compile ( \"[\" u \" \\U0001F600 - \\U0001F64F \" # emoticons u \" \\U0001F300 - \\U0001F5FF \" # symbols & pictographs u \" \\U0001F680 - \\U0001F6FF \" # transport & map symbols u \" \\U0001F1E0 - \\U0001F1FF \" # flags (iOS) u \" \\U00002702 - \\U000027B0 \" u \" \\U000024C2 - \\U0001F251 \" \"]+\" , flags = re . UNICODE ) return emoji_pattern . sub ( r '' , text ) def create_corpus ( df ): corpus = [] for tweet in tqdm . tqdm ( df [ 'text' ]): words = [ word . lower () for word in word_tokenize ( tweet ) if (( word . isalpha () == 1 ) & ( word not in stop ))] corpus . append ( words ) return corpus class Model ( mlflow . pyfunc . PythonModel ): def load_context ( self , context ): self . model_path = context . artifacts [ 'model' ] with open ( context . artifacts [ 'tokenizer_path' ], 'rb' ) as f : self . tokenizer = pickle . load ( f ) self . model = load_model ( self . model_path ) def predict ( self , context , model_input ): df = preprocess_df ( model_input ) corpus = create_corpus ( df ) sequences = self . tokenizer . texts_to_sequences ( corpus ) tweet_pad = pad_sequences ( sequences , maxlen = MAX_LEN , truncating = 'post' , padding = 'post' ) df = tweet_pad [: df . shape [ 0 ]] y_pre = self . model . predict ( df ) y_pre = np . round ( y_pre ) . astype ( int ) . flatten () . tolist () return list ( y_pre ) if __name__ == '__main__' : train_model = True model_path = './' tokenizer_path = './tokenizer.pkl' run_name = 'test_run' mlflow_pyfunc_model_path = \"nlp_kaggle\" mlflow . set_tracking_uri ( \"sqlite:///mlflow.db\" ) if train_model : # preprocess data df = pd . read_csv ( './train.csv' ) target = df [[ 'target' ]] target_arr = target . values df = preprocess_df ( df ) train_corpus = create_corpus ( df ) # load embeddings embedding_dict = {} with open ( './glove.6B.50d.txt' , 'r' ) as f : for line in f : values = line . split () word = values [ 0 ] vectors = np . asarray ( values [ 1 :], 'float32' ) embedding_dict [ word ] = vectors f . close () # generate and save tokenizer tokenizer_obj = Tokenizer () tokenizer_obj . fit_on_texts ( train_corpus ) with open ( tokenizer_path , 'wb' ) as f : pickle . dump ( tokenizer_obj , f ) # tokenize and pad sequences = tokenizer_obj . texts_to_sequences ( train_corpus ) tweet_pad = pad_sequences ( sequences , maxlen = MAX_LEN , truncating = 'post' , padding = 'post' ) df = tweet_pad [: df . shape [ 0 ]] word_index = tokenizer_obj . word_index num_words = len ( word_index ) + 1 embedding_matrix = np . zeros (( num_words , GLOVE_DIM )) # fill embedding matrix for word , i in tqdm . tqdm ( word_index . items ()): if i > num_words : continue emb_vec = embedding_dict . get ( word ) if emb_vec is not None : embedding_matrix [ i ] = emb_vec X_train , X_test , y_train , y_test = train_test_split ( df , target_arr , test_size = 0.15 ) # generate model model = Sequential () embedding = Embedding ( num_words , GLOVE_DIM , embeddings_initializer = Constant ( embedding_matrix ), input_length = MAX_LEN , trainable = False ) model . add ( embedding ) model . add ( SpatialDropout1D ( 0.2 )) model . add ( LSTM ( 64 , dropout = 0.2 , recurrent_dropout = 0.2 )) model . add ( Dense ( 1 , activation = 'sigmoid' )) optimzer = Adam ( learning_rate = 1e-5 ) model . compile ( loss = 'binary_crossentropy' , optimizer = optimzer , metrics = [ 'accuracy' ]) # train and save model . fit ( X_train , y_train , batch_size = 4 , epochs = EPOCHS , validation_data = ( X_test , y_test ), verbose = 2 ) model . save ( model_path ) # save in mlflow format artifacts = { 'model' : model_path , 'tokenizer_path' : tokenizer_path , } conda_env = { 'channels' : [ 'defaults' ], 'dependencies' : [ 'python=3.8' , 'pip' , { 'pip' : [ 'mlflow' , 'tensorflow== {} ' . format ( tensorflow . __version__ ), 'cloudpickle== {} ' . format ( cloudpickle . __version__ ), 'nltk== {} ' . format ( nltk . __version__ ), 'pandas== {} ' . format ( pd . __version__ ), 'numpy== {} ' . format ( np . __version__ ), 'scikit-learn== {} ' . format ( sklearn . __version__ ), 'tqdm== {} ' . format ( tqdm . __version__ ) ], }, ], 'name' : 'nlp_keras_env' } # Save and register the MLflow Model with mlflow . start_run ( run_name = run_name ) as run : mlflow . pyfunc . save_model ( path = mlflow_pyfunc_model_path , python_model = Model (), conda_env = conda_env , artifacts = artifacts ) result = mlflow . register_model ( f \"runs:/ { run . info . run_id } / { mlflow_pyfunc_model_path } \" , f \" { mlflow_pyfunc_model_path } \" )","title":"Full Script"},{"location":"sql/tutorials/crop-prediction/","text":"Crop Recomendation \u00b6 Dataset: Crop recomendation Data Communtiy Author: pixpack Modern agriculture is becoming very dependent on technology. From advanced machinery to specially selected crops. All the technology produces a lot of data that can be used for better adjustment of the farming process. One use case of machine learning in agriculture could be the selection of the best crop for a specific field to maximize the potential yield. Such problems are often called Classification Problems in machine learning. With MindsDB you can easily make automated machine learning predictions straight from your existing database. Even without advanced ML engineering skills, you can start leveraging predictive models that help you make better business decisions. In this tutorial, you will learn how to predict the best crop type based on field parameters using MindsDB . Pre-requisites \u00b6 Before you start make sure you have: Access to MindsDB. In this tutorial, we will use MindsDB Cloud GUI . If you want you can also deploy mindsdb on your premises, Check out the installation guide for Docker or PyPi . Downloaded the dataset. You can get it from Kaggle . Add your file to MindsDB \u00b6 MindsDB can integrates with many databases, in most scenarios your data will be stored in a database, if you decide to load this dataset into your database of choice, please follow instructions here as to how to connect mindsdb to your database. In this tutorial, we will be adding the dataset directly to MindsDB's GUI. For this example MindsDB Cloud GUI will be used.If you need to create an account you can find the guide on how to do it here . Alternatively, you can also use MindsDB's local deployment and access the GUI in your browser with 127.0.0.1:47334 . The first step will be to access MindsDB cloud where we will also make use of the SQL Editor: Once you are logged onto the Cloud GUI,navigate to either the Add Data button or the plug icon on the left side bar to select it. The screen will navigate to the Select your datasource page, select the option Files. Select Import Files. Click on Import a File to select your dataset from your local drive. Your dataset should be a maximum size of 10MB. Under Table name type in the name you would like to give your dataset which will be stored in MindsDB files. Once the dataset has been successfully uploaded into a table, you can query the dataset directly from the table. In the SQL Editor,type in the following syntax and select the button Run or Shift+Enter to execute the code: SELECT * FROM files . crops ; This confirms that the dataset has been successfully uploaded with all its rows. Create a predictor \u00b6 Now we can create a machine learning model with crops columns serving as features, and MindsDB takes care of the rest of the ML workflow automatically. There is a way to get your hands into the insides of the model to fine tune it, but we will not cover it in this tutorial. In the SQL Editor, type in the below syntax to create and train a machine learning predictive model: CREATE MODEL crop_predictor FROM files ( SELECT * FROM crops ) PREDICT label ; Select the button Run or Shift+Enter to execute the code. If the predictor is successfully created the console will display a message Query successfully completed . Now the predictor will begin training. You can check the status of the predictor with the following query. SELECT * FROM mindsdb . models WHERE name = 'crop_predictor' ; After the predictor has finished training, you will see a similar output. Note that MindsDB does model testing for you automatically, so you will immediately see if the predictor is accurate enough. You are now done with creating the predictor! \u2728 Make predictions \u00b6 In this section you will learn how to make predictions using your trained model. To run a prediction against new or existing data, you can use the following query. SELECT label FROM mindsdb . crop_predictor WHERE N = 77 AND P = 52 AND K = 17 AND temperature = 24 AND humidity = 20 . 74 AND ph = 5 . 71 AND rainfall = 75 . 82 ; As we have used a real data point from our dataset we can verify the prediction. N, P, K, temperature, humidity, ph, rainfall, label 77, 52, 17, 24.86374934, 65.7420046, 5.714799723, 75.82270467, maize As you can see, the model correctly predicted the most appropriate crop type for our field. OK, we made a prediction using a single query, but what if you want to make a batch prediction for a large set of data in your database? In this case, MindsDB allows you to Join this other table with the Predictor. In result, you will get another table as an output with a predicted value as one of its columns. Let\u2019s see how it works. Use the following command to create the batch prediction and execute with the Run button. SELECT collected_data . N , collected_data . P , collected_data . K , collected_data . temperature , collected_data . humidity , collected_data . ph , collected_data . rainfall , predictions . label AS predicted_crop_type FROM crops_integration . crops AS collected_data JOIN mindsdb . crop_predictor AS predictions LIMIT 5 ; As you can see below, the predictor has made multiple predictions for each data point in the collected_data table. You can also try selecting other fields to get more insight on the predictions. See the JOIN clause documentation for more information. You are now done with the tutorial! \ud83c\udf89 Please feel free to try it yourself. Sign up for a free MindsDB account to get up and running in 5 minutes, and if you need any help, feel free to ask in Slack or Github . For more check out other tutorials and MindsDB documentation .","title":"Crop Recommendation"},{"location":"sql/tutorials/crop-prediction/#crop-recomendation","text":"Dataset: Crop recomendation Data Communtiy Author: pixpack Modern agriculture is becoming very dependent on technology. From advanced machinery to specially selected crops. All the technology produces a lot of data that can be used for better adjustment of the farming process. One use case of machine learning in agriculture could be the selection of the best crop for a specific field to maximize the potential yield. Such problems are often called Classification Problems in machine learning. With MindsDB you can easily make automated machine learning predictions straight from your existing database. Even without advanced ML engineering skills, you can start leveraging predictive models that help you make better business decisions. In this tutorial, you will learn how to predict the best crop type based on field parameters using MindsDB .","title":"Crop Recomendation"},{"location":"sql/tutorials/crop-prediction/#pre-requisites","text":"Before you start make sure you have: Access to MindsDB. In this tutorial, we will use MindsDB Cloud GUI . If you want you can also deploy mindsdb on your premises, Check out the installation guide for Docker or PyPi . Downloaded the dataset. You can get it from Kaggle .","title":"Pre-requisites"},{"location":"sql/tutorials/crop-prediction/#add-your-file-to-mindsdb","text":"MindsDB can integrates with many databases, in most scenarios your data will be stored in a database, if you decide to load this dataset into your database of choice, please follow instructions here as to how to connect mindsdb to your database. In this tutorial, we will be adding the dataset directly to MindsDB's GUI. For this example MindsDB Cloud GUI will be used.If you need to create an account you can find the guide on how to do it here . Alternatively, you can also use MindsDB's local deployment and access the GUI in your browser with 127.0.0.1:47334 . The first step will be to access MindsDB cloud where we will also make use of the SQL Editor: Once you are logged onto the Cloud GUI,navigate to either the Add Data button or the plug icon on the left side bar to select it. The screen will navigate to the Select your datasource page, select the option Files. Select Import Files. Click on Import a File to select your dataset from your local drive. Your dataset should be a maximum size of 10MB. Under Table name type in the name you would like to give your dataset which will be stored in MindsDB files. Once the dataset has been successfully uploaded into a table, you can query the dataset directly from the table. In the SQL Editor,type in the following syntax and select the button Run or Shift+Enter to execute the code: SELECT * FROM files . crops ; This confirms that the dataset has been successfully uploaded with all its rows.","title":"Add your file to MindsDB"},{"location":"sql/tutorials/crop-prediction/#create-a-predictor","text":"Now we can create a machine learning model with crops columns serving as features, and MindsDB takes care of the rest of the ML workflow automatically. There is a way to get your hands into the insides of the model to fine tune it, but we will not cover it in this tutorial. In the SQL Editor, type in the below syntax to create and train a machine learning predictive model: CREATE MODEL crop_predictor FROM files ( SELECT * FROM crops ) PREDICT label ; Select the button Run or Shift+Enter to execute the code. If the predictor is successfully created the console will display a message Query successfully completed . Now the predictor will begin training. You can check the status of the predictor with the following query. SELECT * FROM mindsdb . models WHERE name = 'crop_predictor' ; After the predictor has finished training, you will see a similar output. Note that MindsDB does model testing for you automatically, so you will immediately see if the predictor is accurate enough. You are now done with creating the predictor! \u2728","title":"Create a predictor"},{"location":"sql/tutorials/crop-prediction/#make-predictions","text":"In this section you will learn how to make predictions using your trained model. To run a prediction against new or existing data, you can use the following query. SELECT label FROM mindsdb . crop_predictor WHERE N = 77 AND P = 52 AND K = 17 AND temperature = 24 AND humidity = 20 . 74 AND ph = 5 . 71 AND rainfall = 75 . 82 ; As we have used a real data point from our dataset we can verify the prediction. N, P, K, temperature, humidity, ph, rainfall, label 77, 52, 17, 24.86374934, 65.7420046, 5.714799723, 75.82270467, maize As you can see, the model correctly predicted the most appropriate crop type for our field. OK, we made a prediction using a single query, but what if you want to make a batch prediction for a large set of data in your database? In this case, MindsDB allows you to Join this other table with the Predictor. In result, you will get another table as an output with a predicted value as one of its columns. Let\u2019s see how it works. Use the following command to create the batch prediction and execute with the Run button. SELECT collected_data . N , collected_data . P , collected_data . K , collected_data . temperature , collected_data . humidity , collected_data . ph , collected_data . rainfall , predictions . label AS predicted_crop_type FROM crops_integration . crops AS collected_data JOIN mindsdb . crop_predictor AS predictions LIMIT 5 ; As you can see below, the predictor has made multiple predictions for each data point in the collected_data table. You can also try selecting other fields to get more insight on the predictions. See the JOIN clause documentation for more information. You are now done with the tutorial! \ud83c\udf89 Please feel free to try it yourself. Sign up for a free MindsDB account to get up and running in 5 minutes, and if you need any help, feel free to ask in Slack or Github . For more check out other tutorials and MindsDB documentation .","title":"Make predictions"},{"location":"sql/tutorials/customer-churn/","text":"Predicting Customer Churn with MindsDB \u00b6 Introduction \u00b6 In this tutorial, we'll create and train a machine learning model, or as we call it, an AI Table or a predictor . By querying the model, we'll predict the probability of churn for new customers of a telecom company. Make sure you have access to a working MindsDB installation, either locally or at MindsDB Cloud . If you want to learn how to set up your account at MindsDB Cloud, follow this guide . Another way is to set up MindsDB locally using Docker or Python . Let's get started. Data Setup \u00b6 Connecting the Data \u00b6 There are a couple of ways you can get the data to follow through with this tutorial. Connecting as a database Connecting as a file You can connect to a demo database that we've prepared for you. It contains the data used throughout this tutorial (the example_db . demo_data . customer_churn table). CREATE DATABASE example_db WITH ENGINE = \"postgres\" , PARAMETERS = { \"user\" : \"demo_user\" , \"password\" : \"demo_password\" , \"host\" : \"3.220.66.106\" , \"port\" : \"5432\" , \"database\" : \"demo\" } ; Now you can run queries directly on the demo database. Let's preview the data that we'll use to train our predictor. SELECT * FROM example_db . demo_data . customer_churn LIMIT 10 ; You can download the CSV data file here and upload it via MindsDB SQL Editor . Follow this guide to find out how to upload a file to MindsDB. Now you can run queries directly on the file as if it were a table. Let's preview the data that we'll use to train our predictor. SELECT * FROM files . churn LIMIT 10 ; Pay Attention to the Queries From now on, we'll use the files . churn file as a table. Make sure you replace it with example_db.demo_data.customer_churn if you connect the data as a database. Understanding the Data \u00b6 We use the customer churn dataset, where each row is one customer, to predict whether the customer is going to stop using the company products. Below is the sample data stored in the files . churn table. + ----------+------+-------------+-------+----------+------+------------+----------------+---------------+--------------+------------+----------------+-----------+-----------+---------------+--------------+----------------+-------------------------+--------------+------------+-----+ | customerID | gender | SeniorCitizen | Partner | Dependents | tenure | PhoneService | MultipleLines | InternetService | OnlineSecurity | OnlineBackup | DeviceProtection | TechSupport | StreamingTV | StreamingMovies | Contract | PaperlessBilling | PaymentMethod | MonthlyCharges | TotalCharges | Churn | + ----------+------+-------------+-------+----------+------+------------+----------------+---------------+--------------+------------+----------------+-----------+-----------+---------------+--------------+----------------+-------------------------+--------------+------------+-----+ | 7590 - VHVEG | Female | 0 | Yes | No | 1 | No | No phone service | DSL | No | Yes | No | No | No | No | Month - to - month | Yes | Electronic check | 29 . 85 | 29 . 85 | No | | 5575 - GNVDE | Male | 0 | No | No | 34 | Yes | No | DSL | Yes | No | Yes | No | No | No | One year | No | Mailed check | 56 . 95 | 1889 . 5 | No | | 3668 - QPYBK | Male | 0 | No | No | 2 | Yes | No | DSL | Yes | Yes | No | No | No | No | Month - to - month | Yes | Mailed check | 53 . 85 | 108 . 15 | Yes | | 7795 - CFOCW | Male | 0 | No | No | 45 | No | No phone service | DSL | Yes | No | Yes | Yes | No | No | One year | No | Bank transfer ( automatic ) | 42 . 3 | 1840 . 75 | No | | 9237 - HQITU | Female | 0 | No | No | 2 | Yes | No | Fiber optic | No | No | No | No | No | No | Month - to - month | Yes | Electronic check | 70 . 7 | 151 . 65 | Yes | + ----------+------+-------------+-------+----------+------+------------+----------------+---------------+--------------+------------+----------------+-----------+-----------+---------------+--------------+----------------+-------------------------+--------------+------------+-----+ Where: Column Description Data Type Usage CustomerId The identification number of a customer. character varying Feature Gender The gender of a customer. character varying Feature SeniorCitizen It indicates whether the customer is a senior citizen ( 1 ) or not ( 0 ). integer Feature Partner It indicates whether the customer has a partner ( Yes ) or not ( No ). character varying Feature Dependents It indicates whether the customer has dependents ( Yes ) or not ( No ). character varying Feature Tenure Number of months the customer has been staying with the company. integer Feature PhoneService It indicates whether the customer has a phone service ( Yes ) or not ( No ). character varying Feature MultipleLines It indicates whether the customer has multiple lines ( Yes ) or not ( No , No phone service ). character varying Feature InternetService Customer\u2019s internet service provider ( DSL , Fiber optic , No ). character varying Feature OnlineSecurity It indicates whether the customer has online security ( Yes ) or not ( No , No internet service ). character varying Feature OnlineBackup It indicates whether the customer has online backup ( Yes ) or not ( No , No internet service ). character varying Feature DeviceProtection It indicates whether the customer has device protection ( Yes ) or not ( No , No internet service ). character varying Feature TechSupport It indicates whether the customer has tech support ( Yes ) or not ( No , No internet service ). character varying Feature StreamingTv It indicates whether the customer has streaming TV ( Yes ) or not ( No , No internet service ). character varying Feature StreamingMovies It indicates whether the customer has streaming movies ( Yes ) or not ( No , No internet service ). character varying Feature Contract The contract term of the customer ( Month-to-month , One year , Two year ). character varying Feature PaperlessBilling It indicates whether the customer has paperless billinig ( Yes ) or not ( No ). character varying Feature PaymentMethod Customer\u2019s payment method ( Electronic check , Mailed check , Bank transfer (automatic) , Credit card (automatic) ). character varying Feature MonthlyCharges The monthly charge amount. money Feature TotalCharges The total amount charged to the customer. money Feature Churn It indicates whether the customer churned ( Yes ) or not ( No ). character varying Label Labels and Features A label is a column whose values will be predicted (the y variable in simple linear regression). A feature is a column used to train the model (the x variable in simple linear regression). Training a Predictor \u00b6 Let's create and train the machine learning model. For that, we use the CREATE MODEL statement and specify the input columns used to train FROM (features) and what we want to PREDICT (labels). CREATE MODEL mindsdb . customer_churn_predictor FROM files ( SELECT * FROM churn ) PREDICT Churn ; We use all of the columns as features, except for the Churn column, whose values will be predicted. Status of a Predictor \u00b6 A predictor may take a couple of minutes for the training to complete. You can monitor the status of the predictor by using this SQL command: SELECT status FROM mindsdb . models WHERE name = 'customer_churn_predictor' ; If we run it right after creating a predictor, we get this output: + ------------+ | status | + ------------+ | generating | + ------------+ A bit later, this is the output: + ----------+ | status | + ----------+ | training | + ----------+ And at last, this should be the output: + ----------+ | status | + ----------+ | complete | + ----------+ Now, if the status of our predictor says complete , we can start making predictions! Making Predictions \u00b6 Making a Single Prediction \u00b6 You can make predictions by querying the predictor as if it were a table. The SELECT statement lets you make predictions for the label based on the chosen features. SELECT Churn , Churn_confidence , Churn_explain FROM mindsdb . customer_churn_predictor WHERE SeniorCitizen = 0 AND Partner = 'Yes' AND Dependents = 'No' AND tenure = 1 AND PhoneService = 'No' AND MultipleLines = 'No phone service' AND InternetService = 'DSL' ; On execution, we get: + -------+---------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------+ | Churn | Churn_confidence | Churn_explain | + -------+---------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------+ | Yes | 0 . 7752808988764045 | { \"predicted_value\" : \"Yes\" , \"confidence\" : 0 . 7752808988764045 , \"anomaly\" : null , \"truth\" : null , \"probability_class_No\" : 0 . 4756 , \"probability_class_Yes\" : 0 . 5244 } | + -------+---------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------+ To get more accurate predictions, we should provide as much data as possible in the WHERE clause. Let's run another query. SELECT Churn , Churn_confidence , Churn_explain FROM mindsdb . customer_churn_predictor WHERE SeniorCitizen = 0 AND Partner = 'Yes' AND Dependents = 'No' AND tenure = 1 AND PhoneService = 'No' AND MultipleLines = 'No phone service' AND InternetService = 'DSL' AND Contract = 'Month-to-month' AND MonthlyCharges = 29 . 85 AND TotalCharges = 29 . 85 AND OnlineBackup = 'Yes' AND OnlineSecurity = 'No' AND DeviceProtection = 'No' AND TechSupport = 'No' AND StreamingTV = 'No' AND StreamingMovies = 'No' AND PaperlessBilling = 'Yes' AND PaymentMethod = 'Electronic check' ; On execution, we get: + -------+---------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------+ | Churn | Churn_confidence | Churn_explain | + -------+---------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------+ | Yes | 0 . 8202247191011236 | { \"predicted_value\" : \"Yes\" , \"confidence\" : 0 . 8202247191011236 , \"anomaly\" : null , \"truth\" : null , \"probability_class_No\" : 0 . 4098 , \"probability_class_Yes\" : 0 . 5902 } | + -------+---------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------+ MindsDB predicted the probability of this customer churning with confidence of around 82%. The previous query predicted it with confidence of around 79%. So providing more data improved the confidence level of predictions. Making Batch Predictions \u00b6 Also, you can make bulk predictions by joining a data table with your predictor using JOIN . SELECT t . customerID , t . Contract , t . MonthlyCharges , m . Churn FROM files . churn AS t JOIN mindsdb . customer_churn_predictor AS m LIMIT 100 ; On execution, we get: + ----------------+-------------------+------------------+---------+ | customerID | Contract | MonthlyCharges | Churn | + ----------------+-------------------+------------------+---------+ | 7590 - VHVEG | Month - to - month | 29 . 85 | Yes | | 5575 - GNVDE | One year | 56 . 95 | No | | 3668 - QPYBK | Month - to - month | 53 . 85 | Yes | | 7795 - CFOCW | One year | 42 . 3 | No | | 9237 - HQITU | Month - to - month | 70 . 7 | Yes | + ----------------+-------------------+------------------+---------+ What's Next? \u00b6 Have fun while trying it out yourself! Bookmark MindsDB repository on GitHub . Sign up for a free MindsDB account . Engage with the MindsDB community on Slack or GitHub to ask questions and share your ideas and thoughts. If this tutorial was helpful, please give us a GitHub star here .","title":"Customer Churn"},{"location":"sql/tutorials/customer-churn/#predicting-customer-churn-with-mindsdb","text":"","title":"Predicting Customer Churn with MindsDB"},{"location":"sql/tutorials/customer-churn/#introduction","text":"In this tutorial, we'll create and train a machine learning model, or as we call it, an AI Table or a predictor . By querying the model, we'll predict the probability of churn for new customers of a telecom company. Make sure you have access to a working MindsDB installation, either locally or at MindsDB Cloud . If you want to learn how to set up your account at MindsDB Cloud, follow this guide . Another way is to set up MindsDB locally using Docker or Python . Let's get started.","title":"Introduction"},{"location":"sql/tutorials/customer-churn/#data-setup","text":"","title":"Data Setup"},{"location":"sql/tutorials/customer-churn/#connecting-the-data","text":"There are a couple of ways you can get the data to follow through with this tutorial. Connecting as a database Connecting as a file You can connect to a demo database that we've prepared for you. It contains the data used throughout this tutorial (the example_db . demo_data . customer_churn table). CREATE DATABASE example_db WITH ENGINE = \"postgres\" , PARAMETERS = { \"user\" : \"demo_user\" , \"password\" : \"demo_password\" , \"host\" : \"3.220.66.106\" , \"port\" : \"5432\" , \"database\" : \"demo\" } ; Now you can run queries directly on the demo database. Let's preview the data that we'll use to train our predictor. SELECT * FROM example_db . demo_data . customer_churn LIMIT 10 ; You can download the CSV data file here and upload it via MindsDB SQL Editor . Follow this guide to find out how to upload a file to MindsDB. Now you can run queries directly on the file as if it were a table. Let's preview the data that we'll use to train our predictor. SELECT * FROM files . churn LIMIT 10 ; Pay Attention to the Queries From now on, we'll use the files . churn file as a table. Make sure you replace it with example_db.demo_data.customer_churn if you connect the data as a database.","title":"Connecting the Data"},{"location":"sql/tutorials/customer-churn/#understanding-the-data","text":"We use the customer churn dataset, where each row is one customer, to predict whether the customer is going to stop using the company products. Below is the sample data stored in the files . churn table. + ----------+------+-------------+-------+----------+------+------------+----------------+---------------+--------------+------------+----------------+-----------+-----------+---------------+--------------+----------------+-------------------------+--------------+------------+-----+ | customerID | gender | SeniorCitizen | Partner | Dependents | tenure | PhoneService | MultipleLines | InternetService | OnlineSecurity | OnlineBackup | DeviceProtection | TechSupport | StreamingTV | StreamingMovies | Contract | PaperlessBilling | PaymentMethod | MonthlyCharges | TotalCharges | Churn | + ----------+------+-------------+-------+----------+------+------------+----------------+---------------+--------------+------------+----------------+-----------+-----------+---------------+--------------+----------------+-------------------------+--------------+------------+-----+ | 7590 - VHVEG | Female | 0 | Yes | No | 1 | No | No phone service | DSL | No | Yes | No | No | No | No | Month - to - month | Yes | Electronic check | 29 . 85 | 29 . 85 | No | | 5575 - GNVDE | Male | 0 | No | No | 34 | Yes | No | DSL | Yes | No | Yes | No | No | No | One year | No | Mailed check | 56 . 95 | 1889 . 5 | No | | 3668 - QPYBK | Male | 0 | No | No | 2 | Yes | No | DSL | Yes | Yes | No | No | No | No | Month - to - month | Yes | Mailed check | 53 . 85 | 108 . 15 | Yes | | 7795 - CFOCW | Male | 0 | No | No | 45 | No | No phone service | DSL | Yes | No | Yes | Yes | No | No | One year | No | Bank transfer ( automatic ) | 42 . 3 | 1840 . 75 | No | | 9237 - HQITU | Female | 0 | No | No | 2 | Yes | No | Fiber optic | No | No | No | No | No | No | Month - to - month | Yes | Electronic check | 70 . 7 | 151 . 65 | Yes | + ----------+------+-------------+-------+----------+------+------------+----------------+---------------+--------------+------------+----------------+-----------+-----------+---------------+--------------+----------------+-------------------------+--------------+------------+-----+ Where: Column Description Data Type Usage CustomerId The identification number of a customer. character varying Feature Gender The gender of a customer. character varying Feature SeniorCitizen It indicates whether the customer is a senior citizen ( 1 ) or not ( 0 ). integer Feature Partner It indicates whether the customer has a partner ( Yes ) or not ( No ). character varying Feature Dependents It indicates whether the customer has dependents ( Yes ) or not ( No ). character varying Feature Tenure Number of months the customer has been staying with the company. integer Feature PhoneService It indicates whether the customer has a phone service ( Yes ) or not ( No ). character varying Feature MultipleLines It indicates whether the customer has multiple lines ( Yes ) or not ( No , No phone service ). character varying Feature InternetService Customer\u2019s internet service provider ( DSL , Fiber optic , No ). character varying Feature OnlineSecurity It indicates whether the customer has online security ( Yes ) or not ( No , No internet service ). character varying Feature OnlineBackup It indicates whether the customer has online backup ( Yes ) or not ( No , No internet service ). character varying Feature DeviceProtection It indicates whether the customer has device protection ( Yes ) or not ( No , No internet service ). character varying Feature TechSupport It indicates whether the customer has tech support ( Yes ) or not ( No , No internet service ). character varying Feature StreamingTv It indicates whether the customer has streaming TV ( Yes ) or not ( No , No internet service ). character varying Feature StreamingMovies It indicates whether the customer has streaming movies ( Yes ) or not ( No , No internet service ). character varying Feature Contract The contract term of the customer ( Month-to-month , One year , Two year ). character varying Feature PaperlessBilling It indicates whether the customer has paperless billinig ( Yes ) or not ( No ). character varying Feature PaymentMethod Customer\u2019s payment method ( Electronic check , Mailed check , Bank transfer (automatic) , Credit card (automatic) ). character varying Feature MonthlyCharges The monthly charge amount. money Feature TotalCharges The total amount charged to the customer. money Feature Churn It indicates whether the customer churned ( Yes ) or not ( No ). character varying Label Labels and Features A label is a column whose values will be predicted (the y variable in simple linear regression). A feature is a column used to train the model (the x variable in simple linear regression).","title":"Understanding the Data"},{"location":"sql/tutorials/customer-churn/#training-a-predictor","text":"Let's create and train the machine learning model. For that, we use the CREATE MODEL statement and specify the input columns used to train FROM (features) and what we want to PREDICT (labels). CREATE MODEL mindsdb . customer_churn_predictor FROM files ( SELECT * FROM churn ) PREDICT Churn ; We use all of the columns as features, except for the Churn column, whose values will be predicted.","title":"Training a Predictor"},{"location":"sql/tutorials/customer-churn/#status-of-a-predictor","text":"A predictor may take a couple of minutes for the training to complete. You can monitor the status of the predictor by using this SQL command: SELECT status FROM mindsdb . models WHERE name = 'customer_churn_predictor' ; If we run it right after creating a predictor, we get this output: + ------------+ | status | + ------------+ | generating | + ------------+ A bit later, this is the output: + ----------+ | status | + ----------+ | training | + ----------+ And at last, this should be the output: + ----------+ | status | + ----------+ | complete | + ----------+ Now, if the status of our predictor says complete , we can start making predictions!","title":"Status of a Predictor"},{"location":"sql/tutorials/customer-churn/#making-predictions","text":"","title":"Making Predictions"},{"location":"sql/tutorials/customer-churn/#making-a-single-prediction","text":"You can make predictions by querying the predictor as if it were a table. The SELECT statement lets you make predictions for the label based on the chosen features. SELECT Churn , Churn_confidence , Churn_explain FROM mindsdb . customer_churn_predictor WHERE SeniorCitizen = 0 AND Partner = 'Yes' AND Dependents = 'No' AND tenure = 1 AND PhoneService = 'No' AND MultipleLines = 'No phone service' AND InternetService = 'DSL' ; On execution, we get: + -------+---------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------+ | Churn | Churn_confidence | Churn_explain | + -------+---------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------+ | Yes | 0 . 7752808988764045 | { \"predicted_value\" : \"Yes\" , \"confidence\" : 0 . 7752808988764045 , \"anomaly\" : null , \"truth\" : null , \"probability_class_No\" : 0 . 4756 , \"probability_class_Yes\" : 0 . 5244 } | + -------+---------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------+ To get more accurate predictions, we should provide as much data as possible in the WHERE clause. Let's run another query. SELECT Churn , Churn_confidence , Churn_explain FROM mindsdb . customer_churn_predictor WHERE SeniorCitizen = 0 AND Partner = 'Yes' AND Dependents = 'No' AND tenure = 1 AND PhoneService = 'No' AND MultipleLines = 'No phone service' AND InternetService = 'DSL' AND Contract = 'Month-to-month' AND MonthlyCharges = 29 . 85 AND TotalCharges = 29 . 85 AND OnlineBackup = 'Yes' AND OnlineSecurity = 'No' AND DeviceProtection = 'No' AND TechSupport = 'No' AND StreamingTV = 'No' AND StreamingMovies = 'No' AND PaperlessBilling = 'Yes' AND PaymentMethod = 'Electronic check' ; On execution, we get: + -------+---------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------+ | Churn | Churn_confidence | Churn_explain | + -------+---------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------+ | Yes | 0 . 8202247191011236 | { \"predicted_value\" : \"Yes\" , \"confidence\" : 0 . 8202247191011236 , \"anomaly\" : null , \"truth\" : null , \"probability_class_No\" : 0 . 4098 , \"probability_class_Yes\" : 0 . 5902 } | + -------+---------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------+ MindsDB predicted the probability of this customer churning with confidence of around 82%. The previous query predicted it with confidence of around 79%. So providing more data improved the confidence level of predictions.","title":"Making a Single Prediction"},{"location":"sql/tutorials/customer-churn/#making-batch-predictions","text":"Also, you can make bulk predictions by joining a data table with your predictor using JOIN . SELECT t . customerID , t . Contract , t . MonthlyCharges , m . Churn FROM files . churn AS t JOIN mindsdb . customer_churn_predictor AS m LIMIT 100 ; On execution, we get: + ----------------+-------------------+------------------+---------+ | customerID | Contract | MonthlyCharges | Churn | + ----------------+-------------------+------------------+---------+ | 7590 - VHVEG | Month - to - month | 29 . 85 | Yes | | 5575 - GNVDE | One year | 56 . 95 | No | | 3668 - QPYBK | Month - to - month | 53 . 85 | Yes | | 7795 - CFOCW | One year | 42 . 3 | No | | 9237 - HQITU | Month - to - month | 70 . 7 | Yes | + ----------------+-------------------+------------------+---------+","title":"Making Batch Predictions"},{"location":"sql/tutorials/customer-churn/#whats-next","text":"Have fun while trying it out yourself! Bookmark MindsDB repository on GitHub . Sign up for a free MindsDB account . Engage with the MindsDB community on Slack or GitHub to ask questions and share your ideas and thoughts. If this tutorial was helpful, please give us a GitHub star here .","title":"What's Next?"},{"location":"sql/tutorials/diabetes/","text":"Diabetes Dataset: Diabetes Data Communtiy Author: Chandre Tosca Van Der Westhuizen Diabetes is a metabolic disease that causes high blood sugar and if left untreated can damage your nerves, eyes, kidneys and other organs. It is known as a silent killer, as recent studies have shown that by the year 2040 the world's diabetic patients will reach 642 million. The need to analyze vast medical data to assist in the diagnoses, treatment and management of illnesses is increasing for the medical community. With the rapid development of machine learning, it has been applied to many aspects of medical health and is transforming the healthcare system. The vitality to intelligently transform information into valuable knowledge through machine learning has become more present in biosciences. With the use of predictive models, MindsDB can assist in classifying diabetic and non-diabetic patients or those who pose a high risk. This is just a small showcase on how MindsDB's machine learning will be able to assist in vastly enhancing the reach of illnesses, thereby making it more efficient and can revolutionize businesses and most importantly the health care system. In this tutorial we will be exploring how we can use a machine learning model to classify negative and positive cases for diabetes. MindsDB allows you to train your model from CSV data directly, however for this tutorial you will: Establish a connection between your database and MindsDB via MindsDB GUI(Cloud and local instance). Allow connections to the database using Ngrok. Create a machine learning model using SQL. Make a prediction. Connect your database to MindsDB GUI \u00b6 MindsDB has a functionality to upload your dataset directly to MindsDB. However in this tutorial, you will be shown how to connect your database to MindsDB cloud and local instance. For this example we will be using a local postgres database, therefore we will connect using an ngrok tunnel. Running a Ngrok Tunnel \u00b6 To make our database publicly avaiable, we will use ngrok . The following command can be run in docker or a terminal on your device to set up a ngrok tunnel. ngrok tcp [ db-port ] For this example the port number used is 5432. You should see a similar output: Session Status online Account chandre (Plan: Free) Version 2.3.40 Region United States (us) Web Interface http://127.0.0.1:4040 Forwarding tcp://4.tcp.ngrok.io:13096 -> localhost:5432 The information required is by the forwarded address, next to 'Forwarding' select and copy 4.tcp.ngrok.io:13096 . Once you have copied the information, you can add it to the information requested by the MindsDB GUI which we will get to in a moment. For the next steps we will log into the MindsDB cloud interface and local gui. MindsDB Cloud is perfect if you are unable to install MindsDB on your device. If you are not registered yet, feel free to follow the below guide. If you have already registered, skip the next steps to connect your database. MindsDB GUI- Cloud MySQL Editor & MySQL CLI \u00b6 The next step will be to connect your database to MindsDB. You can visit this link and follow the steps to create a MindsDB Cloud account. Access the MySQL Editor here . If you are using MindsDB through Docker, you can run the command below to start MindsDB: docker run -p 47334 :47334 -p 47335 :47335 mindsdb/mindsdb Once you are connected,head over to your browser and in the browser tab type in the host number 127.0.0.1:47334 to access the local MindsDB GUI. Establish connection between your database and MindsDB GUI \u00b6 Once you have accessed the GUI, you will be able to select a database type and enter the required parameters: - On the user interface,select the plug icon on the left sidebar. - A page will populate with different datasources to select from. For this tutorial we are choosing PostgreSQL. The page will automatically direct to the SQL editor defaulting the syntax parameters required to create a connection with PostgreSQL: CREATE DATABASE : This will be the chosen display name for your database. For this tutorial we will choose airbyte. WITH ENGINE : name of the mindsdb handler,in this example it will be Postgres. PARAMETERS = \"user\": Your database user. for this tutorial it is postgres \"password\": Your password. \"host\":host, it can be an ip or an url. For this tutorial it will be the forwarding address 4.tcp.ngrok.io \"port\": \"13096\",common port is 5432, for this tutorial we will use 15076. \"database\":The name of your database (optional). For this tutorial we will be using postgres. Select the button RUN or select Shift+Enter to execute the query. You are now done with connecting MindsDB to your database! \ud83d\ude80 Create a predictor \u00b6 Now we are ready to create our own predictor! We will start by using the MySQL API to connect to MindsDB and with a single SQL command create a predictor. The predictor we will create will be trained to determine negative and positive cases for diabetes. Use the following query to create a predictor that will predict the Class ( positive or negative ) for the specific field parameters. CREATE MODEL diabetes_predictor FROM mindsdb_prediction ( SELECT * FROM diabetes ) PREDICT class ; Select the RUN button,alternatively select Shift+Enter to execute the query. You will receive the message 'Query successfully completed' if the machine learning model is successfully created. The predictor was created successfully and has started training. To check the status of the model, use the below query. SELECT * FROM mindsdb . models WHERE name = 'diabetes_predictor' ; After the predictor has finished training, you will see a similar output. Note that MindsDB does model testing for you automatically, so you will immediately see if the predictor is accurate enough. The predictor has completed its training, indicated by the status column, and shows the accuracy of the model. You can revisit training new predictors to increase accuracy by changing the query to better suit the dataset i.e. omitting certain columns etc. Good job! We have successfully created and trained a predictive model \u2728 Make predictions \u00b6 In this section you will learn how to make predictions using your trained model. Now we will use the trained model to make predictions using a SQL query Use the following query using mock data with the predictor. SELECT Class FROM mindsdb . diabetes_predictor WHERE number_of_times_pregnant = 0 AND plasma_glucose_concentration = 135 . 0 AND diastolic_blood_pressure = 65 . 0 AND triceps_skin_fold_thickness = 30 AND two_Hour_serum_insulin = 0 AND body_mass_index = 23 . 5 AND diabetes_pedigree_function = 0 . 366 AND age = 31 ; MindsDB will provide you with results similar to below: The machine learning model predicted the diabetic class to be negative. Viola! We have successfully created and trained a model and made our own prediction. How easy and amazing is MindsDB? \ud83c\udf89 Want to try it out? Sign up for a free MindsDB account Join MindsDB community on Slack and GitHub to ask questions, share and express ideas and thoughts!","title":"Diabetes"},{"location":"sql/tutorials/diabetes/#connect-your-database-to-mindsdb-gui","text":"MindsDB has a functionality to upload your dataset directly to MindsDB. However in this tutorial, you will be shown how to connect your database to MindsDB cloud and local instance. For this example we will be using a local postgres database, therefore we will connect using an ngrok tunnel.","title":"Connect your database to MindsDB GUI"},{"location":"sql/tutorials/diabetes/#running-a-ngrok-tunnel","text":"To make our database publicly avaiable, we will use ngrok . The following command can be run in docker or a terminal on your device to set up a ngrok tunnel. ngrok tcp [ db-port ] For this example the port number used is 5432. You should see a similar output: Session Status online Account chandre (Plan: Free) Version 2.3.40 Region United States (us) Web Interface http://127.0.0.1:4040 Forwarding tcp://4.tcp.ngrok.io:13096 -> localhost:5432 The information required is by the forwarded address, next to 'Forwarding' select and copy 4.tcp.ngrok.io:13096 . Once you have copied the information, you can add it to the information requested by the MindsDB GUI which we will get to in a moment. For the next steps we will log into the MindsDB cloud interface and local gui. MindsDB Cloud is perfect if you are unable to install MindsDB on your device. If you are not registered yet, feel free to follow the below guide. If you have already registered, skip the next steps to connect your database.","title":"Running a Ngrok Tunnel"},{"location":"sql/tutorials/diabetes/#mindsdb-gui-cloud-mysql-editor-mysql-cli","text":"The next step will be to connect your database to MindsDB. You can visit this link and follow the steps to create a MindsDB Cloud account. Access the MySQL Editor here . If you are using MindsDB through Docker, you can run the command below to start MindsDB: docker run -p 47334 :47334 -p 47335 :47335 mindsdb/mindsdb Once you are connected,head over to your browser and in the browser tab type in the host number 127.0.0.1:47334 to access the local MindsDB GUI.","title":"MindsDB GUI- Cloud MySQL Editor &amp; MySQL CLI"},{"location":"sql/tutorials/diabetes/#establish-connection-between-your-database-and-mindsdb-gui","text":"Once you have accessed the GUI, you will be able to select a database type and enter the required parameters: - On the user interface,select the plug icon on the left sidebar. - A page will populate with different datasources to select from. For this tutorial we are choosing PostgreSQL. The page will automatically direct to the SQL editor defaulting the syntax parameters required to create a connection with PostgreSQL: CREATE DATABASE : This will be the chosen display name for your database. For this tutorial we will choose airbyte. WITH ENGINE : name of the mindsdb handler,in this example it will be Postgres. PARAMETERS = \"user\": Your database user. for this tutorial it is postgres \"password\": Your password. \"host\":host, it can be an ip or an url. For this tutorial it will be the forwarding address 4.tcp.ngrok.io \"port\": \"13096\",common port is 5432, for this tutorial we will use 15076. \"database\":The name of your database (optional). For this tutorial we will be using postgres. Select the button RUN or select Shift+Enter to execute the query. You are now done with connecting MindsDB to your database! \ud83d\ude80","title":"Establish connection between your database and MindsDB GUI"},{"location":"sql/tutorials/diabetes/#create-a-predictor","text":"Now we are ready to create our own predictor! We will start by using the MySQL API to connect to MindsDB and with a single SQL command create a predictor. The predictor we will create will be trained to determine negative and positive cases for diabetes. Use the following query to create a predictor that will predict the Class ( positive or negative ) for the specific field parameters. CREATE MODEL diabetes_predictor FROM mindsdb_prediction ( SELECT * FROM diabetes ) PREDICT class ; Select the RUN button,alternatively select Shift+Enter to execute the query. You will receive the message 'Query successfully completed' if the machine learning model is successfully created. The predictor was created successfully and has started training. To check the status of the model, use the below query. SELECT * FROM mindsdb . models WHERE name = 'diabetes_predictor' ; After the predictor has finished training, you will see a similar output. Note that MindsDB does model testing for you automatically, so you will immediately see if the predictor is accurate enough. The predictor has completed its training, indicated by the status column, and shows the accuracy of the model. You can revisit training new predictors to increase accuracy by changing the query to better suit the dataset i.e. omitting certain columns etc. Good job! We have successfully created and trained a predictive model \u2728","title":"Create a predictor"},{"location":"sql/tutorials/diabetes/#make-predictions","text":"In this section you will learn how to make predictions using your trained model. Now we will use the trained model to make predictions using a SQL query Use the following query using mock data with the predictor. SELECT Class FROM mindsdb . diabetes_predictor WHERE number_of_times_pregnant = 0 AND plasma_glucose_concentration = 135 . 0 AND diastolic_blood_pressure = 65 . 0 AND triceps_skin_fold_thickness = 30 AND two_Hour_serum_insulin = 0 AND body_mass_index = 23 . 5 AND diabetes_pedigree_function = 0 . 366 AND age = 31 ; MindsDB will provide you with results similar to below: The machine learning model predicted the diabetic class to be negative. Viola! We have successfully created and trained a model and made our own prediction. How easy and amazing is MindsDB? \ud83c\udf89 Want to try it out? Sign up for a free MindsDB account Join MindsDB community on Slack and GitHub to ask questions, share and express ideas and thoughts!","title":"Make predictions"},{"location":"sql/tutorials/eeg-forecasting/","text":"Forecasting Eye State from Electroencephalogram Readings with MindsDB \u00b6 Introduction \u00b6 In this tutorial, we'll create and train a machine learning model, or as we call it, an AI Table or a predictor . By querying the model, we'll produce categorical forecasts for a multivariate time series. Make sure you have access to a working MindsDB installation, either locally or at MindsDB Cloud . If you want to learn how to set up your account at MindsDB Cloud, follow this guide . Another way is to set up MindsDB locally using Docker or Python . Let's get started. Data Setup \u00b6 Connecting the Data \u00b6 There are a couple of ways you can get the data to follow through with this tutorial. Connecting as a database Connecting as a file You can connect to a demo database that we've prepared for you. It contains the data used throughout this tutorial (the example_db . demo_data . eeg_eye table). CREATE DATABASE example_db WITH ENGINE = \"postgres\" , PARAMETERS = { \"user\" : \"demo_user\" , \"password\" : \"demo_password\" , \"host\" : \"3.220.66.106\" , \"port\" : \"5432\" , \"database\" : \"demo\" } ; Now you can run queries directly on the demo database. Let's preview the data that we'll use to train our predictor. SELECT * FROM example_db . demo_data . eeg_eye LIMIT 10 ; The dataset we use in this tutorial is the UCI's EEG Eye State dataset. You can download it here in the ARFF format that shoud be converted to the CSV format before uploading it via MindsDB SQL Editor . Follow this guide to find out how to upload a file to MindsDB. Now you can run queries directly on the file as if it were a table. Let's preview the data that we'll use to train our predictor. SELECT * FROM files . eeg_eye LIMIT 10 ; Pay Attention to the Queries From now on, we'll use the files . eeg_eye file as a table. Make sure you replace it with example_db.demo_data.eeg_eye if you connect the data as a database. Understanding the Data \u00b6 We use the UCI's EEG Eye State dataset, where each row contains data of one electroencephalogram (EEG) reading plus the current state of the patient's eye, where 0 indicates open eye and 1 indicates closed eye. We want to know ahead of time when the eye state will change, so we predict the eyeDetection column. Below is the sample data stored in the files . eeg_eye table. + ----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+--------------+ | AF3 | F7 | F3 | FC5 | T7 | P7 | O1 | O2 | P8 | T8 | FC6 | F4 | F8 | AF4 | eyeDetection | + ----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+--------------+ | 4329 . 23 | 4009 . 23 | 4289 . 23 | 4148 . 21 | 4350 . 26 | 4586 . 15 | 4096 . 92 | 4641 . 03 | 4222 . 05 | 4238 . 46 | 4211 . 28 | 4280 . 51 | 4635 . 9 | 4393 . 85 | 0 | | 4324 . 62 | 4004 . 62 | 4293 . 85 | 4148 . 72 | 4342 . 05 | 4586 . 67 | 4097 . 44 | 4638 . 97 | 4210 . 77 | 4226 . 67 | 4207 . 69 | 4279 . 49 | 4632 . 82 | 4384 . 1 | 0 | | 4327 . 69 | 4006 . 67 | 4295 . 38 | 4156 . 41 | 4336 . 92 | 4583 . 59 | 4096 . 92 | 4630 . 26 | 4207 . 69 | 4222 . 05 | 4206 . 67 | 4282 . 05 | 4628 . 72 | 4389 . 23 | 0 | | 4328 . 72 | 4011 . 79 | 4296 . 41 | 4155 . 9 | 4343 . 59 | 4582 . 56 | 4097 . 44 | 4630 . 77 | 4217 . 44 | 4235 . 38 | 4210 . 77 | 4287 . 69 | 4632 . 31 | 4396 . 41 | 0 | | 4326 . 15 | 4011 . 79 | 4292 . 31 | 4151 . 28 | 4347 . 69 | 4586 . 67 | 4095 . 9 | 4627 . 69 | 4210 . 77 | 4244 . 1 | 4212 . 82 | 4288 . 21 | 4632 . 82 | 4398 . 46 | 0 | + ----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+--------------+ Where: Column Description Data Type Usage AF3 , F7 , F3 , FC5 , T7 , P7 , O1 , O2 , P8 , T8 , FC6 , F4 , F8 , AF4 The EEG measurement data. float Feature eyeDetectin The state of the patient's eye where 0 indicates open eye and 1 indicates closed eye. binary Label Labels and Features A label is a column whose values will be predicted (the y variable in simple linear regression). A feature is a column used to train the model (the x variable in simple linear regression). Training a Predictor \u00b6 Let's create and train the machine learning model. For that, we use the CREATE MODEL statement and specify the input columns used to train FROM (features) and what we want to PREDICT (labels). The eyeDetection column is our target variable. The interesting thing about this example is that we aim to forecast labels that are not strictly numerical. Even though this example is simple (because the variable is a binary category), this can easily be generalized to more than two categories. We order the measurements by the Timestamps column that shows readings frequency of approximately 8 miliseconds. CREATE MODEL mindsdb . eeg_eye_forecast FROM files ( SELECT * FROM eeg_eye ) PREDICT eyeDetection ORDER BY Timestamps WINDOW 50 HORIZON 10 ; As the sampling frequency is 8 ms, this predictor is trained using a historical context of roughly 400 ms ( (50 * 8) = 400 [ms] ) to predict the following 80 ms ( (10 * 8) = 80 [ms] ). Status of a Predictor \u00b6 A predictor may take a couple of minutes for the training to complete. You can monitor the status of the predictor by using this SQL command: SELECT status FROM mindsdb . models WHERE name = 'eeg_eye_forecast' ; If we run it right after creating a predictor, we get this output: + ------------+ | status | + ------------+ | generating | + ------------+ A bit later, this is the output: + ----------+ | status | + ----------+ | training | + ----------+ And at last, this should be the output: + ----------+ | status | + ----------+ | complete | + ----------+ Now, if the status of our predictor says complete , we can start making predictions! Making Predictions \u00b6 You can make predictions by querying the predictor joined with the data table. The SELECT statement lets you make predictions for the label based on the chosen features for a given time period. Usually, you want to know what happens right after the latest training data point that was fed. We have a special keyword for that, the LATEST keyword. Let's run a query to get predictions for the next HORIZON timesteps into the future, which in this case is roughly 80 miliseconds. SELECT m . Timestamps as timestamps , m . eyeDetection as eye_status FROM files . eeg_eye as t JOIN mindsdb . eeg_eye_forecast as m WHERE t . timestamps > LATEST LIMIT 10 ; On execution, we get: That's it. We can now JOIN any set of WINDOW rows worth of measurements with this predictor, and forecasts will be emitted to help us expect a change in the state of the patient's eye based on the EEG readings. Alternate Problem Framings \u00b6 It is also possible to reframe this task as a normal forecasting scenario where the variable is numeric. There are a few options here. It boils down to what the broader scenario is and what format would maximize the value of any specific prediction. For example, a simple mapping of eye is open to 0 and eye is closed to 1 would be enough to replicate the above behavior. We could also explore other options. With some data transformations on the data layer, we could get a countdown to the next change in state, effectively predicting a date if we cast this back into the timestamp domain. What's Next? \u00b6 Have fun while trying it out yourself! Bookmark MindsDB repository on GitHub . Sign up for a free MindsDB account . Engage with the MindsDB community on Slack or GitHub to ask questions and share your ideas and thoughts. If this tutorial was helpful, please give us a GitHub star here .","title":"Brain Activity"},{"location":"sql/tutorials/eeg-forecasting/#forecasting-eye-state-from-electroencephalogram-readings-with-mindsdb","text":"","title":"Forecasting Eye State from Electroencephalogram Readings with MindsDB"},{"location":"sql/tutorials/eeg-forecasting/#introduction","text":"In this tutorial, we'll create and train a machine learning model, or as we call it, an AI Table or a predictor . By querying the model, we'll produce categorical forecasts for a multivariate time series. Make sure you have access to a working MindsDB installation, either locally or at MindsDB Cloud . If you want to learn how to set up your account at MindsDB Cloud, follow this guide . Another way is to set up MindsDB locally using Docker or Python . Let's get started.","title":"Introduction"},{"location":"sql/tutorials/eeg-forecasting/#data-setup","text":"","title":"Data Setup"},{"location":"sql/tutorials/eeg-forecasting/#connecting-the-data","text":"There are a couple of ways you can get the data to follow through with this tutorial. Connecting as a database Connecting as a file You can connect to a demo database that we've prepared for you. It contains the data used throughout this tutorial (the example_db . demo_data . eeg_eye table). CREATE DATABASE example_db WITH ENGINE = \"postgres\" , PARAMETERS = { \"user\" : \"demo_user\" , \"password\" : \"demo_password\" , \"host\" : \"3.220.66.106\" , \"port\" : \"5432\" , \"database\" : \"demo\" } ; Now you can run queries directly on the demo database. Let's preview the data that we'll use to train our predictor. SELECT * FROM example_db . demo_data . eeg_eye LIMIT 10 ; The dataset we use in this tutorial is the UCI's EEG Eye State dataset. You can download it here in the ARFF format that shoud be converted to the CSV format before uploading it via MindsDB SQL Editor . Follow this guide to find out how to upload a file to MindsDB. Now you can run queries directly on the file as if it were a table. Let's preview the data that we'll use to train our predictor. SELECT * FROM files . eeg_eye LIMIT 10 ; Pay Attention to the Queries From now on, we'll use the files . eeg_eye file as a table. Make sure you replace it with example_db.demo_data.eeg_eye if you connect the data as a database.","title":"Connecting the Data"},{"location":"sql/tutorials/eeg-forecasting/#understanding-the-data","text":"We use the UCI's EEG Eye State dataset, where each row contains data of one electroencephalogram (EEG) reading plus the current state of the patient's eye, where 0 indicates open eye and 1 indicates closed eye. We want to know ahead of time when the eye state will change, so we predict the eyeDetection column. Below is the sample data stored in the files . eeg_eye table. + ----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+--------------+ | AF3 | F7 | F3 | FC5 | T7 | P7 | O1 | O2 | P8 | T8 | FC6 | F4 | F8 | AF4 | eyeDetection | + ----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+--------------+ | 4329 . 23 | 4009 . 23 | 4289 . 23 | 4148 . 21 | 4350 . 26 | 4586 . 15 | 4096 . 92 | 4641 . 03 | 4222 . 05 | 4238 . 46 | 4211 . 28 | 4280 . 51 | 4635 . 9 | 4393 . 85 | 0 | | 4324 . 62 | 4004 . 62 | 4293 . 85 | 4148 . 72 | 4342 . 05 | 4586 . 67 | 4097 . 44 | 4638 . 97 | 4210 . 77 | 4226 . 67 | 4207 . 69 | 4279 . 49 | 4632 . 82 | 4384 . 1 | 0 | | 4327 . 69 | 4006 . 67 | 4295 . 38 | 4156 . 41 | 4336 . 92 | 4583 . 59 | 4096 . 92 | 4630 . 26 | 4207 . 69 | 4222 . 05 | 4206 . 67 | 4282 . 05 | 4628 . 72 | 4389 . 23 | 0 | | 4328 . 72 | 4011 . 79 | 4296 . 41 | 4155 . 9 | 4343 . 59 | 4582 . 56 | 4097 . 44 | 4630 . 77 | 4217 . 44 | 4235 . 38 | 4210 . 77 | 4287 . 69 | 4632 . 31 | 4396 . 41 | 0 | | 4326 . 15 | 4011 . 79 | 4292 . 31 | 4151 . 28 | 4347 . 69 | 4586 . 67 | 4095 . 9 | 4627 . 69 | 4210 . 77 | 4244 . 1 | 4212 . 82 | 4288 . 21 | 4632 . 82 | 4398 . 46 | 0 | + ----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+--------------+ Where: Column Description Data Type Usage AF3 , F7 , F3 , FC5 , T7 , P7 , O1 , O2 , P8 , T8 , FC6 , F4 , F8 , AF4 The EEG measurement data. float Feature eyeDetectin The state of the patient's eye where 0 indicates open eye and 1 indicates closed eye. binary Label Labels and Features A label is a column whose values will be predicted (the y variable in simple linear regression). A feature is a column used to train the model (the x variable in simple linear regression).","title":"Understanding the Data"},{"location":"sql/tutorials/eeg-forecasting/#training-a-predictor","text":"Let's create and train the machine learning model. For that, we use the CREATE MODEL statement and specify the input columns used to train FROM (features) and what we want to PREDICT (labels). The eyeDetection column is our target variable. The interesting thing about this example is that we aim to forecast labels that are not strictly numerical. Even though this example is simple (because the variable is a binary category), this can easily be generalized to more than two categories. We order the measurements by the Timestamps column that shows readings frequency of approximately 8 miliseconds. CREATE MODEL mindsdb . eeg_eye_forecast FROM files ( SELECT * FROM eeg_eye ) PREDICT eyeDetection ORDER BY Timestamps WINDOW 50 HORIZON 10 ; As the sampling frequency is 8 ms, this predictor is trained using a historical context of roughly 400 ms ( (50 * 8) = 400 [ms] ) to predict the following 80 ms ( (10 * 8) = 80 [ms] ).","title":"Training a Predictor"},{"location":"sql/tutorials/eeg-forecasting/#status-of-a-predictor","text":"A predictor may take a couple of minutes for the training to complete. You can monitor the status of the predictor by using this SQL command: SELECT status FROM mindsdb . models WHERE name = 'eeg_eye_forecast' ; If we run it right after creating a predictor, we get this output: + ------------+ | status | + ------------+ | generating | + ------------+ A bit later, this is the output: + ----------+ | status | + ----------+ | training | + ----------+ And at last, this should be the output: + ----------+ | status | + ----------+ | complete | + ----------+ Now, if the status of our predictor says complete , we can start making predictions!","title":"Status of a Predictor"},{"location":"sql/tutorials/eeg-forecasting/#making-predictions","text":"You can make predictions by querying the predictor joined with the data table. The SELECT statement lets you make predictions for the label based on the chosen features for a given time period. Usually, you want to know what happens right after the latest training data point that was fed. We have a special keyword for that, the LATEST keyword. Let's run a query to get predictions for the next HORIZON timesteps into the future, which in this case is roughly 80 miliseconds. SELECT m . Timestamps as timestamps , m . eyeDetection as eye_status FROM files . eeg_eye as t JOIN mindsdb . eeg_eye_forecast as m WHERE t . timestamps > LATEST LIMIT 10 ; On execution, we get: That's it. We can now JOIN any set of WINDOW rows worth of measurements with this predictor, and forecasts will be emitted to help us expect a change in the state of the patient's eye based on the EEG readings.","title":"Making Predictions"},{"location":"sql/tutorials/eeg-forecasting/#alternate-problem-framings","text":"It is also possible to reframe this task as a normal forecasting scenario where the variable is numeric. There are a few options here. It boils down to what the broader scenario is and what format would maximize the value of any specific prediction. For example, a simple mapping of eye is open to 0 and eye is closed to 1 would be enough to replicate the above behavior. We could also explore other options. With some data transformations on the data layer, we could get a countdown to the next change in state, effectively predicting a date if we cast this back into the timestamp domain.","title":"Alternate Problem Framings"},{"location":"sql/tutorials/eeg-forecasting/#whats-next","text":"Have fun while trying it out yourself! Bookmark MindsDB repository on GitHub . Sign up for a free MindsDB account . Engage with the MindsDB community on Slack or GitHub to ask questions and share your ideas and thoughts. If this tutorial was helpful, please give us a GitHub star here .","title":"What's Next?"},{"location":"sql/tutorials/heart-disease/","text":"Dataset: Heart Disease Data Communtiy Author: Mohd Talha Cardiovascular disease remains the leading cause of morbidity and mortality according to the National Center for Health Statistics in the United States, and consequently, early diagnosis is of paramount importance. Machine learning technology, a subfield of artificial intelligence, is enabling scientists, clinicians and patients to detect it in the earlier stages and therefore save lives. Until now, building, deploying and maintaining applied machine learning solutions was a complicated and expensive task, because it required skilled personnel and expensive tools. But not only that. A traditional machine learning project requires building integrations with data and applications, that is not only a technical but also an organizational challenge. So what if machine learning can become a part of the standard tools that are already in use? This is exactly the problem that MindsDB is solving. It makes machine learning easy to use by automating and integrating it into the mainstream instruments for data and analytics, namely databases and business intelligence software. It adds an AI \u201cbrain\u201d to databases so that they can learn automatically from existing data, allowing you to generate and visualize predictions using standard data query language like SQL. Lastly, MindsDB is open-source, and anyone can use it for free. In this article, we will show step by step how to use MindsDB inside databases to predict the risk of heart disease for patients. You can follow this tutorial by connecting to your own database and using different data - the same workflow applies to most machine learning use cases. Let\u2019s get started! Pre-requisites \u00b6 If you want to install MindsDB locally, check out the installation guide for Docker or PyPi and you can follow this tutorial. If you are OK with using MindsDB cloud, then simply create a free account and you will be up and running in just one minute. Second, you will need to have a mysql client like DBeaver, MySQL Workbench etc. installed locally to connect to the MindsDB MySQL API. MindsDB contains a SQL Editor which can be accessed on MindsDB cloud or the URL 127.0.0.1:47334/. Data Overview \u00b6 For the example of this tutorial, we will use the heart disease dataset available publicly in Kaggle . Each row represents a patient and we will train a machine learning model to help us predict if the patient is classified as a heart disease patient. Below is a short description of each feature inside the data. age - In Years sex - 1 = Male; 0 = Female cp - chest pain type (4 values) trestbps - Resting blood pressure (in mm Hg on admission to the hospital) chol - Serum cholesterol in mg/dl fbs - Fasting blood sugar > 120 mg/dl (1 = true; 0 = false) restecg - Resting electrocardiographic results thalach - Maximum heart rate achieved exang - Exercise induced angina (1 = yes; 0 = no) oldpeak - ST depression induced by exercise relative to rest slope - the slope of the peak exercise ST segment ca - Number of major vessels (0-3) colored by fluoroscopy thal - 1 = normal; 2 = fixed defect; 3 = reversible defect target - 1 or 0 (This is what we will predict) How to use MindsDB \u00b6 MindsDB allows you to automatically create & train machine learning models from the data in your database that you have connected to in the previous step. MindsDB works via MySQL wire protocol, which means you can do all these steps through SQL commands. When it comes to making predictions, SQL queries become even handier, because you can easily make them straight from your existing applications or Business Intelligence tools that already speak SQL. The ML models are available to use immediately after being trained as if they were virtual database tables (a concept called \u201cAI Tables\u201d). So, let\u2019s see how it works. Connect to your data \u00b6 First, we need to connect MindsDB to the database where the Heart Disease data is stored: - Access MindsDB GUI on either cloud or local via the URL 127.0.0.1:47334/ - On the default page, select the button Add Data or alternatively select the plug icon on the left sidebar. - The 'Select your data source' page will populate for you to choose your database type. For this tutorial we will be selecting the postgres database button. Once you have selected the database type,the page will automatically navigate to the SQL Editor where the syntax to create a database connection will automatically populate for you to enter the required parameters. The required parameters are: CREATE DATABASE display_name --- display name for database. WITH ENGINE = \"postgres\", --- name of the mindsdb handler PARAMETERS = { \"user\": \" \", --- Your database user. \"password\": \" \", --- Your password. \"host\": \" \", --- host, it can be an ip or an url. \"port\": \"5432\", --- common port is 5432. \"database\": \" \" --- The name of your database *optional.} Select the Run button or Shift+Enter to execute the syntax. Once the Database connection is created the console will display a message 'Query successfully completed'. Create a machine learning model. \u00b6 We can create a machine learning predictive model by using simple SQL statements executed in the SQL Editor. To create and train a new machine learning model we will need to use the CREATE MODEL statement: CREATE MODEL mindsdb . predictor_name FROM integration_name ( SELECT column_name , column_name2 FROM table_name ) PREDICT column_name ; The required values that we need to provide are: predictor_name (string): The name of the model integration_name (string): The name of the connection to your database. column_name (string): The feature you want to predict. To train the model that will predict the risk of heart disease as target we will run: CREATE MODEL patients_target FROM mindsdb_predictions ( SELECT * FROM heart_disease ) PREDICT target ; Select the Run button or Shift+Enter to execute the syntax. Once the machine learning model is created the console will display a message 'Query successfully completed'. What we did here was to create a predictor called patients_target to predict the presence of heart disease as target . The model has started training. To check if the training has finished you can SELECT the model name from the predictors table: SELECT * FROM mindsdb . models WHERE name = 'patients_target' ; The complete status means that the model training has successfully finished. Using SQL Statements to make predictions \u00b6 The next steps would be to query the model and predict the heart disease risk. Let\u2019s imagine a patient. This patient\u2019s age is 30, she has a cholesterol level of 177 mg/dl, with slope of the peak exercise ST segment as 2, and thal as 2. Add all of this information to the WHERE clause. SELECT target AS prediction , target_confidence AS confidence , target_explain AS info FROM mindsdb . patients_target WHERE age = 30 AND chol = 177 AND slope = 2 AND thal = 2 ; With a confidence of around 83%, MindsDB predicted a high risk of heart disease for this patient. The above example shows how you can make predictions for a single patient. But what if you have a table in your database with many patients\u2019 diagnosis data, and you want to make predictions for them in bulk? For this purpose, you can join the predictor with such a table. SELECT * FROM mindsdb_predictions . heart_disease AS t JOIN mindsdb . patients_target AS tb WHERE t . thal = 2 ; Now you can even connect the output table to your BI tool and for more convenient visualization of the results using graphs or pivots. Conclusion \u00b6 In this tutorial, you have seen how easy it is to apply machine learning for your predictive needs. MindsDB's innovative open-source technology is making it easy to leverage machine learning for people who are not experts in this field. However, MindsDB is a great tool for ML practitioners as well: if you are a skilled data scientist, you could also benefit from the convenience of deploying custom machine learning solutions within databases by building & configuring models manually through a declarative syntax called JSON-AI . There are other interesting ML use cases where MindsDB is positioned extremely well, like multivariate time-series and real-time data streams, so feel free to check it yourself .","title":"Heart Disease"},{"location":"sql/tutorials/heart-disease/#pre-requisites","text":"If you want to install MindsDB locally, check out the installation guide for Docker or PyPi and you can follow this tutorial. If you are OK with using MindsDB cloud, then simply create a free account and you will be up and running in just one minute. Second, you will need to have a mysql client like DBeaver, MySQL Workbench etc. installed locally to connect to the MindsDB MySQL API. MindsDB contains a SQL Editor which can be accessed on MindsDB cloud or the URL 127.0.0.1:47334/.","title":"Pre-requisites"},{"location":"sql/tutorials/heart-disease/#data-overview","text":"For the example of this tutorial, we will use the heart disease dataset available publicly in Kaggle . Each row represents a patient and we will train a machine learning model to help us predict if the patient is classified as a heart disease patient. Below is a short description of each feature inside the data. age - In Years sex - 1 = Male; 0 = Female cp - chest pain type (4 values) trestbps - Resting blood pressure (in mm Hg on admission to the hospital) chol - Serum cholesterol in mg/dl fbs - Fasting blood sugar > 120 mg/dl (1 = true; 0 = false) restecg - Resting electrocardiographic results thalach - Maximum heart rate achieved exang - Exercise induced angina (1 = yes; 0 = no) oldpeak - ST depression induced by exercise relative to rest slope - the slope of the peak exercise ST segment ca - Number of major vessels (0-3) colored by fluoroscopy thal - 1 = normal; 2 = fixed defect; 3 = reversible defect target - 1 or 0 (This is what we will predict)","title":"Data Overview"},{"location":"sql/tutorials/heart-disease/#how-to-use-mindsdb","text":"MindsDB allows you to automatically create & train machine learning models from the data in your database that you have connected to in the previous step. MindsDB works via MySQL wire protocol, which means you can do all these steps through SQL commands. When it comes to making predictions, SQL queries become even handier, because you can easily make them straight from your existing applications or Business Intelligence tools that already speak SQL. The ML models are available to use immediately after being trained as if they were virtual database tables (a concept called \u201cAI Tables\u201d). So, let\u2019s see how it works.","title":"How to use MindsDB"},{"location":"sql/tutorials/heart-disease/#connect-to-your-data","text":"First, we need to connect MindsDB to the database where the Heart Disease data is stored: - Access MindsDB GUI on either cloud or local via the URL 127.0.0.1:47334/ - On the default page, select the button Add Data or alternatively select the plug icon on the left sidebar. - The 'Select your data source' page will populate for you to choose your database type. For this tutorial we will be selecting the postgres database button. Once you have selected the database type,the page will automatically navigate to the SQL Editor where the syntax to create a database connection will automatically populate for you to enter the required parameters. The required parameters are: CREATE DATABASE display_name --- display name for database. WITH ENGINE = \"postgres\", --- name of the mindsdb handler PARAMETERS = { \"user\": \" \", --- Your database user. \"password\": \" \", --- Your password. \"host\": \" \", --- host, it can be an ip or an url. \"port\": \"5432\", --- common port is 5432. \"database\": \" \" --- The name of your database *optional.} Select the Run button or Shift+Enter to execute the syntax. Once the Database connection is created the console will display a message 'Query successfully completed'.","title":"Connect to your data"},{"location":"sql/tutorials/heart-disease/#create-a-machine-learning-model","text":"We can create a machine learning predictive model by using simple SQL statements executed in the SQL Editor. To create and train a new machine learning model we will need to use the CREATE MODEL statement: CREATE MODEL mindsdb . predictor_name FROM integration_name ( SELECT column_name , column_name2 FROM table_name ) PREDICT column_name ; The required values that we need to provide are: predictor_name (string): The name of the model integration_name (string): The name of the connection to your database. column_name (string): The feature you want to predict. To train the model that will predict the risk of heart disease as target we will run: CREATE MODEL patients_target FROM mindsdb_predictions ( SELECT * FROM heart_disease ) PREDICT target ; Select the Run button or Shift+Enter to execute the syntax. Once the machine learning model is created the console will display a message 'Query successfully completed'. What we did here was to create a predictor called patients_target to predict the presence of heart disease as target . The model has started training. To check if the training has finished you can SELECT the model name from the predictors table: SELECT * FROM mindsdb . models WHERE name = 'patients_target' ; The complete status means that the model training has successfully finished.","title":"Create a machine learning model."},{"location":"sql/tutorials/heart-disease/#using-sql-statements-to-make-predictions","text":"The next steps would be to query the model and predict the heart disease risk. Let\u2019s imagine a patient. This patient\u2019s age is 30, she has a cholesterol level of 177 mg/dl, with slope of the peak exercise ST segment as 2, and thal as 2. Add all of this information to the WHERE clause. SELECT target AS prediction , target_confidence AS confidence , target_explain AS info FROM mindsdb . patients_target WHERE age = 30 AND chol = 177 AND slope = 2 AND thal = 2 ; With a confidence of around 83%, MindsDB predicted a high risk of heart disease for this patient. The above example shows how you can make predictions for a single patient. But what if you have a table in your database with many patients\u2019 diagnosis data, and you want to make predictions for them in bulk? For this purpose, you can join the predictor with such a table. SELECT * FROM mindsdb_predictions . heart_disease AS t JOIN mindsdb . patients_target AS tb WHERE t . thal = 2 ; Now you can even connect the output table to your BI tool and for more convenient visualization of the results using graphs or pivots.","title":"Using SQL Statements to make predictions"},{"location":"sql/tutorials/heart-disease/#conclusion","text":"In this tutorial, you have seen how easy it is to apply machine learning for your predictive needs. MindsDB's innovative open-source technology is making it easy to leverage machine learning for people who are not experts in this field. However, MindsDB is a great tool for ML practitioners as well: if you are a skilled data scientist, you could also benefit from the convenience of deploying custom machine learning solutions within databases by building & configuring models manually through a declarative syntax called JSON-AI . There are other interesting ML use cases where MindsDB is positioned extremely well, like multivariate time-series and real-time data streams, so feel free to check it yourself .","title":"Conclusion"},{"location":"sql/tutorials/home-rentals/","text":"Predicting Home Rental Prices with MindsDB \u00b6 Introduction \u00b6 In this tutorial, we'll create and train a machine learning model, or as we call it, an AI Table or a predictor . By querying the model, we'll predict the rental prices of the properties based on their attributes, such as the number of rooms, area, or neighborhood. Make sure you have access to a working MindsDB installation, either locally or at MindsDB Cloud . If you want to learn how to set up your account at MindsDB Cloud, follow this guide . Another way is to set up MindsDB locally using Docker or Python . Let's get started. Data Setup \u00b6 Connecting the Data \u00b6 There are a couple of ways you can get the data to follow through with this tutorial. Connecting as a database Connecting as a file You can connect to a demo database that we've prepared for you. It contains the data used throughout this tutorial (the example_db . demo_data . home_rentals table). CREATE DATABASE example_db WITH ENGINE = \"postgres\" , PARAMETERS = { \"user\" : \"demo_user\" , \"password\" : \"demo_password\" , \"host\" : \"3.220.66.106\" , \"port\" : \"5432\" , \"database\" : \"demo\" } ; Now you can run queries directly on the demo database. Let's preview the data that we'll use to train our predictor. SELECT * FROM example_db . demo_data . home_rentals LIMIT 10 ; You can download the CSV data file here and upload it via MindsDB SQL Editor . Follow this guide to find out how to upload a file to MindsDB. Now you can run queries directly on the file as if it were a table. Let's preview the data that we'll use to train our predictor. SELECT * FROM files . home_rentals LIMIT 10 ; Pay Attention to the Queries From now on, we'll use the example_db . demo_data . home_rentals table. Make sure you replace it with files.home_rentals if you connect the data as a file. Understanding the Data \u00b6 We use the home rentals dataset, where each row is one property, to predict the rental_price column value for all the newly added properties. Below is the sample data stored in the example_db . demo_data . home_rentals table. + -----------------+---------------------+------+----------+----------------+----------------+--------------+ | number_of_rooms | number_of_bathrooms | sqft | location | days_on_market | neighborhood | rental_price | + -----------------+---------------------+------+----------+----------------+----------------+--------------+ | 2 | 1 | 917 | great | 13 | berkeley_hills | 3901 | | 0 | 1 | 194 | great | 10 | berkeley_hills | 2042 | | 1 | 1 | 543 | poor | 18 | westbrae | 1871 | | 2 | 1 | 503 | good | 10 | downtown | 3026 | | 3 | 2 | 1066 | good | 13 | thowsand_oaks | 4774 | + -----------------+---------------------+------+----------+----------------+----------------+--------------+ Where: Column Description Data Type Usage number_of_rooms Number of rooms in a property [0,1,2,3] . integer Feature number_of_bathrooms Number of bathrooms in a property [1,2] . integer Feature sqft Area of a property in square feet. integer Feature location Rating of the location of a property [poor, great, good] . character varying Feature days_on_market Number of days a property has been on the market. integer Feature neighborhood Neighborhood [alcatraz_ave, westbrae, ..., south_side, thowsand_oaks] . character varying Feature rental_price Rental price of a property in USD. integer Label Labels and Features A label is a column whose values will be predicted (the y variable in simple linear regression). A feature is a column used to train the model (the x variable in simple linear regression). Training a Predictor \u00b6 Let's create and train the machine learning model. For that, we use the CREATE MODEL statement and specify the input columns used to train FROM (features) and what we want to PREDICT (labels). CREATE MODEL mindsdb . home_rentals_model FROM example_db ( SELECT * FROM demo_data . home_rentals ) PREDICT rental_price ; We use all of the columns as features, except for the rental_price column, whose values will be predicted. Status of a Predictor \u00b6 A predictor may take a couple of minutes for the training to complete. You can monitor the status of the predictor by using this SQL command: SELECT status FROM mindsdb . models WHERE name = 'home_rentals_model' ; If we run it right after creating a predictor, we get this output: + ------------+ | status | + ------------+ | generating | + ------------+ A bit later, this is the output: + ----------+ | status | + ----------+ | training | + ----------+ And at last, this should be the output: + ----------+ | status | + ----------+ | complete | + ----------+ Now, if the status of our predictor says complete , we can start making predictions! Making Predictions \u00b6 Making a Single Prediction \u00b6 You can make predictions by querying the predictor as if it were a table. The SELECT statement lets you make predictions for the label based on the chosen features. SELECT rental_price , rental_price_explain FROM mindsdb . home_rentals_model WHERE sqft = 823 AND location = 'good' AND neighborhood = 'downtown' AND days_on_market = 10 ; On execution, we get: + --------------+-----------------------------------------------------------------------------------------------------------------------------------------------+ | rental_price | rental_price_explain | + --------------+-----------------------------------------------------------------------------------------------------------------------------------------------+ | 4394 | { \"predicted_value\" : 4394 , \"confidence\" : 0 . 99 , \"anomaly\" : null , \"truth\" : null , \"confidence_lower_bound\" : 4313 , \"confidence_upper_bound\" : 4475 } | + --------------+-----------------------------------------------------------------------------------------------------------------------------------------------+ Making Batch Predictions \u00b6 Also, you can make bulk predictions by joining a data table with your predictor using JOIN . SELECT t . rental_price AS real_price , m . rental_price AS predicted_price , t . number_of_rooms , t . number_of_bathrooms , t . sqft , t . location , t . days_on_market FROM example_db . demo_data . home_rentals AS t JOIN mindsdb . home_rentals_model AS m LIMIT 100 ; On execution, we get: + ------------+-----------------+-----------------+---------------------+------+----------+----------------+ | real_price | predicted_price | number_of_rooms | number_of_bathrooms | sqft | location | days_on_market | + ------------+-----------------+-----------------+---------------------+------+----------+----------------+ | 3901 | 3886 | 2 | 1 | 917 | great | 13 | | 2042 | 2007 | 0 | 1 | 194 | great | 10 | | 1871 | 1865 | 1 | 1 | 543 | poor | 18 | | 3026 | 3020 | 2 | 1 | 503 | good | 10 | | 4774 | 4748 | 3 | 2 | 1066 | good | 13 | + ------------+-----------------+-----------------+---------------------+------+----------+----------------+ What's Next? \u00b6 Have fun while trying it out yourself! Bookmark MindsDB repository on GitHub . Sign up for a free MindsDB account . Engage with the MindsDB community on Slack or GitHub to ask questions and share your ideas and thoughts. If this tutorial was helpful, please give us a GitHub star here .","title":"Home Rentals"},{"location":"sql/tutorials/home-rentals/#predicting-home-rental-prices-with-mindsdb","text":"","title":"Predicting Home Rental Prices with MindsDB"},{"location":"sql/tutorials/home-rentals/#introduction","text":"In this tutorial, we'll create and train a machine learning model, or as we call it, an AI Table or a predictor . By querying the model, we'll predict the rental prices of the properties based on their attributes, such as the number of rooms, area, or neighborhood. Make sure you have access to a working MindsDB installation, either locally or at MindsDB Cloud . If you want to learn how to set up your account at MindsDB Cloud, follow this guide . Another way is to set up MindsDB locally using Docker or Python . Let's get started.","title":"Introduction"},{"location":"sql/tutorials/home-rentals/#data-setup","text":"","title":"Data Setup"},{"location":"sql/tutorials/home-rentals/#connecting-the-data","text":"There are a couple of ways you can get the data to follow through with this tutorial. Connecting as a database Connecting as a file You can connect to a demo database that we've prepared for you. It contains the data used throughout this tutorial (the example_db . demo_data . home_rentals table). CREATE DATABASE example_db WITH ENGINE = \"postgres\" , PARAMETERS = { \"user\" : \"demo_user\" , \"password\" : \"demo_password\" , \"host\" : \"3.220.66.106\" , \"port\" : \"5432\" , \"database\" : \"demo\" } ; Now you can run queries directly on the demo database. Let's preview the data that we'll use to train our predictor. SELECT * FROM example_db . demo_data . home_rentals LIMIT 10 ; You can download the CSV data file here and upload it via MindsDB SQL Editor . Follow this guide to find out how to upload a file to MindsDB. Now you can run queries directly on the file as if it were a table. Let's preview the data that we'll use to train our predictor. SELECT * FROM files . home_rentals LIMIT 10 ; Pay Attention to the Queries From now on, we'll use the example_db . demo_data . home_rentals table. Make sure you replace it with files.home_rentals if you connect the data as a file.","title":"Connecting the Data"},{"location":"sql/tutorials/home-rentals/#understanding-the-data","text":"We use the home rentals dataset, where each row is one property, to predict the rental_price column value for all the newly added properties. Below is the sample data stored in the example_db . demo_data . home_rentals table. + -----------------+---------------------+------+----------+----------------+----------------+--------------+ | number_of_rooms | number_of_bathrooms | sqft | location | days_on_market | neighborhood | rental_price | + -----------------+---------------------+------+----------+----------------+----------------+--------------+ | 2 | 1 | 917 | great | 13 | berkeley_hills | 3901 | | 0 | 1 | 194 | great | 10 | berkeley_hills | 2042 | | 1 | 1 | 543 | poor | 18 | westbrae | 1871 | | 2 | 1 | 503 | good | 10 | downtown | 3026 | | 3 | 2 | 1066 | good | 13 | thowsand_oaks | 4774 | + -----------------+---------------------+------+----------+----------------+----------------+--------------+ Where: Column Description Data Type Usage number_of_rooms Number of rooms in a property [0,1,2,3] . integer Feature number_of_bathrooms Number of bathrooms in a property [1,2] . integer Feature sqft Area of a property in square feet. integer Feature location Rating of the location of a property [poor, great, good] . character varying Feature days_on_market Number of days a property has been on the market. integer Feature neighborhood Neighborhood [alcatraz_ave, westbrae, ..., south_side, thowsand_oaks] . character varying Feature rental_price Rental price of a property in USD. integer Label Labels and Features A label is a column whose values will be predicted (the y variable in simple linear regression). A feature is a column used to train the model (the x variable in simple linear regression).","title":"Understanding the Data"},{"location":"sql/tutorials/home-rentals/#training-a-predictor","text":"Let's create and train the machine learning model. For that, we use the CREATE MODEL statement and specify the input columns used to train FROM (features) and what we want to PREDICT (labels). CREATE MODEL mindsdb . home_rentals_model FROM example_db ( SELECT * FROM demo_data . home_rentals ) PREDICT rental_price ; We use all of the columns as features, except for the rental_price column, whose values will be predicted.","title":"Training a Predictor"},{"location":"sql/tutorials/home-rentals/#status-of-a-predictor","text":"A predictor may take a couple of minutes for the training to complete. You can monitor the status of the predictor by using this SQL command: SELECT status FROM mindsdb . models WHERE name = 'home_rentals_model' ; If we run it right after creating a predictor, we get this output: + ------------+ | status | + ------------+ | generating | + ------------+ A bit later, this is the output: + ----------+ | status | + ----------+ | training | + ----------+ And at last, this should be the output: + ----------+ | status | + ----------+ | complete | + ----------+ Now, if the status of our predictor says complete , we can start making predictions!","title":"Status of a Predictor"},{"location":"sql/tutorials/home-rentals/#making-predictions","text":"","title":"Making Predictions"},{"location":"sql/tutorials/home-rentals/#making-a-single-prediction","text":"You can make predictions by querying the predictor as if it were a table. The SELECT statement lets you make predictions for the label based on the chosen features. SELECT rental_price , rental_price_explain FROM mindsdb . home_rentals_model WHERE sqft = 823 AND location = 'good' AND neighborhood = 'downtown' AND days_on_market = 10 ; On execution, we get: + --------------+-----------------------------------------------------------------------------------------------------------------------------------------------+ | rental_price | rental_price_explain | + --------------+-----------------------------------------------------------------------------------------------------------------------------------------------+ | 4394 | { \"predicted_value\" : 4394 , \"confidence\" : 0 . 99 , \"anomaly\" : null , \"truth\" : null , \"confidence_lower_bound\" : 4313 , \"confidence_upper_bound\" : 4475 } | + --------------+-----------------------------------------------------------------------------------------------------------------------------------------------+","title":"Making a Single Prediction"},{"location":"sql/tutorials/home-rentals/#making-batch-predictions","text":"Also, you can make bulk predictions by joining a data table with your predictor using JOIN . SELECT t . rental_price AS real_price , m . rental_price AS predicted_price , t . number_of_rooms , t . number_of_bathrooms , t . sqft , t . location , t . days_on_market FROM example_db . demo_data . home_rentals AS t JOIN mindsdb . home_rentals_model AS m LIMIT 100 ; On execution, we get: + ------------+-----------------+-----------------+---------------------+------+----------+----------------+ | real_price | predicted_price | number_of_rooms | number_of_bathrooms | sqft | location | days_on_market | + ------------+-----------------+-----------------+---------------------+------+----------+----------------+ | 3901 | 3886 | 2 | 1 | 917 | great | 13 | | 2042 | 2007 | 0 | 1 | 194 | great | 10 | | 1871 | 1865 | 1 | 1 | 543 | poor | 18 | | 3026 | 3020 | 2 | 1 | 503 | good | 10 | | 4774 | 4748 | 3 | 2 | 1066 | good | 13 | + ------------+-----------------+-----------------+---------------------+------+----------+----------------+","title":"Making Batch Predictions"},{"location":"sql/tutorials/home-rentals/#whats-next","text":"Have fun while trying it out yourself! Bookmark MindsDB repository on GitHub . Sign up for a free MindsDB account . Engage with the MindsDB community on Slack or GitHub to ask questions and share your ideas and thoughts. If this tutorial was helpful, please give us a GitHub star here .","title":"What's Next?"},{"location":"sql/tutorials/house-sales-forecasting/","text":"Forecasting Quarterly House Sales with MindsDB \u00b6 Introduction \u00b6 In this tutorial, we'll create and train a machine learning model, or as we call it, an AI Table or a predictor . By querying the model, we'll predict the real estate sales using a multivariate time series strategy. Make sure you have access to a working MindsDB installation, either locally or at MindsDB Cloud . If you want to learn how to set up your account at MindsDB Cloud, follow this guide . Another way is to set up MindsDB locally using Docker or Python . Let's get started. Data Setup \u00b6 Connecting the Data \u00b6 There are a couple of ways you can get the data to follow through with this tutorial. Connecting as a database Connecting as a file You can connect to a demo database that we've prepared for you. It contains the data used throughout this tutorial (the example_db . demo_data . house_sales table). CREATE DATABASE example_db WITH ENGINE = \"postgres\" , PARAMETERS = { \"user\" : \"demo_user\" , \"password\" : \"demo_password\" , \"host\" : \"3.220.66.106\" , \"port\" : \"5432\" , \"database\" : \"demo\" } ; Now you can run queries directly on the demo database. Let's preview the data that we'll use to train our predictor. SELECT * FROM example_db . demo_data . house_sales LIMIT 10 ; The dataset we use in this tutorial is the pre-processed version of the House Property Sales data. You can download the CSV data file here (we use the ma_lga_12345.csv file) and upload it via MindsDB SQL Editor . Follow this guide to find out how to upload a file to MindsDB. Now you can run queries directly on the file as if it were a table. Let's preview the data that we'll use to train our predictor. SELECT * FROM files . house_sales LIMIT 10 ; Pay Attention to the Queries From now on, we'll use the files . house_sales file as a table. Make sure you replace it with example_db.demo_data.house_sales if you connect the data as a database. Understanding the Data \u00b6 We use the house sales dataset, where each row is one house or one unit, to predict the MA column values. It tracks quarterly moving averages ( MA ) of house sales aggregated by real estate type and the number of bedrooms in each listing. Below is the sample data stored in the files . house_sales table. + ----------+------+-----+--------+ | saledate | MA | type | bedrooms | + ----------+------+-----+--------+ | 30 / 09 / 2007 | 441854 | house | 2 | | 31 / 12 / 2007 | 441854 | house | 2 | | 31 / 03 / 2008 | 441854 | house | 2 | | 30 / 06 / 2016 | 430880 | unit | 2 | | 30 / 09 / 2016 | 430654 | unit | 2 | + ----------+------+-----+--------+ Where: Column Description Data Type Usage saledate The date of sale. date Feature MA Moving average of the historical median price of the house or unit. integer Label type Type of property ( house or unit ). character varying Feature bedrooms Number of bedrooms. integer Feature Labels and Features A label is a column whose values will be predicted (the y variable in simple linear regression). A feature is a column used to train the model (the x variable in simple linear regression). Training a Predictor \u00b6 Let's create and train the machine learning model. For that, we use the CREATE MODEL statement and specify the input columns used to train FROM (features) and what we want to PREDICT (labels). CREATE MODEL mindsdb . house_sales_predictor FROM files ( SELECT * FROM house_sales ) PREDICT MA ORDER BY saledate GROUP BY bedrooms , type -- the target column to be predicted stores one row per quarter WINDOW 8 -- using data from the last two years to make forecasts (last 8 rows) HORIZON 4 ; -- making forecasts for the next year (next 4 rows) We use all of the columns as features, except for the MA column, whose values will be predicted. MindsDB makes it simple so that we don't need to repeat the predictor creation process for every group, that is, for every distinct number of bedrooms or for every distinct type of real estate. Instead, we just group by both the bedrooms and type columns, and the predictor learns from all series and enables forecasts for all of them! Status of a Predictor \u00b6 A predictor may take a couple of minutes for the training to complete. You can monitor the status of the predictor by using this SQL command: SELECT status FROM mindsdb . models WHERE name = 'house_sales_predictor' ; If we run it right after creating a predictor, we get this output: + ------------+ | status | + ------------+ | generating | + ------------+ A bit later, this is the output: + ----------+ | status | + ----------+ | training | + ----------+ And at last, this should be the output: + ----------+ | status | + ----------+ | complete | + ----------+ Now, if the status of our predictor says complete , we can start making predictions! Making Predictions \u00b6 You can make predictions by querying the predictor joined with the data table. The SELECT statement lets you make predictions for the label based on the chosen features for a given time period. Usually, you want to know what happens right after the latest training data point that was fed. We have a special keyword for that, the LATEST keyword. SELECT m . saledate AS date , m . MA AS forecast , MA_explain FROM mindsdb . house_sales_predictor AS m JOIN files . house_sales AS t WHERE t . saledate > LATEST AND t . type = 'house' AND t . bedrooms = 2 LIMIT 4 ; On execution, we get: + -------------+-------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ | date | forecast | MA_explain | + -------------+-------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ | 2019 - 12 - 31 | 441413 . 5849598734 | { \"predicted_value\" : 441413 . 5849598734 , \"confidence\" : 0 . 99 , \"anomaly\" : true , \"truth\" : null , \"confidence_lower_bound\" : 440046 . 28237074096 , \"confidence_upper_bound\" : 442780 . 88754900586 } | | 2020 - 04 - 01 | 443292 . 5194586229 | { \"predicted_value\" : 443292 . 5194586229 , \"confidence\" : 0 . 9991 , \"anomaly\" : null , \"truth\" : null , \"confidence_lower_bound\" : 427609 . 3325864327 , \"confidence_upper_bound\" : 458975 . 7063308131 } | | 2020 - 07 - 02 | 443292 . 5194585953 | { \"predicted_value\" : 443292 . 5194585953 , \"confidence\" : 0 . 9991 , \"anomaly\" : null , \"truth\" : null , \"confidence_lower_bound\" : 424501 . 59192981094 , \"confidence_upper_bound\" : 462083 . 4469873797 } | | 2020 - 10 - 02 | 443292 . 5194585953 | { \"predicted_value\" : 443292 . 5194585953 , \"confidence\" : 0 . 9991 , \"anomaly\" : null , \"truth\" : null , \"confidence_lower_bound\" : 424501 . 59192981094 , \"confidence_upper_bound\" : 462083 . 4469873797 } | + -------------+-------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ Please note that in the SELECT statement, we select m.saledate instead of t.saledate because we make predictions for future dates that are not in the data table. Now, try changing the type column value to unit , or the bedrooms column value to any number between 1 to 5, and check how the forecasts vary. This is because MindsDB recognizes each grouping as being its own different time series. What's Next? \u00b6 Have fun while trying it out yourself! Bookmark MindsDB repository on GitHub . Sign up for a free MindsDB account . Engage with the MindsDB community on Slack or GitHub to ask questions and share your ideas and thoughts. If this tutorial was helpful, please give us a GitHub star here .","title":"House Sales"},{"location":"sql/tutorials/house-sales-forecasting/#forecasting-quarterly-house-sales-with-mindsdb","text":"","title":"Forecasting Quarterly House Sales with MindsDB"},{"location":"sql/tutorials/house-sales-forecasting/#introduction","text":"In this tutorial, we'll create and train a machine learning model, or as we call it, an AI Table or a predictor . By querying the model, we'll predict the real estate sales using a multivariate time series strategy. Make sure you have access to a working MindsDB installation, either locally or at MindsDB Cloud . If you want to learn how to set up your account at MindsDB Cloud, follow this guide . Another way is to set up MindsDB locally using Docker or Python . Let's get started.","title":"Introduction"},{"location":"sql/tutorials/house-sales-forecasting/#data-setup","text":"","title":"Data Setup"},{"location":"sql/tutorials/house-sales-forecasting/#connecting-the-data","text":"There are a couple of ways you can get the data to follow through with this tutorial. Connecting as a database Connecting as a file You can connect to a demo database that we've prepared for you. It contains the data used throughout this tutorial (the example_db . demo_data . house_sales table). CREATE DATABASE example_db WITH ENGINE = \"postgres\" , PARAMETERS = { \"user\" : \"demo_user\" , \"password\" : \"demo_password\" , \"host\" : \"3.220.66.106\" , \"port\" : \"5432\" , \"database\" : \"demo\" } ; Now you can run queries directly on the demo database. Let's preview the data that we'll use to train our predictor. SELECT * FROM example_db . demo_data . house_sales LIMIT 10 ; The dataset we use in this tutorial is the pre-processed version of the House Property Sales data. You can download the CSV data file here (we use the ma_lga_12345.csv file) and upload it via MindsDB SQL Editor . Follow this guide to find out how to upload a file to MindsDB. Now you can run queries directly on the file as if it were a table. Let's preview the data that we'll use to train our predictor. SELECT * FROM files . house_sales LIMIT 10 ; Pay Attention to the Queries From now on, we'll use the files . house_sales file as a table. Make sure you replace it with example_db.demo_data.house_sales if you connect the data as a database.","title":"Connecting the Data"},{"location":"sql/tutorials/house-sales-forecasting/#understanding-the-data","text":"We use the house sales dataset, where each row is one house or one unit, to predict the MA column values. It tracks quarterly moving averages ( MA ) of house sales aggregated by real estate type and the number of bedrooms in each listing. Below is the sample data stored in the files . house_sales table. + ----------+------+-----+--------+ | saledate | MA | type | bedrooms | + ----------+------+-----+--------+ | 30 / 09 / 2007 | 441854 | house | 2 | | 31 / 12 / 2007 | 441854 | house | 2 | | 31 / 03 / 2008 | 441854 | house | 2 | | 30 / 06 / 2016 | 430880 | unit | 2 | | 30 / 09 / 2016 | 430654 | unit | 2 | + ----------+------+-----+--------+ Where: Column Description Data Type Usage saledate The date of sale. date Feature MA Moving average of the historical median price of the house or unit. integer Label type Type of property ( house or unit ). character varying Feature bedrooms Number of bedrooms. integer Feature Labels and Features A label is a column whose values will be predicted (the y variable in simple linear regression). A feature is a column used to train the model (the x variable in simple linear regression).","title":"Understanding the Data"},{"location":"sql/tutorials/house-sales-forecasting/#training-a-predictor","text":"Let's create and train the machine learning model. For that, we use the CREATE MODEL statement and specify the input columns used to train FROM (features) and what we want to PREDICT (labels). CREATE MODEL mindsdb . house_sales_predictor FROM files ( SELECT * FROM house_sales ) PREDICT MA ORDER BY saledate GROUP BY bedrooms , type -- the target column to be predicted stores one row per quarter WINDOW 8 -- using data from the last two years to make forecasts (last 8 rows) HORIZON 4 ; -- making forecasts for the next year (next 4 rows) We use all of the columns as features, except for the MA column, whose values will be predicted. MindsDB makes it simple so that we don't need to repeat the predictor creation process for every group, that is, for every distinct number of bedrooms or for every distinct type of real estate. Instead, we just group by both the bedrooms and type columns, and the predictor learns from all series and enables forecasts for all of them!","title":"Training a Predictor"},{"location":"sql/tutorials/house-sales-forecasting/#status-of-a-predictor","text":"A predictor may take a couple of minutes for the training to complete. You can monitor the status of the predictor by using this SQL command: SELECT status FROM mindsdb . models WHERE name = 'house_sales_predictor' ; If we run it right after creating a predictor, we get this output: + ------------+ | status | + ------------+ | generating | + ------------+ A bit later, this is the output: + ----------+ | status | + ----------+ | training | + ----------+ And at last, this should be the output: + ----------+ | status | + ----------+ | complete | + ----------+ Now, if the status of our predictor says complete , we can start making predictions!","title":"Status of a Predictor"},{"location":"sql/tutorials/house-sales-forecasting/#making-predictions","text":"You can make predictions by querying the predictor joined with the data table. The SELECT statement lets you make predictions for the label based on the chosen features for a given time period. Usually, you want to know what happens right after the latest training data point that was fed. We have a special keyword for that, the LATEST keyword. SELECT m . saledate AS date , m . MA AS forecast , MA_explain FROM mindsdb . house_sales_predictor AS m JOIN files . house_sales AS t WHERE t . saledate > LATEST AND t . type = 'house' AND t . bedrooms = 2 LIMIT 4 ; On execution, we get: + -------------+-------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ | date | forecast | MA_explain | + -------------+-------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ | 2019 - 12 - 31 | 441413 . 5849598734 | { \"predicted_value\" : 441413 . 5849598734 , \"confidence\" : 0 . 99 , \"anomaly\" : true , \"truth\" : null , \"confidence_lower_bound\" : 440046 . 28237074096 , \"confidence_upper_bound\" : 442780 . 88754900586 } | | 2020 - 04 - 01 | 443292 . 5194586229 | { \"predicted_value\" : 443292 . 5194586229 , \"confidence\" : 0 . 9991 , \"anomaly\" : null , \"truth\" : null , \"confidence_lower_bound\" : 427609 . 3325864327 , \"confidence_upper_bound\" : 458975 . 7063308131 } | | 2020 - 07 - 02 | 443292 . 5194585953 | { \"predicted_value\" : 443292 . 5194585953 , \"confidence\" : 0 . 9991 , \"anomaly\" : null , \"truth\" : null , \"confidence_lower_bound\" : 424501 . 59192981094 , \"confidence_upper_bound\" : 462083 . 4469873797 } | | 2020 - 10 - 02 | 443292 . 5194585953 | { \"predicted_value\" : 443292 . 5194585953 , \"confidence\" : 0 . 9991 , \"anomaly\" : null , \"truth\" : null , \"confidence_lower_bound\" : 424501 . 59192981094 , \"confidence_upper_bound\" : 462083 . 4469873797 } | + -------------+-------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ Please note that in the SELECT statement, we select m.saledate instead of t.saledate because we make predictions for future dates that are not in the data table. Now, try changing the type column value to unit , or the bedrooms column value to any number between 1 to 5, and check how the forecasts vary. This is because MindsDB recognizes each grouping as being its own different time series.","title":"Making Predictions"},{"location":"sql/tutorials/house-sales-forecasting/#whats-next","text":"Have fun while trying it out yourself! Bookmark MindsDB repository on GitHub . Sign up for a free MindsDB account . Engage with the MindsDB community on Slack or GitHub to ask questions and share your ideas and thoughts. If this tutorial was helpful, please give us a GitHub star here .","title":"What's Next?"},{"location":"sql/tutorials/insurance-cost-prediction/","text":"Predict Insurance Cost using MindsDB \u00b6 Dataset: Medical Cost Personal Data Communtiy Author: Kinie K Kusuma Pre-requisites \u00b6 First, you need MindsDB installed. So please make sure you've visited Getting Started Guide and Getting Started with Cloud . You may start to use MindsDB by installing it locally or you can use the Cloud service. Let\u2019s use the cloud for this tutorial. Second, you need a MySQL client to connect to the MindsDB MySQL API. Can you accurately predict insurance costs? \u00b6 In this tutorial, you will learn how to predict insurance costs using MindsDB. This tutorial is very easy because you don't need to learn any machine learning algorithms, all you need to know is just SQL. The process looks like the following: First we will connect MindsDB to a database with past data so it can learn from it We will use a single SQL command that will tell MindsDB to train its predictor We will use the standard SQL Select statement to get predictions from AI Tables in MindsDB. Like if this data already exists! MindsDB will execute a complete Machine Learning workflow behind the scenes, it will determine data types for each column, normalize and encode it, train and test ML model. All this happens automatically, so it is very cool! Those who want to get their hands dirty with manual hyperparameters optimization, you can also do that with MindsDB using a declarative syntax called JSON-AI. So let's look at how it works using a real use case. For the demo purpose we will use a public dataset from Kaggle, but you are free to follow this tutorial with your own data. Connect your database \u00b6 First, you need to connect MindsDB to the database where the data is stored. Open MindsDB GUI and in the left navigation click on Database, then click on the ADD DATABASE. Here, you need to provide all of the required parameters for connecting to the database. Supported Database - select the database that you want to connect to Integrations Name - add a name to the integration, here I'm using 'mysql' but you can name it differently Database - the database name Host - database hostname Port - database port Username - database user Password - user's password Then, click on CONNECT. The next step is to use the MySQL client to connect to MindsDB\u2019s MySQL API, train a new model, and make a prediction. Connect to MindsDB\u2019s MySQL API \u00b6 Here I'm using MySQL command-line client, but you can also follow up with the one that works the best for you, like Dbeaver. The first step is to use the MindsDB Cloud user to connect to the MindsDB MySQL API, using this command: You need to specify the hostname and user name explicitly, as well as a password for connecting. Click enter and you are connected to MindsDB API. If you have an authentication error, please make sure you are providing the email address you have used to create an account on MindsDB Cloud. Data \u00b6 Now, let's show the databases. There are 4 databases, and the MySQL database is the database that I've connected to MindsDB. Let's check the MySQL database. There are 3 tables, and because the tutorial is about insurance cost prediction, we will use the insurance table. Let's check what is inside this table. So, these tables have 7 columns: age: The age of the person (integer) sex: Gender (male or female) bmi: Body mass index is a value derived from the mass and height of a person. The BMI is defined as the body mass divided by the square of the body height, and is expressed in units of kg/m\u00b2, resulting from mass in kilograms and height in meters (float) children: The number of children (integer) smoker: Indicator if the person smoke (yes or no) region: Region where the insured lives (southeast, northeast, southwest or northwest) charges: The insurance cost, this is the target of prediction (float) Create the model \u00b6 Now, to create the model, let's move to the MindsDB database, and see what's inside. There are 2 tables, predictors, and commands. Predictors contain your predictors record, and commands contain your last commands used. To train a new machine learning model we will need to CREATE MODEL as a new record inside the predictors table, and using this command: CREATE MODEL mindsdb . predictor_name FROM integration_name ( SELECT column_name , column_name2 FROM table_name ) PREDICT column_name ; The values that we need to provide are: predictor_name (string) - The name of the model. integration_name (string) - The name of the connection to your database. ds_name (string) - the name of the dataset you want to create, it's optional if you don't specify this value MindsDB will generate by itself. column_name (string) - The feature you want to predict. column_alias - Alias name of the feature you want to predict. So, use this command to create the models: CREATE MODEL insurance_cost_predictor FROM insurance_costs ( SELECT * FROM insurance ) PREDICT charges ; If there's no error, that means your model is created and training has started. To see if your model is finished, use this command: SELECT * FROM mindsdb . models WHERE name = predictor_name ; And values that we need to provide are: predictor_name (string) - The name of the model. SELECT * FROM mindsdb . models WHERE name = 'insurance_cost_predictor' ; If the predictor is ready, it will look like this. The model has been created and trained! The reported accuracy is 75%. If you want to have more control over the model, head to lightwood.io to see how that can be customized. Make prediction \u00b6 Now you are in the last step of this tutorial, making the prediction. To make a prediction you can use this command: SELECT target_variable , target_variable_explain FROM model_table WHERE column3 = \"value\" AND column2 = value ; You need to set these values: target_variable - The original value of the target variable. target_variable_confidence - Model confidence score. target_variable_explain - JSON object that contains additional information as confidence_lower_bound, confidence_upper_bound, anomaly, truth. when_data - The data to make the predictions from(WHERE clause params). SELECT charges , charges_confidence , charges_explain AS info FROM insurance_cost_predictor WHERE age = 20 AND sex = 'male' AND bmi = 33 . 20 AND children = 0 AND smoker = 'no' AND region = 'southeast' ; Finally, we have trained an insurance model using SQL and MindsDB. Conclusions \u00b6 As you can see it is very easy to start making predictions with machine learning even without being a data scientist! Feel free to check this yourself, MindsDB has an option of a free cloud account that is more than enough to give it a try.","title":"Insurance Cost"},{"location":"sql/tutorials/insurance-cost-prediction/#predict-insurance-cost-using-mindsdb","text":"Dataset: Medical Cost Personal Data Communtiy Author: Kinie K Kusuma","title":"Predict Insurance Cost using MindsDB"},{"location":"sql/tutorials/insurance-cost-prediction/#pre-requisites","text":"First, you need MindsDB installed. So please make sure you've visited Getting Started Guide and Getting Started with Cloud . You may start to use MindsDB by installing it locally or you can use the Cloud service. Let\u2019s use the cloud for this tutorial. Second, you need a MySQL client to connect to the MindsDB MySQL API.","title":"Pre-requisites"},{"location":"sql/tutorials/insurance-cost-prediction/#can-you-accurately-predict-insurance-costs","text":"In this tutorial, you will learn how to predict insurance costs using MindsDB. This tutorial is very easy because you don't need to learn any machine learning algorithms, all you need to know is just SQL. The process looks like the following: First we will connect MindsDB to a database with past data so it can learn from it We will use a single SQL command that will tell MindsDB to train its predictor We will use the standard SQL Select statement to get predictions from AI Tables in MindsDB. Like if this data already exists! MindsDB will execute a complete Machine Learning workflow behind the scenes, it will determine data types for each column, normalize and encode it, train and test ML model. All this happens automatically, so it is very cool! Those who want to get their hands dirty with manual hyperparameters optimization, you can also do that with MindsDB using a declarative syntax called JSON-AI. So let's look at how it works using a real use case. For the demo purpose we will use a public dataset from Kaggle, but you are free to follow this tutorial with your own data.","title":"Can you accurately predict insurance costs?"},{"location":"sql/tutorials/insurance-cost-prediction/#connect-your-database","text":"First, you need to connect MindsDB to the database where the data is stored. Open MindsDB GUI and in the left navigation click on Database, then click on the ADD DATABASE. Here, you need to provide all of the required parameters for connecting to the database. Supported Database - select the database that you want to connect to Integrations Name - add a name to the integration, here I'm using 'mysql' but you can name it differently Database - the database name Host - database hostname Port - database port Username - database user Password - user's password Then, click on CONNECT. The next step is to use the MySQL client to connect to MindsDB\u2019s MySQL API, train a new model, and make a prediction.","title":"Connect your database"},{"location":"sql/tutorials/insurance-cost-prediction/#connect-to-mindsdbs-mysql-api","text":"Here I'm using MySQL command-line client, but you can also follow up with the one that works the best for you, like Dbeaver. The first step is to use the MindsDB Cloud user to connect to the MindsDB MySQL API, using this command: You need to specify the hostname and user name explicitly, as well as a password for connecting. Click enter and you are connected to MindsDB API. If you have an authentication error, please make sure you are providing the email address you have used to create an account on MindsDB Cloud.","title":"Connect to MindsDB\u2019s MySQL API"},{"location":"sql/tutorials/insurance-cost-prediction/#data","text":"Now, let's show the databases. There are 4 databases, and the MySQL database is the database that I've connected to MindsDB. Let's check the MySQL database. There are 3 tables, and because the tutorial is about insurance cost prediction, we will use the insurance table. Let's check what is inside this table. So, these tables have 7 columns: age: The age of the person (integer) sex: Gender (male or female) bmi: Body mass index is a value derived from the mass and height of a person. The BMI is defined as the body mass divided by the square of the body height, and is expressed in units of kg/m\u00b2, resulting from mass in kilograms and height in meters (float) children: The number of children (integer) smoker: Indicator if the person smoke (yes or no) region: Region where the insured lives (southeast, northeast, southwest or northwest) charges: The insurance cost, this is the target of prediction (float)","title":"Data"},{"location":"sql/tutorials/insurance-cost-prediction/#create-the-model","text":"Now, to create the model, let's move to the MindsDB database, and see what's inside. There are 2 tables, predictors, and commands. Predictors contain your predictors record, and commands contain your last commands used. To train a new machine learning model we will need to CREATE MODEL as a new record inside the predictors table, and using this command: CREATE MODEL mindsdb . predictor_name FROM integration_name ( SELECT column_name , column_name2 FROM table_name ) PREDICT column_name ; The values that we need to provide are: predictor_name (string) - The name of the model. integration_name (string) - The name of the connection to your database. ds_name (string) - the name of the dataset you want to create, it's optional if you don't specify this value MindsDB will generate by itself. column_name (string) - The feature you want to predict. column_alias - Alias name of the feature you want to predict. So, use this command to create the models: CREATE MODEL insurance_cost_predictor FROM insurance_costs ( SELECT * FROM insurance ) PREDICT charges ; If there's no error, that means your model is created and training has started. To see if your model is finished, use this command: SELECT * FROM mindsdb . models WHERE name = predictor_name ; And values that we need to provide are: predictor_name (string) - The name of the model. SELECT * FROM mindsdb . models WHERE name = 'insurance_cost_predictor' ; If the predictor is ready, it will look like this. The model has been created and trained! The reported accuracy is 75%. If you want to have more control over the model, head to lightwood.io to see how that can be customized.","title":"Create the model"},{"location":"sql/tutorials/insurance-cost-prediction/#make-prediction","text":"Now you are in the last step of this tutorial, making the prediction. To make a prediction you can use this command: SELECT target_variable , target_variable_explain FROM model_table WHERE column3 = \"value\" AND column2 = value ; You need to set these values: target_variable - The original value of the target variable. target_variable_confidence - Model confidence score. target_variable_explain - JSON object that contains additional information as confidence_lower_bound, confidence_upper_bound, anomaly, truth. when_data - The data to make the predictions from(WHERE clause params). SELECT charges , charges_confidence , charges_explain AS info FROM insurance_cost_predictor WHERE age = 20 AND sex = 'male' AND bmi = 33 . 20 AND children = 0 AND smoker = 'no' AND region = 'southeast' ; Finally, we have trained an insurance model using SQL and MindsDB.","title":"Make prediction"},{"location":"sql/tutorials/insurance-cost-prediction/#conclusions","text":"As you can see it is very easy to start making predictions with machine learning even without being a data scientist! Feel free to check this yourself, MindsDB has an option of a free cloud account that is more than enough to give it a try.","title":"Conclusions"},{"location":"sql/tutorials/mindsdb-superset-snowflake/","text":"Using MindsDB Machine Learning to Solve a Real-World time series Problem \u00b6 Let\u2019s use these powerful AI tables in a real-world scenario. (if you are not familiar with AI-Tables, you can learn about them in here . Imagine that you are a data analyst at the Chicago Transit Authority. Every day, you need to optimize the number of buses per route to avoid overcrowded or empty buses. You need machine learning to forecast the number of rides per bus, per route, and by time of day. The data you have looks like the table below with route_id, timestamp, number of rides, and day-type (W = weekend) This is a difficult machine learning problem that is common in databases. A timestamp indicates that we are dealing with the time-series problem. The data is further complicated by the type of day (day-type) the row contains and this is called multivariate. Additionally, there is high-cardinality as each route will have multiple row entries each with different timestamps, rides, and day types. Let\u2019s see how we can use machine learning with MindsDB to optimize the number of buses per route and visualize the results. Set Up MindsDB \u00b6 First things first! You need to connect your database to MindsDB. One of the easy ways to do so is to create a MindsDB cloud account. If you prefer to deploy MindsDB locally, please refer to installation instructions via Docker or PyPI . Once an account is created you can connect to Snowflake using standard parameters like database name (in this case the Chicago Transit Authority), host, port, username, password, etc. Connect MindsDB to the Data for model training \u00b6 MindsDB works through a MySQL Wire protocol. Therefore, you can connect to it using any MySQL client. Here, we\u2019ll use the DBeaver database client and can see the Snowflake databases we are connected to. Step 1: Getting the Training Data \u00b6 We start by getting the training data from the database that we connected to our MindsDB cloud account. It is always good to first make sure that all the databases are present and the connections correct. show databases ; MindsDB comes with some built-in databases as follows: INFORMATION_SCHEMA stores information about MindsDB, MINDSDB stores metadata about the predictors and allows access to the created predictors as tables, DATASOURCE for connecting to data or uploading files. The SNF database is the database of the Chicago Transit Authority that we connected. It provides us with the training data. Let\u2019s check it. SELECT * FROM CHICAGO_TRANSIT_AUTHORITY . PUBLIC . CTA_BUS_RIDES_LATEST LIMIT 100 ; The training data consists of the number of rides per bus route and day. For example, on 2001-07-03, there were 7354 rides on bus route 3. You can download the dataset here and execute the SQL commands along with the tutorial! Step 2: Training the Predictive Model \u00b6 Let\u2019s move on to the next step, which is training the predictive model. For that, we\u2019ll use the MINDSDB database. use mindsdb ; show tables MINDSDB database comes with the predictors and commands tables. The predictors table lets us see the status of our predictive models. For example, assuming that we have already trained our predictive model for forecasting the number of rides, we\u2019ll see the following. SELECT name , status FROM mindsdb . models ; The process of training a predictive model using MindsDB is as simple as creating a view or a table. CREATE MODEL mindsdb . rides_forecaster_demo FROM snf ( SELECT ROUTE , RIDES , DATE FROM CHICAGO_TRANSIT_AUTHORITY . PUBLIC . CTA_BUS_RIDES_LATEST WHERE DATE > '2020-01-01' ) PREDICT RIDES ORDER BY DATE GROUP BY ROUTE WINDOW 10 HORIZON 7 ; Let\u2019s discuss the statement above. We create a predictor table using the CREATE MODEL statement and specifying the database from which the training data comes. The code in yellow selects the filtered training data. After that, we use the PREDICT keyword to define the column whose data we want to forecast. Next, there are standard SQL clauses, such as ORDER BY, GROUP BY, WINDOW, and HORIZON . We use the ORDER BY clause and the DATE column as its argument. By doing so, we emphasize that we deal with a time-series problem. We order the rows by date. The GROUP BY clause divides the data into partitions. Here, each of them relates to a particular bus route. We take into account just the last ten rows for every given prediction. Hence, we use WINDOW 10. To prepare the forecast of the number of bus rides for the next week, we define HORIZON 7. Now, you can execute the CREATE MODEL statement and wait until your predictive model is complete. The mindsdb.models table stores its name as rides_forecaster_demo and its status as training. Once your predictive model is ready, the status changes to complete. Step 3: Getting the Forecasts \u00b6 We are ready to go to the last step, i.e., using the predictive model to get future data. One way is to query the rides_forecaster_demo predictive model directly. Another way is to join this predictive model table to the table with historical data before querying it. We consider a time-series problem. Therefore, it is better to join our predictive model table to the table with historical data. SELECT tb . ROUTE , tb . RIDES AS PREDICTED_RIDES FROM snf . PUBLIC . CTA_BUS_RIDES_LATEST AS ta JOIN mindsdb . rides_forecaster_demo AS tb WHERE ta . ROUTE = \"171\" AND ta . DATE > LATEST LIMIT 7 ; Let\u2019s analyze it. We join the table that stores historical data (i.e., snf.PUBLIC.CTA_BUS_RIDES_LATEST) to our predictive model table (i.e., mindsdb.rides_forecaster_demo). The queried information is the route and the predicted number of rides per route. And the usage of the condition ta.DATE > LATEST (provided by MindsDB) ensures that we get the future number of rides per route. Let\u2019s run the query above to forecast the number of rides for route 171 in the next seven days. Now we know the number of rides for route 171 in the next seven days. We could do it in the same way for all the other routes. Thanks to the special SQL syntax that includes CREATE MODEL , PREDICT , and > LATEST , MindsDB makes it straightforward to run predictors on our chosen data. Now, let\u2019s visualize our predictions. Visualizing the Results using Apache Superset \u00b6 Apache Superset is a modern, open-source data exploration and visualization platform designed for all data personas in an organization. Superset ships with a powerful SQL editor and a no-code chart builder experience. Superset ships with support for most SQL databases out of the box and over 50 visualization types. You can connect to the Snowflake database or your MindsDB database that has a Snowflake connection within. Upon starting up your Superset workspace, your earlier defined database connection is ready to use! So you have access to the Chicago Transit Authority data, as well as to the predictions made by MindsDB. Visualizing Data \u00b6 The two data sets that we are relevant for visualization are the stops_by_route and forecasts data sets. The stops_by_route data set contains the exact location of each bus stop for each bus route. And the forecasts data set stores the actual and predicted number of rides, confidence interval, and lower and upper bounds of prediction, per route and timestamp. Superset lets us visualize the stops_by_route data set as follows. Every bus route has a different color. Also, there is volatility associated with each bus route. Let\u2019s publish this chart to a new dashboard by clicking the +Save button, then switch to the Save as tab, and then type in \u201cRoutes Dashboard\u201d in the Add to Dashboard field. Now, let\u2019s craft a time-series line chart to visualize actual vs predicted riders. Let\u2019s look at the chart that presents the actual number of bus riders (in blue) and the predicted number of bus rides (in purple). Predictions made by MindsDB closely resemble the actual data, except for a short time during March 2020 when the large-scale lockdowns took place. There we see a sudden drop in the number of bus rides. But MindsDB took some time to cope with this new reality and adjust its predictions. Lastly, let\u2019s add a data zoom to this chart for end-users to zoom in on specific date ranges. Click the Customize tab and then click Data Zoom to enable it. Then, click the + Save button and publish to the same \u201cRoutes Dashboard\u201d. Let\u2019s head over to the dashboard now and customize it to make it more dynamic and explorable. Click Dashboards in the top nav bar and then select \u201cRoutes Dashboard\u201d from the list of dashboards. You can rearrange the chart positions by clicking the pencil icon, dragging the corners of the chart objects, and then clicking Save . Let\u2019s add some dashboard filters to this dashboard so dashboard consumers can filter the charts down to specific bus routes and volatility values. Click the right arrow (->) to pop open the filter tray. Then select the pencil icon to start editing this dashboard\u2019s filters. Create the following filters with appropriate filter names: A Value filter on the route column from the forecasts table. A Numerical range filter on the volatility column from the stops_by_route table. Click Save to publish these filters. Let\u2019s give these filters for a test ride! Use the routes filter to only show information for routes 1, 100, and 1001. We could zoom in to see the time during the first large-scale lockdowns in March 2020. For these particular routes, the predictions made by MindsDB are not so far off. Now, let\u2019s use our volatility filter to view only the routes with volatility values greater than 55. Conclusions: Powerful forecasting with MindsDB, your database, and Superset \u00b6 The combination of MindsDB and your database covers all the phases of the ML lifecycle. And Superset helps you to visualize the data in any form of diagrams, charts, or dashboards. MindsDB provides easy-to-use predictive models through AI Tables. You can create these predictive models using SQL statements and feeding the input data. Also, you can query them the same way you query a table. The easiest way to get started with Superset is with the free tier for Preset Cloud , a hassle-free and fully hosted cloud service for Superset. We encourage you to try some predictions with your own data, so please sign up for a free MindsDB cloud account and if you need any help with MindsDB, feel free to ask our Slack and Github communities. What's Next? \u00b6 Have fun while trying it out yourself! Bookmark MindsDB repository on GitHub . Sign up for a free MindsDB account . Engage with the MindsDB community on Slack or GitHub to ask questions and share your ideas and thoughts. If this tutorial was helpful, please give us a GitHub star here .","title":"Using MindsDB Machine Learning to Solve a Real-World **time series** Problem"},{"location":"sql/tutorials/mindsdb-superset-snowflake/#using-mindsdb-machine-learning-to-solve-a-real-world-time-series-problem","text":"Let\u2019s use these powerful AI tables in a real-world scenario. (if you are not familiar with AI-Tables, you can learn about them in here . Imagine that you are a data analyst at the Chicago Transit Authority. Every day, you need to optimize the number of buses per route to avoid overcrowded or empty buses. You need machine learning to forecast the number of rides per bus, per route, and by time of day. The data you have looks like the table below with route_id, timestamp, number of rides, and day-type (W = weekend) This is a difficult machine learning problem that is common in databases. A timestamp indicates that we are dealing with the time-series problem. The data is further complicated by the type of day (day-type) the row contains and this is called multivariate. Additionally, there is high-cardinality as each route will have multiple row entries each with different timestamps, rides, and day types. Let\u2019s see how we can use machine learning with MindsDB to optimize the number of buses per route and visualize the results.","title":"Using MindsDB Machine Learning to Solve a Real-World time series Problem"},{"location":"sql/tutorials/mindsdb-superset-snowflake/#set-up-mindsdb","text":"First things first! You need to connect your database to MindsDB. One of the easy ways to do so is to create a MindsDB cloud account. If you prefer to deploy MindsDB locally, please refer to installation instructions via Docker or PyPI . Once an account is created you can connect to Snowflake using standard parameters like database name (in this case the Chicago Transit Authority), host, port, username, password, etc.","title":"Set Up MindsDB"},{"location":"sql/tutorials/mindsdb-superset-snowflake/#connect-mindsdb-to-the-data-for-model-training","text":"MindsDB works through a MySQL Wire protocol. Therefore, you can connect to it using any MySQL client. Here, we\u2019ll use the DBeaver database client and can see the Snowflake databases we are connected to.","title":"Connect MindsDB to the Data for model training"},{"location":"sql/tutorials/mindsdb-superset-snowflake/#step-1-getting-the-training-data","text":"We start by getting the training data from the database that we connected to our MindsDB cloud account. It is always good to first make sure that all the databases are present and the connections correct. show databases ; MindsDB comes with some built-in databases as follows: INFORMATION_SCHEMA stores information about MindsDB, MINDSDB stores metadata about the predictors and allows access to the created predictors as tables, DATASOURCE for connecting to data or uploading files. The SNF database is the database of the Chicago Transit Authority that we connected. It provides us with the training data. Let\u2019s check it. SELECT * FROM CHICAGO_TRANSIT_AUTHORITY . PUBLIC . CTA_BUS_RIDES_LATEST LIMIT 100 ; The training data consists of the number of rides per bus route and day. For example, on 2001-07-03, there were 7354 rides on bus route 3. You can download the dataset here and execute the SQL commands along with the tutorial!","title":"Step 1: Getting the Training Data"},{"location":"sql/tutorials/mindsdb-superset-snowflake/#step-2-training-the-predictive-model","text":"Let\u2019s move on to the next step, which is training the predictive model. For that, we\u2019ll use the MINDSDB database. use mindsdb ; show tables MINDSDB database comes with the predictors and commands tables. The predictors table lets us see the status of our predictive models. For example, assuming that we have already trained our predictive model for forecasting the number of rides, we\u2019ll see the following. SELECT name , status FROM mindsdb . models ; The process of training a predictive model using MindsDB is as simple as creating a view or a table. CREATE MODEL mindsdb . rides_forecaster_demo FROM snf ( SELECT ROUTE , RIDES , DATE FROM CHICAGO_TRANSIT_AUTHORITY . PUBLIC . CTA_BUS_RIDES_LATEST WHERE DATE > '2020-01-01' ) PREDICT RIDES ORDER BY DATE GROUP BY ROUTE WINDOW 10 HORIZON 7 ; Let\u2019s discuss the statement above. We create a predictor table using the CREATE MODEL statement and specifying the database from which the training data comes. The code in yellow selects the filtered training data. After that, we use the PREDICT keyword to define the column whose data we want to forecast. Next, there are standard SQL clauses, such as ORDER BY, GROUP BY, WINDOW, and HORIZON . We use the ORDER BY clause and the DATE column as its argument. By doing so, we emphasize that we deal with a time-series problem. We order the rows by date. The GROUP BY clause divides the data into partitions. Here, each of them relates to a particular bus route. We take into account just the last ten rows for every given prediction. Hence, we use WINDOW 10. To prepare the forecast of the number of bus rides for the next week, we define HORIZON 7. Now, you can execute the CREATE MODEL statement and wait until your predictive model is complete. The mindsdb.models table stores its name as rides_forecaster_demo and its status as training. Once your predictive model is ready, the status changes to complete.","title":"Step 2: Training the Predictive Model"},{"location":"sql/tutorials/mindsdb-superset-snowflake/#step-3-getting-the-forecasts","text":"We are ready to go to the last step, i.e., using the predictive model to get future data. One way is to query the rides_forecaster_demo predictive model directly. Another way is to join this predictive model table to the table with historical data before querying it. We consider a time-series problem. Therefore, it is better to join our predictive model table to the table with historical data. SELECT tb . ROUTE , tb . RIDES AS PREDICTED_RIDES FROM snf . PUBLIC . CTA_BUS_RIDES_LATEST AS ta JOIN mindsdb . rides_forecaster_demo AS tb WHERE ta . ROUTE = \"171\" AND ta . DATE > LATEST LIMIT 7 ; Let\u2019s analyze it. We join the table that stores historical data (i.e., snf.PUBLIC.CTA_BUS_RIDES_LATEST) to our predictive model table (i.e., mindsdb.rides_forecaster_demo). The queried information is the route and the predicted number of rides per route. And the usage of the condition ta.DATE > LATEST (provided by MindsDB) ensures that we get the future number of rides per route. Let\u2019s run the query above to forecast the number of rides for route 171 in the next seven days. Now we know the number of rides for route 171 in the next seven days. We could do it in the same way for all the other routes. Thanks to the special SQL syntax that includes CREATE MODEL , PREDICT , and > LATEST , MindsDB makes it straightforward to run predictors on our chosen data. Now, let\u2019s visualize our predictions.","title":"Step 3: Getting the Forecasts"},{"location":"sql/tutorials/mindsdb-superset-snowflake/#visualizing-the-results-using-apache-superset","text":"Apache Superset is a modern, open-source data exploration and visualization platform designed for all data personas in an organization. Superset ships with a powerful SQL editor and a no-code chart builder experience. Superset ships with support for most SQL databases out of the box and over 50 visualization types. You can connect to the Snowflake database or your MindsDB database that has a Snowflake connection within. Upon starting up your Superset workspace, your earlier defined database connection is ready to use! So you have access to the Chicago Transit Authority data, as well as to the predictions made by MindsDB.","title":"Visualizing the Results using Apache Superset"},{"location":"sql/tutorials/mindsdb-superset-snowflake/#visualizing-data","text":"The two data sets that we are relevant for visualization are the stops_by_route and forecasts data sets. The stops_by_route data set contains the exact location of each bus stop for each bus route. And the forecasts data set stores the actual and predicted number of rides, confidence interval, and lower and upper bounds of prediction, per route and timestamp. Superset lets us visualize the stops_by_route data set as follows. Every bus route has a different color. Also, there is volatility associated with each bus route. Let\u2019s publish this chart to a new dashboard by clicking the +Save button, then switch to the Save as tab, and then type in \u201cRoutes Dashboard\u201d in the Add to Dashboard field. Now, let\u2019s craft a time-series line chart to visualize actual vs predicted riders. Let\u2019s look at the chart that presents the actual number of bus riders (in blue) and the predicted number of bus rides (in purple). Predictions made by MindsDB closely resemble the actual data, except for a short time during March 2020 when the large-scale lockdowns took place. There we see a sudden drop in the number of bus rides. But MindsDB took some time to cope with this new reality and adjust its predictions. Lastly, let\u2019s add a data zoom to this chart for end-users to zoom in on specific date ranges. Click the Customize tab and then click Data Zoom to enable it. Then, click the + Save button and publish to the same \u201cRoutes Dashboard\u201d. Let\u2019s head over to the dashboard now and customize it to make it more dynamic and explorable. Click Dashboards in the top nav bar and then select \u201cRoutes Dashboard\u201d from the list of dashboards. You can rearrange the chart positions by clicking the pencil icon, dragging the corners of the chart objects, and then clicking Save . Let\u2019s add some dashboard filters to this dashboard so dashboard consumers can filter the charts down to specific bus routes and volatility values. Click the right arrow (->) to pop open the filter tray. Then select the pencil icon to start editing this dashboard\u2019s filters. Create the following filters with appropriate filter names: A Value filter on the route column from the forecasts table. A Numerical range filter on the volatility column from the stops_by_route table. Click Save to publish these filters. Let\u2019s give these filters for a test ride! Use the routes filter to only show information for routes 1, 100, and 1001. We could zoom in to see the time during the first large-scale lockdowns in March 2020. For these particular routes, the predictions made by MindsDB are not so far off. Now, let\u2019s use our volatility filter to view only the routes with volatility values greater than 55.","title":"Visualizing Data"},{"location":"sql/tutorials/mindsdb-superset-snowflake/#conclusions-powerful-forecasting-with-mindsdb-your-database-and-superset","text":"The combination of MindsDB and your database covers all the phases of the ML lifecycle. And Superset helps you to visualize the data in any form of diagrams, charts, or dashboards. MindsDB provides easy-to-use predictive models through AI Tables. You can create these predictive models using SQL statements and feeding the input data. Also, you can query them the same way you query a table. The easiest way to get started with Superset is with the free tier for Preset Cloud , a hassle-free and fully hosted cloud service for Superset. We encourage you to try some predictions with your own data, so please sign up for a free MindsDB cloud account and if you need any help with MindsDB, feel free to ask our Slack and Github communities.","title":"Conclusions: Powerful forecasting with MindsDB, your database, and Superset"},{"location":"sql/tutorials/mindsdb-superset-snowflake/#whats-next","text":"Have fun while trying it out yourself! Bookmark MindsDB repository on GitHub . Sign up for a free MindsDB account . Engage with the MindsDB community on Slack or GitHub to ask questions and share your ideas and thoughts. If this tutorial was helpful, please give us a GitHub star here .","title":"What's Next?"},{"location":"sql/tutorials/mushrooms/","text":"Dataset: Mushrooms Community Author: Chandre Tosca Van Der Westhuizen Mushrooms are a fleshy sporocarp of fungi which can either be edible or poisonous. Its usage dates back centuries with ancient Greek, Chinese and African cultures. They can have high nutritional value and medicinal properties which provide great health benefits. On the other side,some of these fungi can be toxic and consuming the wrong mushroom can have deadly consequences. It is important for industries across the world, like the food and health sector, to identify which type of mushrooms are edible and which are poisonous. We will explore how MindsDB's machine learning predictive model can make it easier classifying mushrooms and predicting which is safe to consume and which can make you ill. Pre-requisites \u00b6 To ensure you can complete all the steps, make sure you have access to the following tools: A MindsDB instance. Check out the installation guide for Docker or PyPi . You can also use MindsDB Cloud . Downloaded the dataset. You can get it from Kaggle Optional: Access to ngrok. You can check the installation details at the ngrok website . Data Overview \u00b6 The dataset consists of the following information: Attribute Information: (classes: edible=e, poisonous=p) - cap_shape: bell=b,conical=c,convex=x,flat=f, knobbed=k,sunken=s - cap_surface: fibrous=f,grooves=g,scaly=y,smooth=s - cap_color: brown=n,buff=b,cinnamon=c,gray=g,green=r,pink=p,purple=u,red=e,white=w,yellow=y - bruises: bruises=t,no=f - odor: almond=a,anise=l,creosote=c,fishy=y,foul=f,musty=m,none=n,pungent=p,spicy=s - gill_attachment: attached=a,descending=d,free=f,notched=n - gill_spacing: close=c,crowded=w,distant=d - gill_size: broad=b,narrow=n - gill_color: black=k,brown=n,buff=b,chocolate=h,gray=g, green=r,orange=o,pink=p,purple=u,red=e,white=w,yellow=y - stalk_shape: enlarging=e,tapering=t - stalk_root: bulbous=b,club=c,cup=u,equal=e,rhizomorphs=z,rooted=r,missing=? - stalk_surface_above_ring: fibrous=f,scaly=y,silky=k,smooth=s - stalk_surface_below_ring: fibrous=f,scaly=y,silky=k,smooth=s - stalk_color_above_ring: brown=n,buff=b,cinnamon=c,gray=g,orange=o,pink=p,red=e,white=w,yellow=y - stalk_color_below_ring: brown=n,buff=b,cinnamon=c,gray=g,orange=o,pink=p,red=e,white=w,yellow=y - veil_type: partial=p,universal=u - veil_color: brown=n,orange=o,white=w,yellow=y - ring_number: none=n,one=o,two=t - ring_type: cobwebby=c,evanescent=e,flaring=f,large=l,none=n,pendant=p,sheathing=s,zone=z - spore_print_color: black=k,brown=n,buff=b,chocolate=h,green=r,orange=o,purple=u,white=w,yellow=y - population: abundant=a,clustered=c,numerous=n,scattered=s,several=v,solitary=y - habitat: grasses=g,leaves=l,meadows=m,paths=p,urban=u,waste=w,woods=d Database connection to MindsDB \u00b6 To establish a database connection we will access MindsDB's GUI. MindsDB has a SQL Editor on Cloud and local via the URL 127.0.0.1:47334/. First, we need to connect MindsDB to the database where the Mushrooms data is stored: Access MindsDB GUI on either cloud or the URL 127.0.0.1:47334/ On the default page, select the button Add Data or alternatively select the plug icon on the left sidebar The 'Select your data source' page will populate for you to choose your database type. For this tutorial we will be selecting the postgres database button. Once you have selected the database type,the page will automatically navigate to the SQL Editor where the syntax to create a database connection will automatically populate for you to enter the required parameters. The required parameters are: CREATE DATABASE display_name --- display name for database. WITH ENGINE = \"postgres\", --- name of the mindsdb handler PARAMETERS = { \"user\": \" \", --- Your database user. \"password\": \" \", --- Your password. \"host\": \" \", --- host, it can be an ip or an url. \"port\": \"5432\", --- common port is 5432. \"database\": \" \" --- The name of your database *optional. } Select the Run button or Shift+Enter to execute the syntax. Once the Database connection is created the console will display a message 'Query successfully completed'. \u200b Please note that some database connections require running a Ngrok tunnel to establish a connection. Run the ngrok command in a terminal: ngrok tcp [ db-port ] for example,if your port number is 5433 you will see a similar output: Session Status online Account myaccount (Plan: Free) Version 2.3.40 Region United States (us) Web Interface http://127.0.0.1:4040 Forwarding tcp://6.tcp.ngrok.io:14789 -> localhost:5433 The forwarded address information will be required when connecting to MindsDB's GUI. Select and copy the 'Forwarding' information, in this case it is 6.tcp.ngrok.io:14789 , where 6.tcp.ngrok.io will be used for the host parameter and 14789 as the port number. Once the database integration is successful we can query the table from the database to ensure the data pulls through on MindsDB. Create a machine learning model. \u00b6 Now we are ready to create our own predictor. We will start by using the SQL Editor to execute simple SQL syntax to create and train a machine learning predictive model. The predictor we will and be trained to determine if a mushroom is edible or poisonous.The following CREATE MODEL statement is used to create predictors: CREATE MODEL mindsdb . predictor_name FROM integration_name ( SELECT column_name , column_name2 FROM table_name ) PREDICT column_name ; The required values that we need to provide are: \u200b - predictor_name (string): The name of the model - integration_name (string): The name of the connection to your database. - column_name (string): The feature you want to predict. Use the following query to create a predictor that will predict the target_class for the specific field parameters. CREATE MODEL mushroom_predictor FROM mindsdb_predictions ( SELECT * FROM mushrooms ) PREDICT class ; Select the Run button or Shift+Enter to execute the syntax. Once the predictor is created the console will display a message 'Query successfully completed'. The predictor was created successfully and has started training. To check the status of the model, use the below query. SELECT * FROM mindsdb . models WHERE name = 'mushroom_predictor' ; After the predictor has finished training, you will see a similar output. Note that MindsDB does model testing for you automatically, so you will immediately see if the predictor is accurate enough. The predictor has completed its training, indicated by the status column, and shows the accuracy of the model. You can revisit training new predictors to increase accuracy by changing the query to better suit the dataset i.e. omitting certain columns etc. Good job! We have successfully created and trained a predictive model \u2728 Make predictions \u00b6 In this section you will learn how to make predictions using your trained model. Now we will use the trained model to make predictions using a SQL syntax. Use the following query using mock data with the predictor. SELECT class FROM mindsdb . mushroom_predictor WHERE cap_shape = 'x' AND cap_surface = 's' AND cap_color = 'n' AND bruises = 't' AND odor = 'p' AND gill_attachment = 'f' AND gill_spacing = 'c' AND gill_size = 'n' AND gill_color = 'k' AND stalk_shape = 'e' AND stalk_root = 'e' AND stalk_surface_above_ring = 's' AND stalk_surface_below_ring = 's' AND stalk_color_above_ring = 'w' AND stalk_color_below_ring = 'w' AND veil_type = 'p' AND veil_color = 'w' AND ring_number = 'o' AND ring_type = 'p' AND spore_print_color = 'k' AND population = 's' AND habitat = 'u' ; The result: The machine learning model has predicted that the mushroom is poisonous. We have successfully created and trained a model that can predict if a model is edible or poisonous. Want to try it out for yourself? Sign up for a free MindsDB account and join our community! Engage with MindsDB community on Slack or Github to ask questions, share and express ideas and thoughts! For more check out other tutorials and MindsDB documentation .","title":"Mushrooms Hunting"},{"location":"sql/tutorials/mushrooms/#pre-requisites","text":"To ensure you can complete all the steps, make sure you have access to the following tools: A MindsDB instance. Check out the installation guide for Docker or PyPi . You can also use MindsDB Cloud . Downloaded the dataset. You can get it from Kaggle Optional: Access to ngrok. You can check the installation details at the ngrok website .","title":"Pre-requisites"},{"location":"sql/tutorials/mushrooms/#data-overview","text":"The dataset consists of the following information: Attribute Information: (classes: edible=e, poisonous=p) - cap_shape: bell=b,conical=c,convex=x,flat=f, knobbed=k,sunken=s - cap_surface: fibrous=f,grooves=g,scaly=y,smooth=s - cap_color: brown=n,buff=b,cinnamon=c,gray=g,green=r,pink=p,purple=u,red=e,white=w,yellow=y - bruises: bruises=t,no=f - odor: almond=a,anise=l,creosote=c,fishy=y,foul=f,musty=m,none=n,pungent=p,spicy=s - gill_attachment: attached=a,descending=d,free=f,notched=n - gill_spacing: close=c,crowded=w,distant=d - gill_size: broad=b,narrow=n - gill_color: black=k,brown=n,buff=b,chocolate=h,gray=g, green=r,orange=o,pink=p,purple=u,red=e,white=w,yellow=y - stalk_shape: enlarging=e,tapering=t - stalk_root: bulbous=b,club=c,cup=u,equal=e,rhizomorphs=z,rooted=r,missing=? - stalk_surface_above_ring: fibrous=f,scaly=y,silky=k,smooth=s - stalk_surface_below_ring: fibrous=f,scaly=y,silky=k,smooth=s - stalk_color_above_ring: brown=n,buff=b,cinnamon=c,gray=g,orange=o,pink=p,red=e,white=w,yellow=y - stalk_color_below_ring: brown=n,buff=b,cinnamon=c,gray=g,orange=o,pink=p,red=e,white=w,yellow=y - veil_type: partial=p,universal=u - veil_color: brown=n,orange=o,white=w,yellow=y - ring_number: none=n,one=o,two=t - ring_type: cobwebby=c,evanescent=e,flaring=f,large=l,none=n,pendant=p,sheathing=s,zone=z - spore_print_color: black=k,brown=n,buff=b,chocolate=h,green=r,orange=o,purple=u,white=w,yellow=y - population: abundant=a,clustered=c,numerous=n,scattered=s,several=v,solitary=y - habitat: grasses=g,leaves=l,meadows=m,paths=p,urban=u,waste=w,woods=d","title":"Data Overview"},{"location":"sql/tutorials/mushrooms/#database-connection-to-mindsdb","text":"To establish a database connection we will access MindsDB's GUI. MindsDB has a SQL Editor on Cloud and local via the URL 127.0.0.1:47334/. First, we need to connect MindsDB to the database where the Mushrooms data is stored: Access MindsDB GUI on either cloud or the URL 127.0.0.1:47334/ On the default page, select the button Add Data or alternatively select the plug icon on the left sidebar The 'Select your data source' page will populate for you to choose your database type. For this tutorial we will be selecting the postgres database button. Once you have selected the database type,the page will automatically navigate to the SQL Editor where the syntax to create a database connection will automatically populate for you to enter the required parameters. The required parameters are: CREATE DATABASE display_name --- display name for database. WITH ENGINE = \"postgres\", --- name of the mindsdb handler PARAMETERS = { \"user\": \" \", --- Your database user. \"password\": \" \", --- Your password. \"host\": \" \", --- host, it can be an ip or an url. \"port\": \"5432\", --- common port is 5432. \"database\": \" \" --- The name of your database *optional. } Select the Run button or Shift+Enter to execute the syntax. Once the Database connection is created the console will display a message 'Query successfully completed'. \u200b Please note that some database connections require running a Ngrok tunnel to establish a connection. Run the ngrok command in a terminal: ngrok tcp [ db-port ] for example,if your port number is 5433 you will see a similar output: Session Status online Account myaccount (Plan: Free) Version 2.3.40 Region United States (us) Web Interface http://127.0.0.1:4040 Forwarding tcp://6.tcp.ngrok.io:14789 -> localhost:5433 The forwarded address information will be required when connecting to MindsDB's GUI. Select and copy the 'Forwarding' information, in this case it is 6.tcp.ngrok.io:14789 , where 6.tcp.ngrok.io will be used for the host parameter and 14789 as the port number. Once the database integration is successful we can query the table from the database to ensure the data pulls through on MindsDB.","title":"Database connection to MindsDB"},{"location":"sql/tutorials/mushrooms/#create-a-machine-learning-model","text":"Now we are ready to create our own predictor. We will start by using the SQL Editor to execute simple SQL syntax to create and train a machine learning predictive model. The predictor we will and be trained to determine if a mushroom is edible or poisonous.The following CREATE MODEL statement is used to create predictors: CREATE MODEL mindsdb . predictor_name FROM integration_name ( SELECT column_name , column_name2 FROM table_name ) PREDICT column_name ; The required values that we need to provide are: \u200b - predictor_name (string): The name of the model - integration_name (string): The name of the connection to your database. - column_name (string): The feature you want to predict. Use the following query to create a predictor that will predict the target_class for the specific field parameters. CREATE MODEL mushroom_predictor FROM mindsdb_predictions ( SELECT * FROM mushrooms ) PREDICT class ; Select the Run button or Shift+Enter to execute the syntax. Once the predictor is created the console will display a message 'Query successfully completed'. The predictor was created successfully and has started training. To check the status of the model, use the below query. SELECT * FROM mindsdb . models WHERE name = 'mushroom_predictor' ; After the predictor has finished training, you will see a similar output. Note that MindsDB does model testing for you automatically, so you will immediately see if the predictor is accurate enough. The predictor has completed its training, indicated by the status column, and shows the accuracy of the model. You can revisit training new predictors to increase accuracy by changing the query to better suit the dataset i.e. omitting certain columns etc. Good job! We have successfully created and trained a predictive model \u2728","title":"Create a machine learning model."},{"location":"sql/tutorials/mushrooms/#make-predictions","text":"In this section you will learn how to make predictions using your trained model. Now we will use the trained model to make predictions using a SQL syntax. Use the following query using mock data with the predictor. SELECT class FROM mindsdb . mushroom_predictor WHERE cap_shape = 'x' AND cap_surface = 's' AND cap_color = 'n' AND bruises = 't' AND odor = 'p' AND gill_attachment = 'f' AND gill_spacing = 'c' AND gill_size = 'n' AND gill_color = 'k' AND stalk_shape = 'e' AND stalk_root = 'e' AND stalk_surface_above_ring = 's' AND stalk_surface_below_ring = 's' AND stalk_color_above_ring = 'w' AND stalk_color_below_ring = 'w' AND veil_type = 'p' AND veil_color = 'w' AND ring_number = 'o' AND ring_type = 'p' AND spore_print_color = 'k' AND population = 's' AND habitat = 'u' ; The result: The machine learning model has predicted that the mushroom is poisonous. We have successfully created and trained a model that can predict if a model is edible or poisonous. Want to try it out for yourself? Sign up for a free MindsDB account and join our community! Engage with MindsDB community on Slack or Github to ask questions, share and express ideas and thoughts! For more check out other tutorials and MindsDB documentation .","title":"Make predictions"},{"location":"sql/tutorials/process-quality/","text":"Dataset: Mining process Data Communtiy Author: pixpack Pre-requisites \u00b6 Before you start make sure that you've: Visited Getting Started Guide Visited Getting Started with Cloud Downloaded the dataset. You can get it from Kaggle . Optional- Visual studio. Manufacturing process quality \u00b6 Predicting process result quality is a common task in manufacturing analytics. Manufacturing plants commonly use quality predictions to gain a competitive edge over their competitors, improve their products or increase their customers satisfaction. MindsDB is a tool that can help you solve quality prediction tasks easily and effectively using machine learning. MindsDB abstracts ML models as virtual \u201cAI Tables\u201d in databases and you can make predictions just using normal SQL commands. In this tutorial you will learn how to predict the quality of a mining process using MindsDB . Upload a file \u00b6 Fix headers: sed -e 's/ /_/g' -e 's/\\(.*\\)/\\L\\1/' -e 's/%_//g' MiningProcess_Flotation_Plant_Database.csv > fixed_headers.csv (for Linux/Unix) edit headers manually: change space to underscore , upper case to lower case, remove % from headers (for Windows) Access MindsDB GUI on local via URL 127.0.0.1:47334/. If you are using MindsDB Cloud, make sure to add the file to a database and create a database connection to MindsDB's GUI. Cloud has a file size limit of 10MB, therefore for this dataset it would be best to either use MindsDB local instance or if on Cloud connect via a database. Select the Add data button or the plug icon on the left side bar. The page will navigate to the 'Select your data source' page. Select 'Files'. Under 'Import a file' select the tab 'Click here to browse local files' and select your data file. Once the file is uploaded 100%, provide a name for the data file that will be saved as a table. Select the button Save and Continue. You can query the file that has been uploaded to see that the data does pull through. SELECT * FROM files . file_name ; Connect to MindsDB SQL Sever \u00b6 MindsDB's GUI has a SQL Editor that allows you to create and train predictors and also makes predictions. However you can also do this via the mysql client in a local terminal by accessing MindsDB SQL Server. mysql - h cloud . mindsdb . com --port 3306 -u username@email.com -p USE mindsdb ; Create a predictor \u00b6 In this section you will connect to MindsDB with the MySql API and create a Predictor. It is in MindsDB terms a machine learning model, but all its complexity is automated and abstracted as a virtual \u201cAI Table\u201d. If you are an ML expert and want to tweak the model, MindsDB also allows you that (please refer to documentation). Use the following query to create a Predictor that will foretell the silica_concentrate at the end of our mining process. The row number is limited to 5000 to speed up training but you can keep the whole dataset. CREATE MODEL mindsdb . process_quality_predictor FROM files ( SELECT iron_feed , silica_feed , starch_flow , amina_flow , ore_pulp_flow , ore_pulp_ph , ore_pulp_density , flotation_column_01_air_flow , flotation_column_02_air_flow , flotation_column_03_air_flow , flotation_column_04_air_flow , flotation_column_05_air_flow , flotation_column_06_air_flow , flotation_column_07_air_flow , flotation_column_01_level , flotation_column_02_level , flotation_column_03_level , flotation_column_04_level , flotation_column_05_level , flotation_column_06_level , flotation_column_07_level , iron_concentrate , silica_concentrate FROM process_quality LIMIT 5000 ) PREDICT silica_concentrate ; On execution, we get: Query OK , 0 rows affected ( 2 min 27 . 52 sec ) Now the Predictor will begin training. You can check the status with the following query. SELECT * FROM mindsdb . models WHERE name = 'process_quality_predictor' ; On execution, we get: + -----------------------------+----------+----------+--------------------+-------------------+------------------+ | name | status | accuracy | predict | select_data_query | training_options | + -----------------------------+----------+----------+--------------------+-------------------+------------------+ | process_quality_predictor | complete | 1 | silica_concentrate | | | + -----------------------------+----------+----------+--------------------+-------------------+------------------+ 1 row in set ( 0 . 28 sec ) As you can see the accuracy of the model is 1 (i.e. 100%). This is the result of using a limited dataset of 5000 rows. In reality when using the whole dataset, you will probably see a more reasonable accuracy. You are now done with creating the predictor! \u2728 Make predictions \u00b6 In this section you will learn how to make predictions using your trained model. To run a prediction against new or existing data, you can use the following query. SELECT silica_concentrate , silica_concentrate_confidence , silica_concentrate_explain FROM mindsdb . process_quality_predictor WHERE iron_feed = 48 . 81 AND silica_feed = 25 . 31 AND starch_flow = 2504 . 94 AND amina_flow = 309 . 448 AND ore_pulp_flow = 377 . 6511682692 AND ore_pulp_ph = 10 . 0607 AND ore_pulp_density = 1 . 68676 ; On execution, we get: + --------------------+-------------------------------+-------------------------------------------------------------------------------------------------------------------------------------------------+ | silica_concentrate | silica_concentrate_confidence | Info | + --------------------+-------------------------------+-------------------------------------------------------------------------------------------------------------------------------------------------+ | 1 . 68 | 0 . 99 | { \"predicted_value\" : \"1.68\" , \"confidence\" : 0 . 99 , \"confidence_lower_bound\" : null , \"confidence_upper_bound\" : null , \"anomaly\" : null , \"truth\" : null } | + --------------------+-------------------------------+-------------------------------------------------------------------------------------------------------------------------------------------------+ 1 row in set ( 0 . 81 sec ) As you can see, the model predicted the silica concentrate for our data point. Again we can see a very high confidence due to the limited dataset. When making predictions you can include different fields. As you can notice, we have only included the first 7 fields of our dataset. You are free to test different combinations. In the previous example, we have made a prediction for a single data point. In a real scenario, you might want to make predictions on multiple data points. In this case, MindsDB allows you to Join this other table with the Predictor. In result, you will get another table as an output with a predicted value as one of its columns. Let\u2019s see how to make batch predictions. Use the following command to create the batch prediction. SELECT collected_data . iron_feed , collected_data . silica_feed , collected_data . starch_flow , collected_data . amina_flow , collected_data . ore_pulp_flow , collected_data . ore_pulp_ph , collected_data . ore_pulp_density , predictions . silica_concentrate_confidence AS confidence , predictions . silica_concentrate AS predicted_silica_concentrate FROM process_quality_integration . process_quality AS collected_data JOIN mindsdb . process_quality_predictor AS predictions LIMIT 5 ; As you can see below, the predictor has made multiple predictions for each data point in the collected_data table! You can also try selecting other fields to get more insight on the predictions. See the JOIN clause documentation for more information. + -----------+-------------+-------------+------------+---------------+-------------+------------------+------------+------------------------------+ | iron_feed | silica_feed | starch_flow | amina_flow | ore_pulp_flow | ore_pulp_ph | ore_pulp_density | confidence | predicted_silica_concentrate | + -----------+-------------+-------------+------------+---------------+-------------+------------------+------------+------------------------------+ | 58 . 84 | 11 . 46 | 3277 . 34 | 564 . 209 | 403 . 242 | 9 . 88472 | 1 . 76297 | 0 . 99 | 2 . 129567174379606 | | 58 . 84 | 11 . 46 | 3333 . 59 | 565 . 308 | 401 . 016 | 9 . 88543 | 1 . 76331 | 0 . 99 | 2 . 129548423407259 | | 58 . 84 | 11 . 46 | 3400 . 39 | 565 . 674 | 399 . 551 | 9 . 88613 | 1 . 76366 | 0 . 99 | 2 . 130100408285386 | | 58 . 84 | 11 . 46 | 3410 . 55 | 563 . 843 | 397 . 559 | 9 . 88684 | 1 . 764 | 0 . 99 | 2 . 1298757513510136 | | 58 . 84 | 11 . 46 | 3408 . 98 | 559 . 57 | 401 . 719 | 9 . 88755 | 1 . 76434 | 0 . 99 | 2 . 130438907683961 | + -----------+-------------+-------------+------------+---------------+-------------+------------------+------------+------------------------------+ What's Next? \u00b6 Have fun while trying it out yourself! Bookmark MindsDB repository on GitHub . Sign up for a free MindsDB account . Engage with the MindsDB community on Slack or GitHub to ask questions and share your ideas and thoughts. If this tutorial was helpful, please give us a GitHub star here .","title":"Process Quality"},{"location":"sql/tutorials/process-quality/#pre-requisites","text":"Before you start make sure that you've: Visited Getting Started Guide Visited Getting Started with Cloud Downloaded the dataset. You can get it from Kaggle . Optional- Visual studio.","title":"Pre-requisites"},{"location":"sql/tutorials/process-quality/#manufacturing-process-quality","text":"Predicting process result quality is a common task in manufacturing analytics. Manufacturing plants commonly use quality predictions to gain a competitive edge over their competitors, improve their products or increase their customers satisfaction. MindsDB is a tool that can help you solve quality prediction tasks easily and effectively using machine learning. MindsDB abstracts ML models as virtual \u201cAI Tables\u201d in databases and you can make predictions just using normal SQL commands. In this tutorial you will learn how to predict the quality of a mining process using MindsDB .","title":"Manufacturing process quality"},{"location":"sql/tutorials/process-quality/#upload-a-file","text":"Fix headers: sed -e 's/ /_/g' -e 's/\\(.*\\)/\\L\\1/' -e 's/%_//g' MiningProcess_Flotation_Plant_Database.csv > fixed_headers.csv (for Linux/Unix) edit headers manually: change space to underscore , upper case to lower case, remove % from headers (for Windows) Access MindsDB GUI on local via URL 127.0.0.1:47334/. If you are using MindsDB Cloud, make sure to add the file to a database and create a database connection to MindsDB's GUI. Cloud has a file size limit of 10MB, therefore for this dataset it would be best to either use MindsDB local instance or if on Cloud connect via a database. Select the Add data button or the plug icon on the left side bar. The page will navigate to the 'Select your data source' page. Select 'Files'. Under 'Import a file' select the tab 'Click here to browse local files' and select your data file. Once the file is uploaded 100%, provide a name for the data file that will be saved as a table. Select the button Save and Continue. You can query the file that has been uploaded to see that the data does pull through. SELECT * FROM files . file_name ;","title":"Upload a file"},{"location":"sql/tutorials/process-quality/#connect-to-mindsdb-sql-sever","text":"MindsDB's GUI has a SQL Editor that allows you to create and train predictors and also makes predictions. However you can also do this via the mysql client in a local terminal by accessing MindsDB SQL Server. mysql - h cloud . mindsdb . com --port 3306 -u username@email.com -p USE mindsdb ;","title":"Connect to MindsDB SQL Sever"},{"location":"sql/tutorials/process-quality/#create-a-predictor","text":"In this section you will connect to MindsDB with the MySql API and create a Predictor. It is in MindsDB terms a machine learning model, but all its complexity is automated and abstracted as a virtual \u201cAI Table\u201d. If you are an ML expert and want to tweak the model, MindsDB also allows you that (please refer to documentation). Use the following query to create a Predictor that will foretell the silica_concentrate at the end of our mining process. The row number is limited to 5000 to speed up training but you can keep the whole dataset. CREATE MODEL mindsdb . process_quality_predictor FROM files ( SELECT iron_feed , silica_feed , starch_flow , amina_flow , ore_pulp_flow , ore_pulp_ph , ore_pulp_density , flotation_column_01_air_flow , flotation_column_02_air_flow , flotation_column_03_air_flow , flotation_column_04_air_flow , flotation_column_05_air_flow , flotation_column_06_air_flow , flotation_column_07_air_flow , flotation_column_01_level , flotation_column_02_level , flotation_column_03_level , flotation_column_04_level , flotation_column_05_level , flotation_column_06_level , flotation_column_07_level , iron_concentrate , silica_concentrate FROM process_quality LIMIT 5000 ) PREDICT silica_concentrate ; On execution, we get: Query OK , 0 rows affected ( 2 min 27 . 52 sec ) Now the Predictor will begin training. You can check the status with the following query. SELECT * FROM mindsdb . models WHERE name = 'process_quality_predictor' ; On execution, we get: + -----------------------------+----------+----------+--------------------+-------------------+------------------+ | name | status | accuracy | predict | select_data_query | training_options | + -----------------------------+----------+----------+--------------------+-------------------+------------------+ | process_quality_predictor | complete | 1 | silica_concentrate | | | + -----------------------------+----------+----------+--------------------+-------------------+------------------+ 1 row in set ( 0 . 28 sec ) As you can see the accuracy of the model is 1 (i.e. 100%). This is the result of using a limited dataset of 5000 rows. In reality when using the whole dataset, you will probably see a more reasonable accuracy. You are now done with creating the predictor! \u2728","title":"Create a predictor"},{"location":"sql/tutorials/process-quality/#make-predictions","text":"In this section you will learn how to make predictions using your trained model. To run a prediction against new or existing data, you can use the following query. SELECT silica_concentrate , silica_concentrate_confidence , silica_concentrate_explain FROM mindsdb . process_quality_predictor WHERE iron_feed = 48 . 81 AND silica_feed = 25 . 31 AND starch_flow = 2504 . 94 AND amina_flow = 309 . 448 AND ore_pulp_flow = 377 . 6511682692 AND ore_pulp_ph = 10 . 0607 AND ore_pulp_density = 1 . 68676 ; On execution, we get: + --------------------+-------------------------------+-------------------------------------------------------------------------------------------------------------------------------------------------+ | silica_concentrate | silica_concentrate_confidence | Info | + --------------------+-------------------------------+-------------------------------------------------------------------------------------------------------------------------------------------------+ | 1 . 68 | 0 . 99 | { \"predicted_value\" : \"1.68\" , \"confidence\" : 0 . 99 , \"confidence_lower_bound\" : null , \"confidence_upper_bound\" : null , \"anomaly\" : null , \"truth\" : null } | + --------------------+-------------------------------+-------------------------------------------------------------------------------------------------------------------------------------------------+ 1 row in set ( 0 . 81 sec ) As you can see, the model predicted the silica concentrate for our data point. Again we can see a very high confidence due to the limited dataset. When making predictions you can include different fields. As you can notice, we have only included the first 7 fields of our dataset. You are free to test different combinations. In the previous example, we have made a prediction for a single data point. In a real scenario, you might want to make predictions on multiple data points. In this case, MindsDB allows you to Join this other table with the Predictor. In result, you will get another table as an output with a predicted value as one of its columns. Let\u2019s see how to make batch predictions. Use the following command to create the batch prediction. SELECT collected_data . iron_feed , collected_data . silica_feed , collected_data . starch_flow , collected_data . amina_flow , collected_data . ore_pulp_flow , collected_data . ore_pulp_ph , collected_data . ore_pulp_density , predictions . silica_concentrate_confidence AS confidence , predictions . silica_concentrate AS predicted_silica_concentrate FROM process_quality_integration . process_quality AS collected_data JOIN mindsdb . process_quality_predictor AS predictions LIMIT 5 ; As you can see below, the predictor has made multiple predictions for each data point in the collected_data table! You can also try selecting other fields to get more insight on the predictions. See the JOIN clause documentation for more information. + -----------+-------------+-------------+------------+---------------+-------------+------------------+------------+------------------------------+ | iron_feed | silica_feed | starch_flow | amina_flow | ore_pulp_flow | ore_pulp_ph | ore_pulp_density | confidence | predicted_silica_concentrate | + -----------+-------------+-------------+------------+---------------+-------------+------------------+------------+------------------------------+ | 58 . 84 | 11 . 46 | 3277 . 34 | 564 . 209 | 403 . 242 | 9 . 88472 | 1 . 76297 | 0 . 99 | 2 . 129567174379606 | | 58 . 84 | 11 . 46 | 3333 . 59 | 565 . 308 | 401 . 016 | 9 . 88543 | 1 . 76331 | 0 . 99 | 2 . 129548423407259 | | 58 . 84 | 11 . 46 | 3400 . 39 | 565 . 674 | 399 . 551 | 9 . 88613 | 1 . 76366 | 0 . 99 | 2 . 130100408285386 | | 58 . 84 | 11 . 46 | 3410 . 55 | 563 . 843 | 397 . 559 | 9 . 88684 | 1 . 764 | 0 . 99 | 2 . 1298757513510136 | | 58 . 84 | 11 . 46 | 3408 . 98 | 559 . 57 | 401 . 719 | 9 . 88755 | 1 . 76434 | 0 . 99 | 2 . 130438907683961 | + -----------+-------------+-------------+------------+---------------+-------------+------------------+------------+------------------------------+","title":"Make predictions"},{"location":"sql/tutorials/process-quality/#whats-next","text":"Have fun while trying it out yourself! Bookmark MindsDB repository on GitHub . Sign up for a free MindsDB account . Engage with the MindsDB community on Slack or GitHub to ask questions and share your ideas and thoughts. If this tutorial was helpful, please give us a GitHub star here .","title":"What's Next?"},{"location":"sql/tutorials/spam_emails_tutorial/","text":"Communtiy Author: PWiederspan Pre-requisites \u00b6 This tutorial will be easier to follow if you have first read: - Getting Started Guide - Getting Started with Cloud And have downloaded this dataset. Filtering Spam Emails \u00b6 Anyone with an email address has experienced it, you open your inbox to find a random email promising a cash reward if you click now. Or maybe it\u2019s sneakier, a tracking link for a package you don\u2019t remember ordering. Spam emails are annoying for anyone, they clutter your inbox and send unnecessary push notifications, but they can be security risks as well. Whether a malicious tracking link in an official looking email, or a phishing scam asking you to confirm credentials, Spam emails are a serious concern for any IT department. Luckily, modern email clients have robust spam filters which remove potentially harmful emails from the inbox and label them as spam. Many major email providers use Machine Learning to analyze incoming emails and determine whether or not they are spam. Let's take a look at how MindsDB can be used to do this from a dataset of about 4600 emails. Upload the Dataset File \u00b6 If you are planning on using MindsDB to make predictions based on your own dataset, you probably have that data in a database, in which case you would want to follow the steps described in the Getting Started Guide , mentioned in the prerequisite section, to connect MindsDB to your database. For this tutorial we will be using the MindsDB cloud editor. If you haven\u2019t familiarized yourself with it, now is a great time to do so. From the main page in the editor, select the \u201cUpload File\u201d button in the top right corner. Then from the new page, browse local files and select the correct Spam Emails CSV, name it appropriately (in this tutorial we use spam_predict), and select the \u201cSave and Continue\u201d button. When the upload is complete it should take you back to the main editor page with something like this in the query box: --- MindsDB ships with a filesystem database called 'files' --- Each file you uploaded is saved as a table there. --- --- You can always check the list tables in files as follows: SHOW TABLES FROM files ; Selecting the \u201cRun\u201d button in the top left corner, or pressing Shift+Enter, will run the query and you should see a list of all files uploaded in the bottom section. Create A Predictor \u00b6 For this particular dataset each row represents a single email, with each column representing the frequency of a given word or symbol in that email. There are also columns for the total, average, and longest run of capital letters throughout the email. First we need to switch to using the mindsdb database. USE mindsdb ; We will start by training a model with the CREATE MODEL command using all columns from the dataset we uploaded. CREATE MODEL mindsdb . spam_predictor FROM files ( SELECT * FROM spam_predict ) PREDICT Spam ; We can check the status of the model by querying the name we used as the predictor. As this is a larger dataset, the status may show \u201cGenerating\u201d or \u201cTraining\u201d for a few minutes. Re-run the query to refresh. SELECT * FROM mindsdb . models WHERE name = 'spam_predictor' ; When the model has finished training the status will change to \"complete\". On execution, we get: + ----------------+----------+--------------------+-------------+------------------+------------------+-------+--------------------+------------------+ | name | status | accuracy | predict | update_status | mindsdb_version | error | select_data_query | training_options | + ----------------+----------+--------------------+-------------+------------------+------------------+-------+--------------------+------------------+ | spam_predictor | complete | 0 . 9580387730450108 | Spam | up_to_date | 22 . 4 . 2 . 1 | null | | | + ----------------+----------+--------------------+-------------+------------------+------------------+-------+--------------------+------------------+ You have just created and trained the Machine Learning Model with approximately 96% accuracy! Make Predictions \u00b6 We will now begin to make predictions about whether a given email is spam using the model we just trained. The generic syntax to make predictions is: SELECT t . column_name1 , t . column_name2 FROM integration_name . table AS t JOIN mindsdb . predictor_name AS p WHERE t . column_name IN ( value1 , value2 , ...); For our prediction we will query the spam, spam_confidence, and spam_explain columns from the spam_predictor model we trained. To simulate a test email we will provide values for several in the WHERE clause representing various word frequencies and capital letter runs. SELECT spam , spam_confidence , spam_explain FROM mindsdb . spam_predictor WHERE word_freq_internet = 2 . 5 AND word_freq_email = 2 AND word_freq_credit = 0 . 9 AND word_freq_money = 0 . 9 AND word_freq_report = 1 . 2 AND word_freq_free = 1 . 2 AND word_freq_your = 0 . 9 AND word_freq_all = 2 . 4 AND capital_run_length_average = 20 AND word_freq_mail = 1 AND capital_run_total = 20 AND capital_run_longest = 20 ; On execution, we get: + -------+-------------------+-------------------------------------------------------------------------------------------+ | spam | spam_confidence | spam_explain | + -------+-------------------+-------------------------------------------------------------------------------------------+ | 1 | 0 . 956989247311828 | { \"predicted_value\" : \"1\" , \"confidence\" : 0 . 956989247311828 , \"anomaly\" : null , \"truth\" : null } | + -------+-------------------+-------------------------------------------------------------------------------------------+ With this output we can see that MindsDB predicts that this is a spam email with approximately 96% accuracy. You can change these values and add additional word frequency columns to the query as you see fit to get different predictions. Batch Predictions \u00b6 In the example above, we saw how we can use MindsDB to determine if a single email is spam, however, in a lot of cases we want to determine if any email in an inbox is spam. We can do batch predictions using the JOIN command. By joining our spam_predictor table on a query of our dataset, we can see MindsDB prediction for each row alongside the actual spam value for each email. The generic syntax to do this is: SELECT t . column_name1 , t . column_name2 FROM integration_name . table AS t JOIN mindsdb . predictor_name AS p WHERE t . column_name IN ( value1 , value2 , ...); As we are using the original dataset, which contains 58 columns, we will only include a few of them. The important column for the purposes of this tutorial is the \u201cspam\u201d column for both tables, which we can use to check the predictions accuracy. SELECT t . word_freq_internet , t . word_freq_email , t . word_freq_credit , t . word_freq_money , t . word_freq_report , t . capital_run_length_longest , t . spam , p . spam AS predicted_spam FROM files . spam_predict AS t JOIN mindsdb . spam_predictor AS p ; On execution, we get: + --------------------+-----------------+------------------+-----------------+------------------+---------------------------+-------+----------------+ | word_freq_internet | word_freq_email | word_freq_credit | word_freq_money | word_freq_report | capital_run_length_longest | spam | spam_predictor | + --------------------+-----------------+------------------+-----------------+------------------+---------------------------+-------+----------------+ | 0 | 0 | 0 | 0 | 0 | 9989 | 1 | 1 | | 0 | 0 | 0 | 0 | 0 | 99 | 1 | 1 | | 0 | 0 . 21 | 0 | 0 | 0 | 99 | 1 | 1 | | 0 | 0 | 0 | 0 | 0 | 99 | 1 | 1 | | 0 | 0 | 0 | 0 | 0 | 99 | 1 | 1 | + --------------------+-----------------+------------------+-----------------+------------------+---------------------------+-------+----------------+ With the ability to scroll through all 4600 rows, broken up 100 per page. You can now see predictions for a whole dataset! What we learned \u00b6 In this tutorial we learned how to : - Upload a dataset into MindsDB Cloud - Create a MindsDB Predictor - Train our Model using MindsDB - Make individual predictions by providing sample data - Make batch predictions by joining a Predictor with a Dataset","title":"Email spam"},{"location":"sql/tutorials/spam_emails_tutorial/#pre-requisites","text":"This tutorial will be easier to follow if you have first read: - Getting Started Guide - Getting Started with Cloud And have downloaded this dataset.","title":"Pre-requisites"},{"location":"sql/tutorials/spam_emails_tutorial/#filtering-spam-emails","text":"Anyone with an email address has experienced it, you open your inbox to find a random email promising a cash reward if you click now. Or maybe it\u2019s sneakier, a tracking link for a package you don\u2019t remember ordering. Spam emails are annoying for anyone, they clutter your inbox and send unnecessary push notifications, but they can be security risks as well. Whether a malicious tracking link in an official looking email, or a phishing scam asking you to confirm credentials, Spam emails are a serious concern for any IT department. Luckily, modern email clients have robust spam filters which remove potentially harmful emails from the inbox and label them as spam. Many major email providers use Machine Learning to analyze incoming emails and determine whether or not they are spam. Let's take a look at how MindsDB can be used to do this from a dataset of about 4600 emails.","title":"Filtering Spam Emails"},{"location":"sql/tutorials/spam_emails_tutorial/#upload-the-dataset-file","text":"If you are planning on using MindsDB to make predictions based on your own dataset, you probably have that data in a database, in which case you would want to follow the steps described in the Getting Started Guide , mentioned in the prerequisite section, to connect MindsDB to your database. For this tutorial we will be using the MindsDB cloud editor. If you haven\u2019t familiarized yourself with it, now is a great time to do so. From the main page in the editor, select the \u201cUpload File\u201d button in the top right corner. Then from the new page, browse local files and select the correct Spam Emails CSV, name it appropriately (in this tutorial we use spam_predict), and select the \u201cSave and Continue\u201d button. When the upload is complete it should take you back to the main editor page with something like this in the query box: --- MindsDB ships with a filesystem database called 'files' --- Each file you uploaded is saved as a table there. --- --- You can always check the list tables in files as follows: SHOW TABLES FROM files ; Selecting the \u201cRun\u201d button in the top left corner, or pressing Shift+Enter, will run the query and you should see a list of all files uploaded in the bottom section.","title":"Upload the Dataset File"},{"location":"sql/tutorials/spam_emails_tutorial/#create-a-predictor","text":"For this particular dataset each row represents a single email, with each column representing the frequency of a given word or symbol in that email. There are also columns for the total, average, and longest run of capital letters throughout the email. First we need to switch to using the mindsdb database. USE mindsdb ; We will start by training a model with the CREATE MODEL command using all columns from the dataset we uploaded. CREATE MODEL mindsdb . spam_predictor FROM files ( SELECT * FROM spam_predict ) PREDICT Spam ; We can check the status of the model by querying the name we used as the predictor. As this is a larger dataset, the status may show \u201cGenerating\u201d or \u201cTraining\u201d for a few minutes. Re-run the query to refresh. SELECT * FROM mindsdb . models WHERE name = 'spam_predictor' ; When the model has finished training the status will change to \"complete\". On execution, we get: + ----------------+----------+--------------------+-------------+------------------+------------------+-------+--------------------+------------------+ | name | status | accuracy | predict | update_status | mindsdb_version | error | select_data_query | training_options | + ----------------+----------+--------------------+-------------+------------------+------------------+-------+--------------------+------------------+ | spam_predictor | complete | 0 . 9580387730450108 | Spam | up_to_date | 22 . 4 . 2 . 1 | null | | | + ----------------+----------+--------------------+-------------+------------------+------------------+-------+--------------------+------------------+ You have just created and trained the Machine Learning Model with approximately 96% accuracy!","title":"Create A Predictor"},{"location":"sql/tutorials/spam_emails_tutorial/#make-predictions","text":"We will now begin to make predictions about whether a given email is spam using the model we just trained. The generic syntax to make predictions is: SELECT t . column_name1 , t . column_name2 FROM integration_name . table AS t JOIN mindsdb . predictor_name AS p WHERE t . column_name IN ( value1 , value2 , ...); For our prediction we will query the spam, spam_confidence, and spam_explain columns from the spam_predictor model we trained. To simulate a test email we will provide values for several in the WHERE clause representing various word frequencies and capital letter runs. SELECT spam , spam_confidence , spam_explain FROM mindsdb . spam_predictor WHERE word_freq_internet = 2 . 5 AND word_freq_email = 2 AND word_freq_credit = 0 . 9 AND word_freq_money = 0 . 9 AND word_freq_report = 1 . 2 AND word_freq_free = 1 . 2 AND word_freq_your = 0 . 9 AND word_freq_all = 2 . 4 AND capital_run_length_average = 20 AND word_freq_mail = 1 AND capital_run_total = 20 AND capital_run_longest = 20 ; On execution, we get: + -------+-------------------+-------------------------------------------------------------------------------------------+ | spam | spam_confidence | spam_explain | + -------+-------------------+-------------------------------------------------------------------------------------------+ | 1 | 0 . 956989247311828 | { \"predicted_value\" : \"1\" , \"confidence\" : 0 . 956989247311828 , \"anomaly\" : null , \"truth\" : null } | + -------+-------------------+-------------------------------------------------------------------------------------------+ With this output we can see that MindsDB predicts that this is a spam email with approximately 96% accuracy. You can change these values and add additional word frequency columns to the query as you see fit to get different predictions.","title":"Make Predictions"},{"location":"sql/tutorials/spam_emails_tutorial/#batch-predictions","text":"In the example above, we saw how we can use MindsDB to determine if a single email is spam, however, in a lot of cases we want to determine if any email in an inbox is spam. We can do batch predictions using the JOIN command. By joining our spam_predictor table on a query of our dataset, we can see MindsDB prediction for each row alongside the actual spam value for each email. The generic syntax to do this is: SELECT t . column_name1 , t . column_name2 FROM integration_name . table AS t JOIN mindsdb . predictor_name AS p WHERE t . column_name IN ( value1 , value2 , ...); As we are using the original dataset, which contains 58 columns, we will only include a few of them. The important column for the purposes of this tutorial is the \u201cspam\u201d column for both tables, which we can use to check the predictions accuracy. SELECT t . word_freq_internet , t . word_freq_email , t . word_freq_credit , t . word_freq_money , t . word_freq_report , t . capital_run_length_longest , t . spam , p . spam AS predicted_spam FROM files . spam_predict AS t JOIN mindsdb . spam_predictor AS p ; On execution, we get: + --------------------+-----------------+------------------+-----------------+------------------+---------------------------+-------+----------------+ | word_freq_internet | word_freq_email | word_freq_credit | word_freq_money | word_freq_report | capital_run_length_longest | spam | spam_predictor | + --------------------+-----------------+------------------+-----------------+------------------+---------------------------+-------+----------------+ | 0 | 0 | 0 | 0 | 0 | 9989 | 1 | 1 | | 0 | 0 | 0 | 0 | 0 | 99 | 1 | 1 | | 0 | 0 . 21 | 0 | 0 | 0 | 99 | 1 | 1 | | 0 | 0 | 0 | 0 | 0 | 99 | 1 | 1 | | 0 | 0 | 0 | 0 | 0 | 99 | 1 | 1 | + --------------------+-----------------+------------------+-----------------+------------------+---------------------------+-------+----------------+ With the ability to scroll through all 4600 rows, broken up 100 per page. You can now see predictions for a whole dataset!","title":"Batch Predictions"},{"location":"sql/tutorials/spam_emails_tutorial/#what-we-learned","text":"In this tutorial we learned how to : - Upload a dataset into MindsDB Cloud - Create a MindsDB Predictor - Train our Model using MindsDB - Make individual predictions by providing sample data - Make batch predictions by joining a Predictor with a Dataset","title":"What we learned"}]}